#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
G2Retro-Pé¢„è®­ç»ƒæ•°æ®é›†ç±» - åŸºäºå¤šä»»åŠ¡é¢„è®­ç»ƒçš„åŠæ¨¡æ¿é€†åˆæˆæ¨¡å‹

è¿™ä¸ªæ–‡ä»¶å®ç°äº†é¢„è®­ç»ƒé˜¶æ®µçš„æ•°æ®åŠ è½½ï¼Œæ”¯æŒï¼š
1. ä¸‰è·¯æ•°æ®åŠ è½½ï¼ˆäº§ç‰©ã€å¢å¼ºäº§ç‰©ã€åˆæˆå­ï¼‰
2. æ‰¹æ¬¡å†…è´Ÿæ ·æœ¬é‡‡æ ·ï¼ˆç”¨äºäº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ ï¼‰
3. MolCLRå¢å¼ºç­–ç•¥çš„åº”ç”¨
4. ä¸G2Retroç°æœ‰ç»„ä»¶çš„å®Œå…¨å…¼å®¹

å‚è€ƒæ–‡çŒ®ï¼š
- G2Retro: https://www.nature.com/articles/s42004-023-00897-3
- PMSR: Learning Chemical Rules of Retrosynthesis with Pre-training
- MolCLR: Molecular Contrastive Learning of Representations via Graph Neural Networks
"""

import os
import pickle
import random
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from rdkit import Chem
from typing import List, Dict, Tuple, Optional, Union

# å¯¼å…¥G2Retroç°æœ‰ç»„ä»¶
from mol_tree import MolTree
from vocab import Vocab
from config import device

# å®šä¹‰ç®€å•çš„make_cudaå‡½æ•°ï¼ˆå¤ç”¨G2Retroç»„ä»¶ï¼‰
def make_cuda(tensors):
    """å°†å¼ é‡è½¬ç§»åˆ°CUDAè®¾å¤‡ï¼ˆå¤ç”¨G2Retroé€»è¾‘ï¼‰"""
    if torch.cuda.is_available():
        if isinstance(tensors, (list, tuple)):
            return [t.to(device) if torch.is_tensor(t) else t for t in tensors]
        else:
            return tensors.to(device)
    return tensors

class PretrainDataset(Dataset):
    """
    G2Retro-Pé¢„è®­ç»ƒæ•°æ®é›†ç±»
    
    æŒ‰ç…§è®¾è®¡æ–¹æ¡ˆå®ç°ï¼š
    1. åŸºç¡€ä»»åŠ¡æ•°æ®ï¼šäº§ç‰©åˆ†å­å›¾ + ååº”ä¸­å¿ƒæ ‡æ³¨ï¼ˆå¤ç”¨G2Retroï¼‰
    2. åˆ†å­æ¢å¤ä»»åŠ¡æ•°æ®ï¼šMolCLRå¢å¼ºçš„äº§ç‰©åˆ†å­å›¾ + æ¢å¤ç›®æ ‡ï¼ˆå®Œå…¨å‚è€ƒMolCLRï¼‰
    3. äº§ç‰©-åˆæˆå­å¯¹æ¯”æ•°æ®ï¼šäº§ç‰©vsåˆæˆå­çš„è‡ªç„¶å·®å¼‚ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
    """
    
    def __init__(self, 
                 pkl_path: str,
                 vocab: Vocab, 
                 avocab: Vocab,
                 contrastive_sampling: bool = True,
                 augment_ratio: float = 0.15,
                 max_samples: Optional[int] = None,
                 use_atomic: bool = False,
                 use_feature: bool = True):
        """
        åˆå§‹åŒ–é¢„è®­ç»ƒæ•°æ®é›†
        
        Args:
            pkl_path: é¢„è®­ç»ƒæ•°æ®pklæ–‡ä»¶è·¯å¾„ï¼ˆç”¨æˆ·ç”Ÿæˆçš„pretrain_tensors_train.pklï¼‰
            vocab: å­ç»“æ„è¯æ±‡è¡¨ï¼ˆG2Retroç»„ä»¶ï¼‰
            avocab: åŸå­è¯æ±‡è¡¨ï¼ˆG2Retroç»„ä»¶ï¼‰
            contrastive_sampling: æ˜¯å¦å¯ç”¨å¯¹æ¯”å­¦ä¹ è´Ÿæ ·æœ¬é‡‡æ ·
            augment_ratio: MolCLRå¢å¼ºæ¯”ä¾‹
            max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰
            use_atomic: æ˜¯å¦ä½¿ç”¨åŸå­ç‰¹å¾ï¼ˆG2Retroå…¼å®¹ï¼‰
            use_feature: æ˜¯å¦ä½¿ç”¨åˆ†å­ç‰¹å¾ï¼ˆG2Retroå…¼å®¹ï¼‰
        """
        print(f"ğŸ”„ åŠ è½½G2Retro-Pé¢„è®­ç»ƒæ•°æ®: {pkl_path}")
        
        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
        if not os.path.exists(pkl_path):
            raise FileNotFoundError(f"é¢„è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {pkl_path}")
        
        # åŠ è½½é¢„å¤„ç†å¥½çš„æ•°æ®
        with open(pkl_path, 'rb') as f:
            self.raw_data = pickle.load(f)
        
        # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆè°ƒè¯•ç”¨ï¼‰
        if max_samples and len(self.raw_data) > max_samples:
            print(f"ğŸ”¬ ä½¿ç”¨å‰{max_samples}ä¸ªæ ·æœ¬è¿›è¡Œè°ƒè¯•")
            self.raw_data = self.raw_data[:max_samples]
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(self.raw_data):,} æ¡è®°å½•")
        
        # ä¿å­˜å‚æ•°
        self.vocab = vocab
        self.avocab = avocab
        self.contrastive_sampling = contrastive_sampling
        self.augment_ratio = augment_ratio
        self.use_atomic = use_atomic
        self.use_feature = use_feature
        
        # éªŒè¯æ•°æ®è´¨é‡
        self._validate_data_quality()
        
        # ä¸ºå¯¹æ¯”å­¦ä¹ å‡†å¤‡è´Ÿæ ·æœ¬é‡‡æ ·
        if self.contrastive_sampling:
            self._prepare_contrastive_sampling()
    
    def _validate_data_quality(self):
        """éªŒè¯åŠ è½½æ•°æ®çš„è´¨é‡å’Œæ ¼å¼"""
        print("ğŸ” éªŒè¯æ•°æ®è´¨é‡...")
        
        required_keys = ['mol_trees', 'pretrain_info', 'augmented_data', 'vocab']
        valid_count = 0
        
        for i, entry in enumerate(self.raw_data[:100]):  # æ£€æŸ¥å‰100ä¸ªæ ·æœ¬
            if not all(key in entry for key in required_keys):
                continue
                
            mol_trees = entry['mol_trees']
            if len(mol_trees) != 3:  # åº”è¯¥æœ‰äº§ç‰©ã€åˆæˆå­ã€ååº”ç‰©ä¸‰ä¸ªåˆ†å­æ ‘
                continue
                
            prod_tree, synthon_tree, react_tree = mol_trees
            if prod_tree is None or synthon_tree is None or react_tree is None:
                continue
                
            # æ£€æŸ¥MolCLRå¢å¼ºæ•°æ®
            aug_data = entry['augmented_data']
            if not isinstance(aug_data, list) or len(aug_data) == 0:
                continue
                
            valid_count += 1
        
        valid_rate = valid_count / min(100, len(self.raw_data))
        print(f"ğŸ“Š æ•°æ®è´¨é‡æ£€æŸ¥: {valid_rate:.1%} æœ‰æ•ˆç‡")
        
        if valid_rate < 0.8:
            print("âš ï¸  è­¦å‘Š: æ•°æ®è´¨é‡è¾ƒä½ï¼Œè¯·æ£€æŸ¥é¢„å¤„ç†è„šæœ¬")
    
    def _prepare_contrastive_sampling(self):
        """
        ä¸ºäº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ å‡†å¤‡è´Ÿæ ·æœ¬é‡‡æ ·ç­–ç•¥
        
        æŒ‰ç…§è®¾è®¡æ–¹æ¡ˆï¼šåˆ©ç”¨äº§ç‰©åˆ†å­ä¸å…¶åˆæˆå­ç»„åˆä¹‹é—´çš„è‡ªç„¶å·®å¼‚è¿›è¡Œå¯¹æ¯”å­¦ä¹ 
        """
        print("ğŸ¯ å‡†å¤‡äº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ è´Ÿæ ·æœ¬é‡‡æ ·...")
        
        # æ„å»ºæœ‰æ•ˆæ•°æ®ç´¢å¼•
        self.valid_indices = []
        for i, entry in enumerate(self.raw_data):
            if self._is_valid_entry(entry):
                self.valid_indices.append(i)
        
        print(f"âœ… æœ‰æ•ˆå¯¹æ¯”å­¦ä¹ æ ·æœ¬: {len(self.valid_indices):,} æ¡")
        
        # é¢„è®¡ç®—äº§ç‰©SMILESçš„å“ˆå¸Œï¼Œç”¨äºå¿«é€Ÿè´Ÿæ ·æœ¬é‡‡æ ·
        self.product_smiles_hash = {}
        for idx in self.valid_indices:
            entry = self.raw_data[idx]
            product_smiles = entry['pretrain_info']['product_smiles']
            if product_smiles not in self.product_smiles_hash:
                self.product_smiles_hash[product_smiles] = []
            self.product_smiles_hash[product_smiles].append(idx)
    
    def _is_valid_entry(self, entry: Dict) -> bool:
        """æ£€æŸ¥æ•°æ®æ¡ç›®æ˜¯å¦æœ‰æ•ˆ"""
        try:
            mol_trees = entry.get('mol_trees', ())
            if len(mol_trees) != 3:
                return False
            
            prod_tree, synthon_tree, react_tree = mol_trees
            if prod_tree is None or synthon_tree is None:
                return False
            
            pretrain_info = entry.get('pretrain_info', {})
            if not pretrain_info.get('has_reaction_center', False):
                return False
            
            return True
        except:
            return False
    
    def __len__(self) -> int:
        """æ•°æ®é›†å¤§å°"""
        return len(self.raw_data)
    
    def __getitem__(self, idx: int) -> Dict:
        """
        è·å–å•ä¸ªè®­ç»ƒæ ·æœ¬
        
        æŒ‰ç…§è®¾è®¡æ–¹æ¡ˆè¿”å›ä¸‰è·¯æ•°æ®ï¼š
        1. åŸå§‹äº§ç‰©åˆ†å­å›¾ï¼ˆç”¨äºåŸºç¡€ä»»åŠ¡å’Œå¯¹æ¯”å­¦ä¹ ï¼‰
        2. å¢å¼ºäº§ç‰©åˆ†å­å›¾ï¼ˆç”¨äºåˆ†å­æ¢å¤ä»»åŠ¡ï¼‰
        3. åˆæˆå­ç»„åˆï¼ˆç”¨äºå¯¹æ¯”å­¦ä¹ ï¼‰
        
        Returns:
            åŒ…å«æ‰€æœ‰é¢„è®­ç»ƒä»»åŠ¡æ‰€éœ€æ•°æ®çš„å­—å…¸
        """
        entry = self.raw_data[idx]
        
        # æå–æ ¸å¿ƒåˆ†å­æ ‘ï¼ˆå®Œå…¨å¤ç”¨G2Retroç»„ä»¶ï¼‰
        prod_tree, synthon_tree, react_tree = entry['mol_trees']
        pretrain_info = entry['pretrain_info']
        augmented_data = entry['augmented_data']
        
        # 1. åŸºç¡€ä»»åŠ¡æ•°æ®ï¼šäº§ç‰©åˆ†å­å›¾ + ååº”ä¸­å¿ƒæ ‡æ³¨ï¼ˆç›´æ¥ä½¿ç”¨G2Retroï¼‰
        base_task_data = {
            'product_tree': prod_tree,
            'react_tree': react_tree,  # åŒ…å«ååº”ä¸­å¿ƒæ ‡æ³¨
            'pretrain_info': pretrain_info
        }
        
        # 2. åˆ†å­æ¢å¤ä»»åŠ¡æ•°æ®ï¼šåº”ç”¨MolCLRå¢å¼ºç­–ç•¥
        recovery_task_data = self._prepare_recovery_task(prod_tree, augmented_data)
        
        # 3. äº§ç‰©-åˆæˆå­å¯¹æ¯”æ•°æ®ï¼šæ ¸å¿ƒåˆ›æ–°
        contrastive_task_data = self._prepare_contrastive_task(
            prod_tree, synthon_tree, idx
        )
        
        return {
            'idx': idx,
            'base_task': base_task_data,
            'recovery_task': recovery_task_data,
            'contrastive_task': contrastive_task_data,
            'vocab_set': entry.get('vocab', set())
        }
    
    def _prepare_recovery_task(self, prod_tree: MolTree, augmented_data: List[Dict]) -> Dict:
        """
        å‡†å¤‡åˆ†å­æ¢å¤ä»»åŠ¡æ•°æ®
        
        å®Œå…¨å‚è€ƒMolCLRçš„ä¸‰ç§å¢å¼ºç­–ç•¥ï¼š
        1. åŸå­æ©ç  (Atom Masking)
        2. é”®åˆ é™¤ (Bond Deletion) 
        3. å­å›¾ç§»é™¤ (Subgraph Removal)
        
        æŒ‰ç…§PMSRçš„span-maskç­–ç•¥è¿›è¡Œmaskç”Ÿæˆ
        """
        recovery_data = {
            'original_tree': prod_tree,
            'augmented_versions': [],
            'recovery_targets': []
        }
        
        # éå†æ‰€æœ‰å¢å¼ºç‰ˆæœ¬ï¼ˆMolCLRä¸‰ç§ç­–ç•¥ï¼‰
        for aug_item in augmented_data:
            aug_type = aug_item.get('augment_type', 'unknown')
            masked_indices = aug_item.get('masked_indices', [])
            original_values = aug_item.get('original_values', [])
            
            # æ„é€ æ¢å¤ç›®æ ‡ï¼ˆå‚è€ƒPMSRçš„span-maskï¼‰
            recovery_target = {
                'augment_type': aug_type,
                'masked_positions': masked_indices,
                'target_values': original_values,
                'mask_ratio': len(masked_indices) / max(prod_tree.mol.GetNumAtoms(), 1)
            }
            
            recovery_data['augmented_versions'].append(aug_item)
            recovery_data['recovery_targets'].append(recovery_target)
        
        return recovery_data
    
    def _prepare_contrastive_task(self, 
                                prod_tree: MolTree, 
                                synthon_tree: MolTree, 
                                current_idx: int) -> Dict:
        """
        å‡†å¤‡äº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ ä»»åŠ¡æ•°æ®
        
        æ ¸å¿ƒåˆ›æ–°ï¼šç›´æ¥åˆ©ç”¨äº§ç‰©åˆ†å­ä¸å…¶å¯¹åº”åˆæˆå­ç»„åˆä¹‹é—´çš„è‡ªç„¶å·®å¼‚
        è¿™ç§å·®å¼‚æœ¬èº«å°±åŒ…å«äº†ååº”ä¸­å¿ƒä¿¡æ¯ï¼Œä¸ä¸‹æ¸¸ä»»åŠ¡å®ç°å®Œç¾å¯¹é½
        """
        contrastive_data = {
            'anchor': prod_tree,      # é”šç‚¹ï¼šäº§ç‰©åˆ†å­
            'positive': synthon_tree, # æ­£æ ·æœ¬ï¼šå¯¹åº”çš„åˆæˆå­ç»„åˆ
            'negatives': []           # è´Ÿæ ·æœ¬ï¼šå…¶ä»–ååº”çš„åˆæˆå­
        }
        
        # æ‰¹æ¬¡å†…è´Ÿæ ·æœ¬é‡‡æ ·ï¼ˆåœ¨collate_fnä¸­å¤„ç†ï¼Œè¿™é‡Œå…ˆå‡†å¤‡ç´¢å¼•ï¼‰
        if self.contrastive_sampling and hasattr(self, 'valid_indices'):
            # éšæœºé€‰æ‹©ä¸€äº›è´Ÿæ ·æœ¬å€™é€‰
            negative_candidates = [
                idx for idx in self.valid_indices 
                if idx != current_idx
            ]
            
            # é™åˆ¶è´Ÿæ ·æœ¬æ•°é‡ä»¥é¿å…å†…å­˜é—®é¢˜
            max_negatives = min(10, len(negative_candidates))
            if max_negatives > 0:
                negative_indices = random.sample(negative_candidates, max_negatives)
                contrastive_data['negative_indices'] = negative_indices
        
        return contrastive_data
    
    def get_contrastive_batch(self, indices: List[int]) -> Dict:
        """
        ä¸ºå¯¹æ¯”å­¦ä¹ æ„é€ æ‰¹æ¬¡æ•°æ®
        
        å®ç°æ‰¹æ¬¡å†…è´Ÿæ ·æœ¬é‡‡æ ·ï¼š
        - æ­£æ ·æœ¬å¯¹ï¼š(äº§ç‰©åˆ†å­å›¾, å¯¹åº”çš„åˆæˆå­ç»„åˆå›¾)
        - è´Ÿæ ·æœ¬å¯¹ï¼šæ‰¹æ¬¡å†…ä¸åŒååº”çš„äº§ç‰©-åˆæˆå­å¯¹
        """
        batch_data = {
            'products': [],           # äº§ç‰©åˆ†å­å›¾
            'synthons_positive': [],  # å¯¹åº”çš„åˆæˆå­
            'synthons_negative': [],  # è´Ÿæ ·æœ¬åˆæˆå­
            'labels': []              # å¯¹æ¯”å­¦ä¹ æ ‡ç­¾
        }
        
        # æ”¶é›†æ‰€æœ‰äº§ç‰©å’Œåˆæˆå­
        all_products = []
        all_synthons = []
        
        for idx in indices:
            if idx < len(self.raw_data) and self._is_valid_entry(self.raw_data[idx]):
                entry = self.raw_data[idx]
                prod_tree, synthon_tree, _ = entry['mol_trees']
                
                all_products.append(prod_tree)
                all_synthons.append(synthon_tree)
        
        # æ„é€ æ­£è´Ÿæ ·æœ¬å¯¹
        for i, (prod, synthon_pos) in enumerate(zip(all_products, all_synthons)):
            batch_data['products'].append(prod)
            batch_data['synthons_positive'].append(synthon_pos)
            
            # æ‰¹æ¬¡å†…è´Ÿæ ·æœ¬ï¼šå…¶ä»–ååº”çš„åˆæˆå­
            negative_synthons = [
                all_synthons[j] for j in range(len(all_synthons)) if j != i
            ]
            batch_data['synthons_negative'].extend(negative_synthons)
            
            # æ ‡ç­¾ï¼š0è¡¨ç¤ºæ­£æ ·æœ¬ï¼Œ1è¡¨ç¤ºè´Ÿæ ·æœ¬
            batch_data['labels'].extend([0] + [1] * len(negative_synthons))
        
        return batch_data

def collate_pretrain_batch(batch: List[Dict]) -> Dict:
    """
    é¢„è®­ç»ƒæ‰¹æ¬¡æ•°æ®æ•´ç†å‡½æ•°
    
    å°†å¤šä¸ªæ ·æœ¬ç»„ç»‡æˆé€‚åˆG2Retro-Pä¸‰ä¸ªå¹¶è¡Œä»»åŠ¡å¤´çš„æ‰¹æ¬¡æ ¼å¼ï¼š
    1. åŸºç¡€ä»»åŠ¡æ‰¹æ¬¡ï¼ˆå¤ç”¨G2Retroçš„MolCenterï¼‰
    2. åˆ†å­æ¢å¤ä»»åŠ¡æ‰¹æ¬¡ï¼ˆå‚è€ƒMolCLRï¼‰
    3. äº§ç‰©-åˆæˆå­å¯¹æ¯”æ‰¹æ¬¡ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
    """
    
    batch_size = len(batch)
    collated = {
        'batch_size': batch_size,
        'indices': [item['idx'] for item in batch],
        'base_tasks': [],
        'recovery_tasks': [],
        'contrastive_tasks': {
            'products': [],
            'synthons_positive': [],
            'synthons_negative': [],
            'batch_labels': []
        }
    }
    
    # 1. æ”¶é›†åŸºç¡€ä»»åŠ¡æ•°æ®ï¼ˆç›´æ¥ä¼ ç»™G2Retroçš„MolCenterï¼‰
    for item in batch:
        base_task = item['base_task']
        collated['base_tasks'].append({
            'product_tree': base_task['product_tree'],
            'react_tree': base_task['react_tree'],
            'pretrain_info': base_task['pretrain_info']
        })
    
    # 2. æ”¶é›†åˆ†å­æ¢å¤ä»»åŠ¡æ•°æ®ï¼ˆé€ç»™åˆ†å­æ¢å¤å¤´ï¼‰
    for item in batch:
        recovery_task = item['recovery_task']
        collated['recovery_tasks'].append(recovery_task)
    
    # 3. æ„é€ å¯¹æ¯”å­¦ä¹ æ‰¹æ¬¡ï¼ˆæ‰¹æ¬¡å†…è´Ÿæ ·æœ¬é‡‡æ ·ï¼‰
    for i, item in enumerate(batch):
        contrastive_task = item['contrastive_task']
        anchor = contrastive_task['anchor']      # äº§ç‰©
        positive = contrastive_task['positive']  # å¯¹åº”åˆæˆå­
        
        collated['contrastive_tasks']['products'].append(anchor)
        collated['contrastive_tasks']['synthons_positive'].append(positive)
        
        # æ‰¹æ¬¡å†…è´Ÿæ ·æœ¬ï¼šå…¶ä»–æ ·æœ¬çš„åˆæˆå­
        negatives = [
            batch[j]['contrastive_task']['positive'] 
            for j in range(batch_size) if j != i
        ]
        collated['contrastive_tasks']['synthons_negative'].extend(negatives)
        
        # å¯¹æ¯”å­¦ä¹ æ ‡ç­¾
        collated['contrastive_tasks']['batch_labels'].extend(
            [i] + [-1] * len(negatives)  # iè¡¨ç¤ºæ­£æ ·æœ¬ï¼Œ-1è¡¨ç¤ºè´Ÿæ ·æœ¬
        )
    
    return collated

def create_pretrain_dataloader(pkl_path: str,
                              vocab: Vocab,
                              avocab: Vocab, 
                              batch_size: int = 32,
                              num_workers: int = 4,
                              shuffle: bool = True,
                              **kwargs) -> DataLoader:
    """
    åˆ›å»ºG2Retro-Pé¢„è®­ç»ƒæ•°æ®åŠ è½½å™¨
    
    è¿™æ˜¯ç”¨æˆ·è°ƒç”¨çš„ä¸»è¦æ¥å£ï¼Œé›†æˆäº†æ‰€æœ‰é¢„è®­ç»ƒåŠŸèƒ½
    """
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = PretrainDataset(
        pkl_path=pkl_path,
        vocab=vocab, 
        avocab=avocab,
        **kwargs
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset=dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        collate_fn=collate_pretrain_batch,
        pin_memory=True,
        drop_last=True  # ç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸€è‡´ï¼ˆå¯¹æ¯”å­¦ä¹ éœ€è¦ï¼‰
    )
    
    print(f"âœ… G2Retro-Pé¢„è®­ç»ƒæ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
    print(f"   - æ•°æ®é›†å¤§å°: {len(dataset):,} æ¡")
    print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   - æ€»æ‰¹æ¬¡æ•°: {len(dataloader):,}")
    
    return dataloader

# æµ‹è¯•å’ŒéªŒè¯å‡½æ•°
def test_pretrain_dataset(pkl_path: str, vocab: Vocab, avocab: Vocab):
    """æµ‹è¯•é¢„è®­ç»ƒæ•°æ®é›†çš„åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•G2Retro-Pé¢„è®­ç»ƒæ•°æ®é›†...")
    
    # åˆ›å»ºå°è§„æ¨¡æµ‹è¯•æ•°æ®é›†
    dataset = PretrainDataset(
        pkl_path=pkl_path,
        vocab=vocab,
        avocab=avocab,
        max_samples=100  # ä»…æµ‹è¯•100ä¸ªæ ·æœ¬
    )
    
    # æµ‹è¯•å•ä¸ªæ ·æœ¬
    sample = dataset[0]
    print(f"âœ… æ ·æœ¬ç»“æ„: {list(sample.keys())}")
    
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset, 
        batch_size=4, 
        collate_fn=collate_pretrain_batch
    )
    
    batch = next(iter(dataloader))
    print(f"âœ… æ‰¹æ¬¡ç»“æ„: {list(batch.keys())}")
    print(f"âœ… æ‰¹æ¬¡å¤§å°: {batch['batch_size']}")
    
    print("ğŸ‰ é¢„è®­ç»ƒæ•°æ®é›†æµ‹è¯•é€šè¿‡ï¼")

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    from vocab import common_atom_vocab
    
    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µè®¾ç½®è·¯å¾„å’Œè¯æ±‡è¡¨
    pkl_path = "../data/pretrain/pretrain_tensors_train.pkl"
    vocab_path = "../data/pretrain/vocab_train.txt"
    
    # åŠ è½½è¯æ±‡è¡¨ï¼ˆå¤ç”¨G2Retroï¼‰
    vocab_list = [x.strip() for x in open(vocab_path)]
    vocab = Vocab(vocab_list)
    avocab = common_atom_vocab
    
    # æµ‹è¯•æ•°æ®é›†
    test_pretrain_dataset(pkl_path, vocab, avocab) 
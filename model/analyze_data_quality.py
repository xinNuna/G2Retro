#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¢„è®­ç»ƒæ•°æ®è´¨é‡åˆ†æè„šæœ¬ - å†…å­˜ä¼˜åŒ–ç‰ˆ

è¿™ä¸ªæ–‡ä»¶åº”è¯¥æ›¿æ¢ç°æœ‰çš„ analyze_data_quality.py
ä¸“é—¨é’ˆå¯¹å¤§æ–‡ä»¶å’Œç£ç›˜ç©ºé—´ä¸è¶³çš„æƒ…å†µè¿›è¡Œä¼˜åŒ–
"""

import pickle
import pandas as pd
import numpy as np
from collections import Counter
import argparse
import os
import gc

def analyze_pretrain_data_efficient(data_path, max_samples=5000):
    """
    é«˜æ•ˆåˆ†æé¢„è®­ç»ƒæ•°æ®çš„è´¨é‡ - ä½¿ç”¨é‡‡æ ·é¿å…å†…å­˜é—®é¢˜
    """
    print(f"ğŸ” é«˜æ•ˆåˆ†æé¢„è®­ç»ƒæ•°æ®: {data_path}")
    
    if not os.path.exists(data_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return None
    
    file_size = os.path.getsize(data_path) / 1024**3
    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.1f} GB")
    
    try:
        print("â³ æ­£åœ¨åŠ è½½æ•°æ®ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
        with open(data_path, 'rb') as f:
            data = pickle.load(f)
        
        total_samples = len(data)
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {total_samples:,} æ¡")
        
        # ä½¿ç”¨é‡‡æ ·åˆ†æä»¥èŠ‚çœå†…å­˜å’Œæ—¶é—´
        if total_samples > max_samples:
            print(f"ğŸ”¬ ä½¿ç”¨é‡‡æ ·åˆ†æ: {max_samples}/{total_samples}")
            # å‡åŒ€é‡‡æ ·
            step = total_samples // max_samples
            sample_indices = list(range(0, total_samples, step))[:max_samples]
            sample_data = [data[i] for i in sample_indices]
        else:
            sample_data = data
        
        print(f"ğŸ“ˆ åˆ†ææ ·æœ¬æ•°: {len(sample_data)}")
        
        # åˆ†æç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_samples': total_samples,
            'analyzed_samples': len(sample_data),
            'vocab_sizes': [],
            'augmentation_counts': [],
            'product_lengths': [],
            'synthon_lengths': [],
            'reactant_lengths': [],
            'non_empty_vocabs': 0,
            'all_vocabs': set()
        }
        
        print("ğŸ”„ æ­£åœ¨åˆ†ææ•°æ®ç‰¹å¾...")
        
        for i, entry in enumerate(sample_data):
            if i % 1000 == 0 and i > 0:
                print(f"   è¿›åº¦: {i}/{len(sample_data)}")
            
            try:
                # åˆ†ævocab
                vocab = entry.get('vocab', set())
                if not isinstance(vocab, set):
                    vocab = set(vocab) if vocab else set()
                
                vocab_size = len(vocab)
                stats['vocab_sizes'].append(vocab_size)
                
                if vocab_size > 0:
                    stats['non_empty_vocabs'] += 1
                    stats['all_vocabs'].update(vocab)
                
                # åˆ†æå¢å¼ºæ•°æ®
                aug_data = entry.get('augmented_data', [])
                stats['augmentation_counts'].append(len(aug_data))
                
                # åˆ†æSMILESé•¿åº¦
                pretrain_info = entry.get('pretrain_info', {})
                if pretrain_info:
                    product_smiles = pretrain_info.get('product_smiles', '')
                    synthon_smiles = pretrain_info.get('synthon_smiles', '')
                    reactant_smiles = pretrain_info.get('reactant_smiles', '')
                    
                    stats['product_lengths'].append(len(product_smiles))
                    stats['synthon_lengths'].append(len(synthon_smiles))
                    stats['reactant_lengths'].append(len(reactant_smiles))
                
            except Exception as e:
                print(f"   âš ï¸ åˆ†æç¬¬{i}æ¡æ•°æ®æ—¶å‡ºé”™: {e}")
                continue
        
        # ç«‹å³é‡Šæ”¾å¤§æ•°æ®å¯¹è±¡
        del data
        del sample_data
        gc.collect()
        
        print(f"âœ… åˆ†æå®Œæˆ")
        return stats
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        return None

def analyze_original_data_efficient(data_path, max_lines=20000):
    """
    é«˜æ•ˆåˆ†æåŸå§‹æ•°æ®çš„é”™è¯¯æ¨¡å¼
    """
    print(f"\nğŸ” é«˜æ•ˆåˆ†æåŸå§‹æ•°æ®: {data_path}")
    
    if not os.path.exists(data_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return None
    
    file_size = os.path.getsize(data_path) / 1024**2
    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.1f} MB")
    
    try:
        # å…ˆè¯»å–å°æ ·æœ¬æ£€æŸ¥æ ¼å¼
        print("ğŸ” æ£€æŸ¥æ•°æ®æ ¼å¼...")
        sample_df = pd.read_csv(data_path, nrows=100)
        columns = sample_df.columns.tolist()
        print(f"ğŸ“‹ åˆ—å: {columns}")
        
        # ç¡®å®šååº”åˆ—å
        rxn_col = None
        if 'rxn_smiles' in columns:
            rxn_col = 'rxn_smiles'
        elif len(columns) >= 2:
            rxn_col = columns[1]  # é€šå¸¸ç¬¬äºŒåˆ—æ˜¯ååº”
        else:
            print("âŒ æ— æ³•ç¡®å®šååº”SMILESåˆ—")
            return None
        
        print(f"âœ… ä½¿ç”¨ååº”åˆ—: {rxn_col}")
        
        # è¯»å–æŒ‡å®šæ•°é‡çš„æ•°æ®è¿›è¡Œåˆ†æ
        print(f"ğŸ“– è¯»å–å‰ {max_lines} è¡Œæ•°æ®è¿›è¡Œåˆ†æ...")
        df = pd.read_csv(data_path, nrows=max_lines)
        
        # è·å–æ€»è¡Œæ•°ï¼ˆä¸åŠ è½½å…¨éƒ¨æ•°æ®ï¼‰
        total_lines = sum(1 for line in open(data_path, 'r')) - 1  # å‡å»header
        print(f"ğŸ“Š æ€»æ•°æ®é‡: {total_lines:,} è¡Œ")
        print(f"ğŸ”¬ åˆ†ææ ·æœ¬: {len(df):,} è¡Œ ({len(df)/total_lines*100:.1f}%)")
        
        error_analysis = {
            'total_reactions': total_lines,
            'analyzed_reactions': len(df),
            'empty_reactions': 0,
            'invalid_format': 0,
            'no_products': 0,
            'no_reactants': 0,
            'complex_reactions': 0,
            'simple_reactions': 0,
            'reaction_lengths': [],
            'reactant_counts': [],
            'product_counts': []
        }
        
        print("ğŸ”„ æ­£åœ¨åˆ†æååº”è´¨é‡...")
        
        for idx, row in df.iterrows():
            if idx % 5000 == 0 and idx > 0:
                print(f"   è¿›åº¦: {idx}/{len(df)}")
            
            rxn_smiles = str(row.get(rxn_col, ''))
            
            if pd.isna(rxn_smiles) or rxn_smiles.strip() == '' or rxn_smiles == 'nan':
                error_analysis['empty_reactions'] += 1
                continue
            
            error_analysis['reaction_lengths'].append(len(rxn_smiles))
            
            if '>>' not in rxn_smiles:
                error_analysis['invalid_format'] += 1
                continue
            
            try:
                parts = rxn_smiles.split('>')
                if len(parts) != 3:
                    error_analysis['invalid_format'] += 1
                    continue
                
                reactants = parts[0].strip()
                products = parts[2].strip()
                
                if not reactants:
                    error_analysis['no_reactants'] += 1
                    continue
                
                if not products:
                    error_analysis['no_products'] += 1
                    continue
                
                # ç»Ÿè®¡ååº”ç‰©å’Œäº§ç‰©æ•°é‡
                reactant_count = len(reactants.split('.')) if reactants else 0
                product_count = len(products.split('.')) if products else 0
                
                error_analysis['reactant_counts'].append(reactant_count)
                error_analysis['product_counts'].append(product_count)
                
                if reactant_count > 2 or product_count > 1:
                    error_analysis['complex_reactions'] += 1
                else:
                    error_analysis['simple_reactions'] += 1
                    
            except Exception:
                error_analysis['invalid_format'] += 1
        
        print(f"âœ… åŸå§‹æ•°æ®åˆ†æå®Œæˆ")
        return error_analysis
        
    except Exception as e:
        print(f"âŒ åˆ†æåŸå§‹æ•°æ®å¤±è´¥: {e}")
        return None

def calculate_statistics(data_list):
    """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
    if not data_list:
        return {"mean": 0, "std": 0, "min": 0, "max": 0, "median": 0}
    
    return {
        "mean": np.mean(data_list),
        "std": np.std(data_list),
        "min": np.min(data_list),
        "max": np.max(data_list),
        "median": np.median(data_list)
    }

def generate_optimized_report(stats, error_analysis, output_dir):
    """
    ç”Ÿæˆä¼˜åŒ–çš„æ•°æ®è´¨é‡æŠ¥å‘Š
    """
    os.makedirs(output_dir, exist_ok=True)
    
    report_path = os.path.join(output_dir, "data_quality_report.txt")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("           é¢„è®­ç»ƒæ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š (ä¼˜åŒ–ç‰ˆ)\n")
        f.write("=" * 80 + "\n\n")
        
        # å¤„ç†åæ•°æ®åˆ†æ
        if stats:
            f.write("ğŸ“Š å¤„ç†åæ•°æ®ç»Ÿè®¡:\n")
            f.write("-" * 60 + "\n")
            f.write(f"æ€»æ ·æœ¬æ•°: {stats['total_samples']:,}\n")
            f.write(f"åˆ†ææ ·æœ¬æ•°: {stats['analyzed_samples']:,}\n")
            
            # è¯æ±‡è¡¨åˆ†æ
            if stats['vocab_sizes']:
                vocab_stats = calculate_statistics(stats['vocab_sizes'])
                f.write(f"\nğŸ“ è¯æ±‡è¡¨åˆ†æ:\n")
                f.write(f"  å¹³å‡å¤§å°: {vocab_stats['mean']:.1f} Â± {vocab_stats['std']:.1f}\n")
                f.write(f"  å¤§å°èŒƒå›´: {vocab_stats['min']:.0f} - {vocab_stats['max']:.0f}\n")
                f.write(f"  ä¸­ä½æ•°: {vocab_stats['median']:.1f}\n")
                f.write(f"  éç©ºè¯æ±‡è¡¨: {stats['non_empty_vocabs']}/{stats['analyzed_samples']} ")
                f.write(f"({stats['non_empty_vocabs']/stats['analyzed_samples']*100:.1f}%)\n")
                f.write(f"  ç´¯è®¡ä¸é‡å¤è¯æ±‡: {len(stats['all_vocabs'])}ä¸ª\n")
            
            # å¢å¼ºæ•°æ®åˆ†æ
            if stats['augmentation_counts']:
                aug_stats = calculate_statistics(stats['augmentation_counts'])
                f.write(f"\nğŸ¯ æ•°æ®å¢å¼ºåˆ†æ:\n")
                f.write(f"  å¹³å‡å¢å¼ºæ•°æ®é‡: {aug_stats['mean']:.1f}\n")
                f.write(f"  å¢å¼ºæ•°æ®èŒƒå›´: {aug_stats['min']:.0f} - {aug_stats['max']:.0f}\n")
            
            # SMILESé•¿åº¦åˆ†æ
            if stats['product_lengths']:
                prod_stats = calculate_statistics(stats['product_lengths'])
                f.write(f"\nğŸ§¬ åˆ†å­SMILESé•¿åº¦åˆ†æ:\n")
                f.write(f"  äº§ç‰©é•¿åº¦: {prod_stats['mean']:.1f} Â± {prod_stats['std']:.1f}\n")
                
            if stats['synthon_lengths']:
                syn_stats = calculate_statistics(stats['synthon_lengths'])
                f.write(f"  åˆæˆå­é•¿åº¦: {syn_stats['mean']:.1f} Â± {syn_stats['std']:.1f}\n")
                
            if stats['reactant_lengths']:
                react_stats = calculate_statistics(stats['reactant_lengths'])
                f.write(f"  ååº”ç‰©é•¿åº¦: {react_stats['mean']:.1f} Â± {react_stats['std']:.1f}\n")
            
            f.write("\n")
        
        # åŸå§‹æ•°æ®åˆ†æ
        if error_analysis:
            f.write("ğŸ“‹ åŸå§‹æ•°æ®è´¨é‡åˆ†æ:\n")
            f.write("-" * 60 + "\n")
            total = error_analysis['total_reactions']
            analyzed = error_analysis['analyzed_reactions']
            
            f.write(f"æ€»ååº”æ•°: {total:,}\n")
            f.write(f"åˆ†ææ ·æœ¬: {analyzed:,} ({analyzed/total*100:.1f}%)\n\n")
            
            f.write(f"ğŸ“Š é”™è¯¯ç»Ÿè®¡ (åŸºäºåˆ†ææ ·æœ¬):\n")
            f.write(f"  ç©ºååº”: {error_analysis['empty_reactions']:,} ({error_analysis['empty_reactions']/analyzed*100:.1f}%)\n")
            f.write(f"  æ ¼å¼é”™è¯¯: {error_analysis['invalid_format']:,} ({error_analysis['invalid_format']/analyzed*100:.1f}%)\n")
            f.write(f"  æ— äº§ç‰©: {error_analysis['no_products']:,} ({error_analysis['no_products']/analyzed*100:.1f}%)\n")
            f.write(f"  æ— ååº”ç‰©: {error_analysis['no_reactants']:,} ({error_analysis['no_reactants']/analyzed*100:.1f}%)\n")
            f.write(f"  å¤æ‚ååº”: {error_analysis['complex_reactions']:,} ({error_analysis['complex_reactions']/analyzed*100:.1f}%)\n")
            f.write(f"  ç®€å•ååº”: {error_analysis['simple_reactions']:,} ({error_analysis['simple_reactions']/analyzed*100:.1f}%)\n\n")
            
            if error_analysis['reactant_counts']:
                react_stats = calculate_statistics(error_analysis['reactant_counts'])
                f.write(f"âš—ï¸ ååº”å¤æ‚åº¦:\n")
                f.write(f"  å¹³å‡ååº”ç‰©æ•°é‡: {react_stats['mean']:.1f}\n")
                
            if error_analysis['product_counts']:
                prod_stats = calculate_statistics(error_analysis['product_counts'])
                f.write(f"  å¹³å‡äº§ç‰©æ•°é‡: {prod_stats['mean']:.1f}\n")
                
            if error_analysis['reaction_lengths']:
                len_stats = calculate_statistics(error_analysis['reaction_lengths'])
                f.write(f"  ååº”SMILESå¹³å‡é•¿åº¦: {len_stats['mean']:.1f} Â± {len_stats['std']:.1f}\n")
            
            f.write("\n")
        
        # è´¨é‡è¯„ä¼°
        f.write("ğŸ¯ æ•°æ®è´¨é‡è¯„ä¼°:\n")
        f.write("-" * 60 + "\n")
        
        if stats and error_analysis:
            success_rate = stats['total_samples'] / error_analysis['total_reactions']
            f.write(f"ğŸ“ˆ æ•°æ®å¤„ç†æˆåŠŸç‡: {success_rate*100:.1f}%\n")
            
            if success_rate > 0.7:
                f.write("âœ… ä¼˜ç§€: å¤„ç†æˆåŠŸç‡å¾ˆé«˜ï¼Œæ•°æ®è´¨é‡ä¼˜ç§€\n")
            elif success_rate > 0.5:
                f.write("âœ… è‰¯å¥½: å¤„ç†æˆåŠŸç‡å¯æ¥å—ï¼Œæ•°æ®è´¨é‡è‰¯å¥½\n")
            elif success_rate > 0.3:
                f.write("âš ï¸ ä¸€èˆ¬: å¤„ç†æˆåŠŸç‡ä¸­ç­‰ï¼Œå¯ä»¥ä½¿ç”¨ä½†éœ€å…³æ³¨\n")
            else:
                f.write("âŒ è¾ƒä½: å¤„ç†æˆåŠŸç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®å¤„ç†æµç¨‹\n")
        
        # è¯æ±‡è¡¨è´¨é‡è¯„ä¼°
        if stats and stats['all_vocabs']:
            vocab_quality = len(stats['all_vocabs'])
            if vocab_quality > 1000:
                f.write("âœ… è¯æ±‡è¡¨ä¸°å¯Œ: åŒ…å«è¶³å¤Ÿå¤šæ ·çš„åŒ–å­¦åŸºå›¢\n")
            elif vocab_quality > 500:
                f.write("âœ… è¯æ±‡è¡¨é€‚ä¸­: åŒ–å­¦åŸºå›¢æ•°é‡åˆç†\n")
            elif vocab_quality > 100:
                f.write("âš ï¸ è¯æ±‡è¡¨è¾ƒå°: åŒ–å­¦åŸºå›¢å¤šæ ·æ€§æœ‰é™\n")
            else:
                f.write("âŒ è¯æ±‡è¡¨è¿‡å°: å¯èƒ½å½±å“é¢„è®­ç»ƒæ•ˆæœ\n")
        
        # éç©ºè¯æ±‡è¡¨æ¯”ä¾‹è¯„ä¼°
        if stats and stats['analyzed_samples'] > 0:
            non_empty_ratio = stats['non_empty_vocabs'] / stats['analyzed_samples']
            if non_empty_ratio > 0.8:
                f.write("âœ… ååº”ä¸­å¿ƒè¯†åˆ«ç‡é«˜: å¤§éƒ¨åˆ†ååº”æˆåŠŸè¯†åˆ«ååº”ä¸­å¿ƒ\n")
            elif non_empty_ratio > 0.5:
                f.write("âœ… ååº”ä¸­å¿ƒè¯†åˆ«ç‡ä¸­ç­‰: å¤šæ•°ååº”è¯†åˆ«æˆåŠŸ\n")
            elif non_empty_ratio > 0.2:
                f.write("âš ï¸ ååº”ä¸­å¿ƒè¯†åˆ«ç‡è¾ƒä½: éœ€è¦å…³æ³¨å¤„ç†è´¨é‡\n")
            else:
                f.write("âŒ ååº”ä¸­å¿ƒè¯†åˆ«ç‡è¿‡ä½: å¤„ç†æµç¨‹å¯èƒ½æœ‰é—®é¢˜\n")
        
        f.write(f"\nğŸš€ ä½¿ç”¨å»ºè®®:\n")
        f.write(f"-" * 60 + "\n")
        
        if stats and stats['total_samples'] > 100000:
            f.write("âœ… æ•°æ®è§„æ¨¡å……è¶³ï¼Œå¯ä»¥å¼€å§‹é¢„è®­ç»ƒ\n")
        elif stats and stats['total_samples'] > 50000:
            f.write("âœ… æ•°æ®è§„æ¨¡é€‚ä¸­ï¼Œé€‚åˆé¢„è®­ç»ƒ\n")
        else:
            f.write("âš ï¸ æ•°æ®è§„æ¨¡è¾ƒå°ï¼Œå»ºè®®å¢åŠ æ•°æ®é‡\n")
        
        f.write("ğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:\n")
        f.write("  1. ä½¿ç”¨é¢„è®­ç»ƒæ•°æ®åŠ è½½å™¨éªŒè¯æ•°æ®å®Œæ•´æ€§\n")
        f.write("  2. è®¾è®¡å¹¶å®ç°G2Retro-Pé¢„è®­ç»ƒæ¨¡å‹\n")
        f.write("  3. å¼€å§‹å¤šä»»åŠ¡é¢„è®­ç»ƒå®éªŒ\n")
        f.write("  4. åœ¨USPTO-50Kä¸Šè¿›è¡Œå¾®è°ƒéªŒè¯\n")
    
    print(f"âœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    # ç”Ÿæˆç®€åŒ–æ‘˜è¦
    summary_path = os.path.join(output_dir, "quality_summary.txt")
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("æ•°æ®è´¨é‡æ‘˜è¦\n")
        f.write("=" * 30 + "\n\n")
        
        if stats:
            f.write(f"å¤„ç†åæ•°æ®: {stats['total_samples']:,} æ¡\n")
            if stats['vocab_sizes']:
                avg_vocab = np.mean(stats['vocab_sizes'])
                f.write(f"å¹³å‡è¯æ±‡è¡¨å¤§å°: {avg_vocab:.1f}\n")
            f.write(f"ç´¯è®¡è¯æ±‡æ•°: {len(stats['all_vocabs'])}\n")
            f.write(f"éç©ºè¯æ±‡è¡¨æ¯”ä¾‹: {stats['non_empty_vocabs']/stats['analyzed_samples']*100:.1f}%\n")
        
        if error_analysis:
            f.write(f"åŸå§‹æ•°æ®: {error_analysis['total_reactions']:,} æ¡\n")
            if stats and error_analysis:
                success_rate = stats['total_samples'] / error_analysis['total_reactions'] * 100
                f.write(f"å¤„ç†æˆåŠŸç‡: {success_rate:.1f}%\n")
        
        if stats and len(stats['all_vocabs']) > 0 and stats['non_empty_vocabs'] > 0:
            f.write("\nçŠ¶æ€: âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ç”¨äºé¢„è®­ç»ƒ\n")
        else:
            f.write("\nçŠ¶æ€: âš ï¸ æ•°æ®è´¨é‡éœ€è¦å…³æ³¨\n")
    
    print(f"âœ… æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_path}")

def main():
    parser = argparse.ArgumentParser(description="ä¼˜åŒ–ç‰ˆé¢„è®­ç»ƒæ•°æ®è´¨é‡åˆ†æ")
    parser.add_argument('--processed_data', type=str, help="å¤„ç†åçš„é¢„è®­ç»ƒæ•°æ®è·¯å¾„(.pkl)")
    parser.add_argument('--original_data', type=str, help="åŸå§‹æ•°æ®è·¯å¾„(.csv)")
    parser.add_argument('--output_dir', type=str, default="./quality_analysis/", help="è¾“å‡ºç›®å½•")
    parser.add_argument('--max_samples', type=int, default=5000, help="æœ€å¤§åˆ†ææ ·æœ¬æ•°")
    parser.add_argument('--max_lines', type=int, default=20000, help="åŸå§‹æ•°æ®æœ€å¤§åˆ†æè¡Œæ•°")
    
    args = parser.parse_args()
    
    print("ğŸ” ä¼˜åŒ–ç‰ˆé¢„è®­ç»ƒæ•°æ®è´¨é‡åˆ†æ")
    print("=" * 80)
    print("âš¡ é’ˆå¯¹å¤§æ–‡ä»¶å’Œç£ç›˜ç©ºé—´é™åˆ¶è¿›è¡Œäº†ä¼˜åŒ–")
    print("=" * 80)
    
    stats = None
    error_analysis = None
    
    # åˆ†æå¤„ç†åçš„æ•°æ®
    if args.processed_data and os.path.exists(args.processed_data):
        try:
            stats = analyze_pretrain_data_efficient(args.processed_data, args.max_samples)
            if stats:
                print("âœ… å¤„ç†åæ•°æ®åˆ†æå®Œæˆ")
            else:
                print("âŒ å¤„ç†åæ•°æ®åˆ†æå¤±è´¥")
        except Exception as e:
            print(f"âŒ åˆ†æå¤„ç†åæ•°æ®æ—¶å‡ºé”™: {e}")
    elif args.processed_data:
        print(f"âŒ å¤„ç†åæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.processed_data}")
    
    # åˆ†æåŸå§‹æ•°æ®
    if args.original_data and os.path.exists(args.original_data):
        try:
            error_analysis = analyze_original_data_efficient(args.original_data, args.max_lines)
            if error_analysis:
                print("âœ… åŸå§‹æ•°æ®åˆ†æå®Œæˆ")
            else:
                print("âŒ åŸå§‹æ•°æ®åˆ†æå¤±è´¥")
        except Exception as e:
            print(f"âŒ åˆ†æåŸå§‹æ•°æ®æ—¶å‡ºé”™: {e}")
    elif args.original_data:
        print(f"âŒ åŸå§‹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.original_data}")
    
    # ç”ŸæˆæŠ¥å‘Š
    if stats or error_analysis:
        try:
            generate_optimized_report(stats, error_analysis, args.output_dir)
            print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
            print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {args.output_dir}")
            print(f"ğŸ“„ æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š: {os.path.join(args.output_dir, 'data_quality_report.txt')}")
            print(f"ğŸ“‹ æŸ¥çœ‹æ‘˜è¦: {os.path.join(args.output_dir, 'quality_summary.txt')}")
        except Exception as e:
            print(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
    else:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®è¿›è¡Œåˆ†æ")
        print("ğŸ’¡ è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")

if __name__ == "__main__":
    main()

def analyze_pretrain_data(data_path):
    """
    åˆ†æé¢„è®­ç»ƒæ•°æ®çš„è´¨é‡
    """
    print(f"åˆ†æé¢„è®­ç»ƒæ•°æ®: {data_path}")
    
    with open(data_path, 'rb') as f:
        data = pickle.load(f)
    
    print(f"æ€»æ•°æ®é‡: {len(data)} æ¡")
    
    # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_samples': len(data),
        'product_smiles': [],
        'synthon_smiles': [],
        'reactant_smiles': [],
        'vocab_sizes': [],
        'augmentation_counts': [],
        'molecular_weights': [],
        'num_atoms': [],
        'num_bonds': [],
        'reaction_center_types': {'bf': 0, 'bc': 0, 'a': 0}
    }
    
    # åˆ†ææ¯ä¸ªæ ·æœ¬
    for entry in data:
        pretrain_info = entry['pretrain_info']
        
        stats['product_smiles'].append(pretrain_info['product_smiles'])
        stats['synthon_smiles'].append(pretrain_info['synthon_smiles'])
        stats['reactant_smiles'].append(pretrain_info['reactant_smiles'])
        stats['vocab_sizes'].append(pretrain_info.get('vocab_size', 0))
        stats['augmentation_counts'].append(len(entry['augmented_data']))
        
        # åˆ†æäº§ç‰©åˆ†å­å±æ€§
        try:
            mol = Chem.MolFromSmiles(pretrain_info['product_smiles'])
            if mol:
                stats['molecular_weights'].append(Descriptors.MolWt(mol))
                stats['num_atoms'].append(mol.GetNumAtoms())
                stats['num_bonds'].append(mol.GetNumBonds())
        except:
            stats['molecular_weights'].append(0)
            stats['num_atoms'].append(0)
            stats['num_bonds'].append(0)
    
    return stats

def analyze_error_patterns(original_data_path, processed_data_path=None):
    """
    åˆ†ææ•°æ®å¤„ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯æ¨¡å¼
    """
    print(f"åˆ†æåŸå§‹æ•°æ®: {original_data_path}")
    
    # è¯»å–åŸå§‹æ•°æ®
    if original_data_path.endswith('.csv'):
        df = pd.read_csv(original_data_path)
        if 'reactants>reagents>production' in df.columns:
            df = df.rename(columns={'reactants>reagents>production': 'rxn_smiles'})
    else:
        print("ä¸æ”¯æŒçš„åŸå§‹æ•°æ®æ ¼å¼")
        return None
    
    print(f"åŸå§‹æ•°æ®é‡: {len(df)} æ¡")
    
    # åˆ†æååº”SMILESçš„åŸºæœ¬ç‰¹å¾
    error_analysis = {
        'total_reactions': len(df),
        'empty_reactions': 0,
        'invalid_format': 0,
        'no_products': 0,
        'no_reactants': 0,
        'complex_reactions': 0,  # å¤šä¸ªäº§ç‰©æˆ–ååº”ç‰©
        'simple_reactions': 0,   # å•ä¸ªäº§ç‰©å’Œååº”ç‰©
        'atom_counts': [],
        'reactant_counts': [],
        'product_counts': []
    }
    
    for idx, row in df.iterrows():
        rxn_smiles = str(row.get('rxn_smiles', ''))
        
        if pd.isna(rxn_smiles) or rxn_smiles.strip() == '':
            error_analysis['empty_reactions'] += 1
            continue
        
        if '>>' not in rxn_smiles:
            error_analysis['invalid_format'] += 1
            continue
        
        try:
            parts = rxn_smiles.split('>')
            if len(parts) != 3:
                error_analysis['invalid_format'] += 1
                continue
            
            reactants = parts[0].strip()
            products = parts[2].strip()
            
            if not reactants:
                error_analysis['no_reactants'] += 1
                continue
            
            if not products:
                error_analysis['no_products'] += 1
                continue
            
            # ç»Ÿè®¡ååº”ç‰©å’Œäº§ç‰©æ•°é‡
            reactant_count = len(reactants.split('.')) if reactants else 0
            product_count = len(products.split('.')) if products else 0
            
            error_analysis['reactant_counts'].append(reactant_count)
            error_analysis['product_counts'].append(product_count)
            
            if reactant_count > 2 or product_count > 1:
                error_analysis['complex_reactions'] += 1
            else:
                error_analysis['simple_reactions'] += 1
            
            # ä¼°ç®—åŸå­æ•°é‡
            try:
                total_atoms = 0
                for smi in reactants.split('.') + products.split('.'):
                    mol = Chem.MolFromSmiles(smi.strip())
                    if mol:
                        total_atoms += mol.GetNumAtoms()
                error_analysis['atom_counts'].append(total_atoms)
            except:
                error_analysis['atom_counts'].append(0)
                
        except Exception as e:
            error_analysis['invalid_format'] += 1
    
    return error_analysis

def generate_quality_report(stats, error_analysis, output_dir):
    """
    ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆæ–‡å­—æŠ¥å‘Š
    report_path = os.path.join(output_dir, "data_quality_report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=== é¢„è®­ç»ƒæ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š ===\n\n")
        
        if stats:
            f.write("1. æˆåŠŸå¤„ç†çš„æ•°æ®ç»Ÿè®¡:\n")
            f.write(f"   æ€»æ ·æœ¬æ•°: {stats['total_samples']}\n")
            f.write(f"   å¹³å‡è¯æ±‡è¡¨å¤§å°: {np.mean(stats['vocab_sizes']):.2f}\n")
            f.write(f"   å¹³å‡å¢å¼ºæ•°æ®é‡: {np.mean(stats['augmentation_counts']):.2f}\n")
            f.write(f"   å¹³å‡åˆ†å­é‡: {np.mean(stats['molecular_weights']):.2f}\n")
            f.write(f"   å¹³å‡åŸå­æ•°: {np.mean(stats['num_atoms']):.2f}\n")
            f.write(f"   å¹³å‡é”®æ•°: {np.mean(stats['num_bonds']):.2f}\n\n")
        
        if error_analysis:
            f.write("2. åŸå§‹æ•°æ®é”™è¯¯åˆ†æ:\n")
            total = error_analysis['total_reactions']
            f.write(f"   æ€»ååº”æ•°: {total}\n")
            f.write(f"   ç©ºååº”: {error_analysis['empty_reactions']} ({error_analysis['empty_reactions']/total*100:.1f}%)\n")
            f.write(f"   æ ¼å¼é”™è¯¯: {error_analysis['invalid_format']} ({error_analysis['invalid_format']/total*100:.1f}%)\n")
            f.write(f"   æ— äº§ç‰©: {error_analysis['no_products']} ({error_analysis['no_products']/total*100:.1f}%)\n")
            f.write(f"   æ— ååº”ç‰©: {error_analysis['no_reactants']} ({error_analysis['no_reactants']/total*100:.1f}%)\n")
            f.write(f"   å¤æ‚ååº”: {error_analysis['complex_reactions']} ({error_analysis['complex_reactions']/total*100:.1f}%)\n")
            f.write(f"   ç®€å•ååº”: {error_analysis['simple_reactions']} ({error_analysis['simple_reactions']/total*100:.1f}%)\n\n")
            
            if error_analysis['reactant_counts']:
                f.write(f"   å¹³å‡ååº”ç‰©æ•°é‡: {np.mean(error_analysis['reactant_counts']):.2f}\n")
                f.write(f"   å¹³å‡äº§ç‰©æ•°é‡: {np.mean(error_analysis['product_counts']):.2f}\n")
                f.write(f"   å¹³å‡åŸå­æ€»æ•°: {np.mean(error_analysis['atom_counts']):.2f}\n\n")
        
        f.write("3. æ•°æ®è´¨é‡å»ºè®®:\n")
        
        if stats and stats['total_samples'] > 0:
            success_indicators = []
            if np.mean(stats['vocab_sizes']) > 5:
                success_indicators.append("âœ“ è¯æ±‡è¡¨å¤§å°æ­£å¸¸")
            if np.mean(stats['num_atoms']) > 10:
                success_indicators.append("âœ“ åˆ†å­å¤æ‚åº¦é€‚ä¸­")
            if np.mean(stats['augmentation_counts']) >= 2:
                success_indicators.append("âœ“ æ•°æ®å¢å¼ºæ­£å¸¸")
            
            for indicator in success_indicators:
                f.write(f"   {indicator}\n")
        
        if error_analysis:
            total = error_analysis['total_reactions']
            if error_analysis['invalid_format'] / total > 0.1:
                f.write("   âš  æ ¼å¼é”™è¯¯ç‡è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®é¢„å¤„ç†\n")
            if error_analysis['complex_reactions'] / total > 0.5:
                f.write("   âš  å¤æ‚ååº”è¾ƒå¤šï¼Œå¯èƒ½å½±å“å¤„ç†æˆåŠŸç‡\n")
            if len(error_analysis['atom_counts']) > 0 and np.mean(error_analysis['atom_counts']) > 100:
                f.write("   âš  åˆ†å­è¿‡äºå¤æ‚ï¼Œå¯èƒ½å¯¼è‡´å¤„ç†å›°éš¾\n")
        
        f.write("\n4. æ”¹è¿›å»ºè®®:\n")
        f.write("   - è¿‡æ»¤æ‰æ ¼å¼ä¸æ­£ç¡®çš„ååº”\n")
        f.write("   - ä¼˜å…ˆå¤„ç†ç®€å•ååº”ï¼ˆå•ä¸€äº§ç‰©ï¼‰\n")
        f.write("   - å¯¹å¤æ‚åˆ†å­è¿›è¡Œé¢„ç­›é€‰\n")
        f.write("   - æé«˜atom mappingè´¨é‡\n")
    
    print(f"è´¨é‡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        plt.style.use('default')
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        fig.suptitle('æ•°æ®è´¨é‡åˆ†æå›¾è¡¨', fontsize=16)
        
        if stats and len(stats['molecular_weights']) > 0:
            # åˆ†å­é‡åˆ†å¸ƒ
            axes[0, 0].hist(stats['molecular_weights'], bins=50, alpha=0.7, color='blue')
            axes[0, 0].set_title('åˆ†å­é‡åˆ†å¸ƒ')
            axes[0, 0].set_xlabel('åˆ†å­é‡')
            axes[0, 0].set_ylabel('é¢‘æ•°')
            
            # åŸå­æ•°åˆ†å¸ƒ
            axes[0, 1].hist(stats['num_atoms'], bins=30, alpha=0.7, color='green')
            axes[0, 1].set_title('åŸå­æ•°åˆ†å¸ƒ')
            axes[0, 1].set_xlabel('åŸå­æ•°')
            axes[0, 1].set_ylabel('é¢‘æ•°')
            
            # è¯æ±‡è¡¨å¤§å°åˆ†å¸ƒ
            axes[0, 2].hist(stats['vocab_sizes'], bins=20, alpha=0.7, color='orange')
            axes[0, 2].set_title('è¯æ±‡è¡¨å¤§å°åˆ†å¸ƒ')
            axes[0, 2].set_xlabel('è¯æ±‡è¡¨å¤§å°')
            axes[0, 2].set_ylabel('é¢‘æ•°')
        
        if error_analysis and len(error_analysis['reactant_counts']) > 0:
            # ååº”ç‰©æ•°é‡åˆ†å¸ƒ
            reactant_counter = Counter(error_analysis['reactant_counts'])
            axes[1, 0].bar(reactant_counter.keys(), reactant_counter.values(), alpha=0.7, color='red')
            axes[1, 0].set_title('ååº”ç‰©æ•°é‡åˆ†å¸ƒ')
            axes[1, 0].set_xlabel('ååº”ç‰©æ•°é‡')
            axes[1, 0].set_ylabel('ååº”æ•°')
            
            # äº§ç‰©æ•°é‡åˆ†å¸ƒ
            product_counter = Counter(error_analysis['product_counts'])
            axes[1, 1].bar(product_counter.keys(), product_counter.values(), alpha=0.7, color='purple')
            axes[1, 1].set_title('äº§ç‰©æ•°é‡åˆ†å¸ƒ')
            axes[1, 1].set_xlabel('äº§ç‰©æ•°é‡')
            axes[1, 1].set_ylabel('ååº”æ•°')
            
            # é”™è¯¯ç±»å‹åˆ†å¸ƒ
            error_types = ['ç©ºååº”', 'æ ¼å¼é”™è¯¯', 'æ— äº§ç‰©', 'æ— ååº”ç‰©']
            error_counts = [
                error_analysis['empty_reactions'],
                error_analysis['invalid_format'],
                error_analysis['no_products'],
                error_analysis['no_reactants']
            ]
            axes[1, 2].bar(error_types, error_counts, alpha=0.7, color='gray')
            axes[1, 2].set_title('é”™è¯¯ç±»å‹ç»Ÿè®¡')
            axes[1, 2].set_ylabel('é”™è¯¯æ•°é‡')
            axes[1, 2].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plot_path = os.path.join(output_dir, "data_quality_plots.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"è´¨é‡åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {plot_path}")
        
    except ImportError:
        print("matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
    except Exception as e:
        print(f"ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {e}")

def main():
    parser = argparse.ArgumentParser(description="åˆ†æé¢„è®­ç»ƒæ•°æ®è´¨é‡")
    parser.add_argument('--processed_data', type=str, help="å¤„ç†åçš„é¢„è®­ç»ƒæ•°æ®è·¯å¾„(.pkl)")
    parser.add_argument('--original_data', type=str, help="åŸå§‹æ•°æ®è·¯å¾„(.csv)")
    parser.add_argument('--output_dir', type=str, default="./quality_analysis/", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    print("=== é¢„è®­ç»ƒæ•°æ®è´¨é‡åˆ†æ ===\n")
    
    stats = None
    error_analysis = None
    
    # åˆ†æå¤„ç†åçš„æ•°æ®
    if args.processed_data and os.path.exists(args.processed_data):
        try:
            stats = analyze_pretrain_data(args.processed_data)
            print("âœ“ æˆåŠŸåˆ†æå¤„ç†åçš„æ•°æ®")
        except Exception as e:
            print(f"âœ— åˆ†æå¤„ç†åæ•°æ®å¤±è´¥: {e}")
    
    # åˆ†æåŸå§‹æ•°æ®çš„é”™è¯¯æ¨¡å¼
    if args.original_data and os.path.exists(args.original_data):
        try:
            error_analysis = analyze_error_patterns(args.original_data)
            print("âœ“ æˆåŠŸåˆ†æåŸå§‹æ•°æ®é”™è¯¯æ¨¡å¼")
        except Exception as e:
            print(f"âœ— åˆ†æåŸå§‹æ•°æ®å¤±è´¥: {e}")
    
    # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
    if stats or error_analysis:
        try:
            generate_quality_report(stats, error_analysis, args.output_dir)
            print("âœ“ æˆåŠŸç”Ÿæˆè´¨é‡æŠ¥å‘Š")
        except Exception as e:
            print(f"âœ— ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
    else:
        print("âœ— æ²¡æœ‰æœ‰æ•ˆæ•°æ®è¿›è¡Œåˆ†æ")
    
    # è¾“å‡ºå…³é”®å»ºè®®
    print("\n=== å…³é”®å»ºè®® ===")
    
    if error_analysis:
        total = error_analysis['total_reactions']
        format_error_rate = error_analysis['invalid_format'] / total
        complex_rate = error_analysis['complex_reactions'] / total
        
        print(f"åŸå§‹æ•°æ®æ ¼å¼é”™è¯¯ç‡: {format_error_rate*100:.1f}%")
        print(f"å¤æ‚ååº”æ¯”ä¾‹: {complex_rate*100:.1f}%")
        
        if format_error_rate > 0.05:
            print("âš  å»ºè®®: æ ¼å¼é”™è¯¯ç‡è¾ƒé«˜ï¼Œéœ€è¦æ”¹è¿›æ•°æ®é¢„å¤„ç†")
        
        if complex_rate > 0.3:
            print("âš  å»ºè®®: å¤æ‚ååº”è¾ƒå¤šï¼Œè€ƒè™‘åˆ†æ‰¹å¤„ç†æˆ–é¢„ç­›é€‰")
    
    if stats:
        success_rate = stats['total_samples']
        if args.original_data and error_analysis:
            success_rate = stats['total_samples'] / error_analysis['total_reactions']
            print(f"æ•°æ®å¤„ç†æˆåŠŸç‡: {success_rate*100:.1f}%")
            
            if success_rate < 0.5:
                print("âš  å»ºè®®: å¤„ç†æˆåŠŸç‡è¾ƒä½ï¼Œéœ€è¦ä¼˜åŒ–å¤„ç†æµç¨‹")
            elif success_rate < 0.8:
                print("âš  å»ºè®®: å¤„ç†æˆåŠŸç‡ä¸­ç­‰ï¼Œå¯è¿›ä¸€æ­¥ä¼˜åŒ–")
            else:
                print("âœ“ æ•°æ®å¤„ç†æˆåŠŸç‡è‰¯å¥½")
    
    print(f"\nè¯¦ç»†åˆ†æç»“æœè¯·æŸ¥çœ‹: {args.output_dir}")

if __name__ == "__main__":
    main()
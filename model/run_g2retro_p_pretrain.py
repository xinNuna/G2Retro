#!/usr/bin/env python3
"""
G2Retro-P å¤šä»»åŠ¡é¢„è®­ç»ƒå¯åŠ¨è„šæœ¬
åŸºäºè®¾è®¡æ–¹æ¡ˆ + RetroExplaineråŠ¨æ€æƒé‡è°ƒæ•´

ä½¿ç”¨æ–¹æ³•:
1. å¿«é€Ÿå¯åŠ¨ï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼‰:
   cd model && python run_g2retro_p_pretrain.py --mode demo

2. å®Œæ•´é¢„è®­ç»ƒ:
   cd model && python run_g2retro_p_pretrain.py --mode full

3. è‡ªå®šä¹‰é…ç½®:
   cd model && python run_g2retro_p_pretrain.py --epochs 100 --batch_size 8 --learning_rate 1e-4
"""

import os
import sys
import argparse
import torch
import numpy as np
import random
from datetime import datetime

# å¯¼å…¥è®­ç»ƒæ¨¡å—ï¼ˆåœ¨modelç›®å½•ä¸­ï¼‰
try:
    from train_g2retro_p_design_aligned import (
        G2RetroPDesignAlignedModel, 
        G2RetroPDesignAlignedTrainer,
        G2RetroPDesignAlignedDataset,
        g2retro_design_aligned_collate_fn
    )
    from torch.utils.data import DataLoader
    print("âœ“ æˆåŠŸå¯¼å…¥G2Retro-Pè®­ç»ƒæ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿åœ¨modelç›®å½•ä¸‹è¿è¡Œ: cd model && python run_g2retro_p_pretrain.py")
    sys.exit(1)

def setup_device():
    """è®¾ç½®è®¾å¤‡"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"âœ“ ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        print(f"âœ“ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        device = torch.device('cpu')
        print("âš ï¸  ä½¿ç”¨CPUï¼ˆå»ºè®®ä½¿ç”¨GPUåŠ é€Ÿï¼‰")
    return device

def check_data_files():
    """æ£€æŸ¥æ•°æ®æ–‡ä»¶"""
    files = {
        'train_data': '../data/pretrain/pretrain_tensors_train.pkl',
        'valid_data': '../data/pretrain/pretrain_tensors_valid.pkl', 
        'vocab': '../data/pretrain/vocab_train.txt'
    }
    
    print("\næ£€æŸ¥æ•°æ®æ–‡ä»¶:")
    all_exist = True
    for name, path in files.items():
        if os.path.exists(path):
            size = os.path.getsize(path) / (1024**3)  # GB
            print(f"âœ“ {name}: {path} ({size:.1f} GB)")
        else:
            print(f"âŒ {name}: {path} - æ–‡ä»¶ä¸å­˜åœ¨")
            all_exist = False
    
    return all_exist, files

class PretrainConfig:
    """é¢„è®­ç»ƒé…ç½®ç±»"""
    def __init__(self, mode='demo'):
        # æ¨¡å‹å‚æ•°ï¼ˆä¸G2Retroä¿æŒä¸€è‡´ï¼‰
        self.hidden_size = 300
        self.latent_size = 300
        self.depthT = 3
        self.depthG = 3
        self.use_feature = True
        self.use_tree = True
        self.network_type = "gru"
        self.use_node_embed = False
        self.use_class = False
        self.use_atomic = False
        self.sum_pool = False
        self.use_brics = False
        self.use_latent_attachatom = False
        self.use_mess = True
        
        # æ ¹æ®æ¨¡å¼è®¾ç½®è®­ç»ƒå‚æ•°
        if mode == 'demo':
            self._setup_demo_config()
        elif mode == 'full':
            self._setup_full_config()
        else:
            self._setup_custom_config()
            
        # åŠ¨æ€æƒé‡é€‚åº”å‚æ•°ï¼ˆåŸºäºRetroExplainer DAMTï¼‰
        self.reset_weights_per_epoch = False
        self.weight_update_frequency = 10
        self.weight_temperature = 2.0
        self.loss_queue_length = 50
        self.min_task_weight = 0.01
        self.max_task_weight = 3.0
        
        # è¾“å‡ºè·¯å¾„ï¼ˆç›¸å¯¹äºmodelç›®å½•ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.save_dir = f"checkpoints_g2retro_p_{mode}_{timestamp}/"
        
    def _setup_demo_config(self):
        """æ¼”ç¤ºæ¨¡å¼é…ç½®ï¼ˆå¿«é€ŸéªŒè¯ï¼‰"""
        print("ğŸ“‹ é…ç½®æ¨¡å¼: æ¼”ç¤ºæ¨¡å¼ï¼ˆå¿«é€ŸéªŒè¯ï¼‰")
        self.batch_size = 2
        self.learning_rate = 1e-4
        self.weight_decay = 1e-5
        self.epochs = 3
        self.step_size = 2
        self.gamma = 0.7
        self.patience = 2
        self.max_train_samples = 50
        self.max_val_samples = 10
        
    def _setup_full_config(self):
        """å®Œæ•´é¢„è®­ç»ƒé…ç½®"""
        print("ğŸ“‹ é…ç½®æ¨¡å¼: å®Œæ•´é¢„è®­ç»ƒ")
        self.batch_size = 8
        self.learning_rate = 1e-4
        self.weight_decay = 1e-5
        self.epochs = 100
        self.step_size = 20
        self.gamma = 0.8
        self.patience = 10
        self.max_train_samples = None  # ä½¿ç”¨å…¨éƒ¨æ•°æ®
        self.max_val_samples = None
        
    def _setup_custom_config(self):
        """è‡ªå®šä¹‰é…ç½®ï¼ˆé»˜è®¤ä¸­ç­‰è§„æ¨¡ï¼‰"""
        print("ğŸ“‹ é…ç½®æ¨¡å¼: è‡ªå®šä¹‰é…ç½®")
        self.batch_size = 4
        self.learning_rate = 1e-4
        self.weight_decay = 1e-5
        self.epochs = 50
        self.step_size = 10
        self.gamma = 0.9
        self.patience = 5
        self.max_train_samples = 10000
        self.max_val_samples = 1000

def create_datasets_and_loaders(config, data_files):
    """åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨"""
    print(f"\nåˆ›å»ºæ•°æ®é›†...")
    
    # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
    train_dataset = G2RetroPDesignAlignedDataset(
        data_files['train_data'], 
        data_files['vocab'], 
        max_samples=config.max_train_samples
    )
    
    # åˆ›å»ºéªŒè¯æ•°æ®é›†  
    val_dataset = G2RetroPDesignAlignedDataset(
        data_files['valid_data'],
        data_files['vocab'],
        max_samples=config.max_val_samples
    )
    
    print(f"âœ“ è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"âœ“ éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    print(f"âœ“ è¯æ±‡è¡¨å¤§å°: {train_dataset.vocab.size()}")
    print(f"âœ“ åŸå­è¯æ±‡è¡¨å¤§å°: {train_dataset.avocab.size()}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=lambda batch: g2retro_design_aligned_collate_fn(batch, train_dataset.vocab, train_dataset.avocab),
        num_workers=2,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=lambda batch: g2retro_design_aligned_collate_fn(batch, val_dataset.vocab, val_dataset.avocab),
        num_workers=2,
        pin_memory=True
    )
    
    print(f"âœ“ è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"âœ“ éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    
    return train_dataset, val_dataset, train_loader, val_loader

def run_pretrain(config):
    """è¿è¡Œé¢„è®­ç»ƒ"""
    print("\n" + "="*80)
    print("ğŸš€ G2Retro-P å¤šä»»åŠ¡é¢„è®­ç»ƒå¯åŠ¨")
    print("ğŸ“ è®¾è®¡æ–¹æ¡ˆå®Œå…¨å¯¹é½ + RetroExplaineråŠ¨æ€æƒé‡è°ƒæ•´")
    print("="*80)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
    
    # è®¾ç½®è®¾å¤‡
    device = setup_device()
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    data_available, data_files = check_data_files()
    if not data_available:
        print("âŒ æ•°æ®æ–‡ä»¶ç¼ºå¤±ï¼Œæ— æ³•å¯åŠ¨è®­ç»ƒ")
        print("ğŸ’¡ æç¤ºï¼šè¯·ç¡®ä¿å·²è¿è¡Œé¢„å¤„ç†è„šæœ¬ç”Ÿæˆæ•°æ®æ–‡ä»¶")
        return False
    
    # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
    train_dataset, val_dataset, train_loader, val_loader = create_datasets_and_loaders(config, data_files)
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\nåˆ›å»ºG2Retro-Pæ¨¡å‹...")
    model = G2RetroPDesignAlignedModel(train_dataset.vocab, train_dataset.avocab, config)
    model = model.to(device)
    
    # æ‰“å°æ¨¡å‹ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nğŸ“Š æ¨¡å‹ç»Ÿè®¡:")
    print(f"å‚æ•°æ€»æ•°: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"æ¨¡å‹å¤§å°: ~{total_params * 4 / 1e6:.1f} MB")
    
    # æ˜¾ç¤ºæ ¸å¿ƒæ¶æ„
    print(f"\nğŸ—ï¸  æ ¸å¿ƒæ¶æ„éªŒè¯:")
    print(f"âœ“ å…±äº«ç¼–ç å™¨: GMPNï¼ˆéšè—ç»´åº¦:{config.hidden_size}ï¼‰")
    print(f"âœ“ åŸºç¡€ä»»åŠ¡å¤´: G2Retroååº”ä¸­å¿ƒè¯†åˆ«")
    print(f"âœ“ åˆ†å­æ¢å¤å¤´: MolCLRå›¾ç»“æ„å¢å¼º")
    print(f"âœ“ å¯¹æ¯”å­¦ä¹ å¤´: äº§ç‰©-åˆæˆå­å¯¹æ¯”")
    print(f"âœ“ åŠ¨æ€æƒé‡: RetroExplainer DAMTç®—æ³•")
    print(f"âœ“ æƒé‡æ›´æ–°é¢‘ç‡: æ¯{config.weight_update_frequency}æ­¥")
    print(f"âœ“ æ¸©åº¦ç³»æ•°: {config.weight_temperature}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = G2RetroPDesignAlignedTrainer(model, train_loader, val_loader, config)
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ¯ å¼€å§‹é¢„è®­ç»ƒ...")
    print(f"Epochs: {config.epochs}")
    print(f"Batch size: {config.batch_size}")
    print(f"Learning rate: {config.learning_rate}")
    print(f"ä¿å­˜è·¯å¾„: {config.save_dir}")
    
    try:
        trainer.train()
        print(f"\nğŸ‰ é¢„è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {config.save_dir}")
        return True
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='G2Retro-På¤šä»»åŠ¡é¢„è®­ç»ƒ')
    
    # é¢„è®¾æ¨¡å¼
    parser.add_argument('--mode', choices=['demo', 'full', 'custom'], default='demo',
                        help='é¢„è®­ç»ƒæ¨¡å¼: demo(æ¼”ç¤º), full(å®Œæ•´), custom(è‡ªå®šä¹‰)')
    
    # è‡ªå®šä¹‰å‚æ•°
    parser.add_argument('--epochs', type=int, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, help='å­¦ä¹ ç‡')
    parser.add_argument('--max_train_samples', type=int, help='æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°')
    parser.add_argument('--max_val_samples', type=int, help='æœ€å¤§éªŒè¯æ ·æœ¬æ•°')
    
    # åŠ¨æ€æƒé‡å‚æ•°
    parser.add_argument('--weight_temperature', type=float, default=2.0, help='æƒé‡æ¸©åº¦ç³»æ•°')
    parser.add_argument('--weight_update_frequency', type=int, default=10, help='æƒé‡æ›´æ–°é¢‘ç‡')
    parser.add_argument('--reset_weights_per_epoch', action='store_true', help='æ¯epoché‡ç½®æƒé‡')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = PretrainConfig(mode=args.mode)
    
    # åº”ç”¨è‡ªå®šä¹‰å‚æ•°
    if args.epochs is not None:
        config.epochs = args.epochs
    if args.batch_size is not None:
        config.batch_size = args.batch_size
    if args.learning_rate is not None:
        config.learning_rate = args.learning_rate
    if args.max_train_samples is not None:
        config.max_train_samples = args.max_train_samples
    if args.max_val_samples is not None:
        config.max_val_samples = args.max_val_samples
    
    # åº”ç”¨åŠ¨æ€æƒé‡å‚æ•°
    config.weight_temperature = args.weight_temperature
    config.weight_update_frequency = args.weight_update_frequency
    config.reset_weights_per_epoch = args.reset_weights_per_epoch
    
    # æ˜¾ç¤ºé…ç½®
    print("=" * 50)
    print("ğŸ”§ é¢„è®­ç»ƒé…ç½®:")
    print("=" * 50)
    print(f"æ¨¡å¼: {args.mode}")
    print(f"Epochs: {config.epochs}")
    print(f"Batch size: {config.batch_size}")
    print(f"Learning rate: {config.learning_rate}")
    if config.max_train_samples:
        print(f"æœ€å¤§è®­ç»ƒæ ·æœ¬: {config.max_train_samples:,}")
    if config.max_val_samples:
        print(f"æœ€å¤§éªŒè¯æ ·æœ¬: {config.max_val_samples:,}")
    print(f"æƒé‡æ¸©åº¦: {config.weight_temperature}")
    print(f"æƒé‡æ›´æ–°é¢‘ç‡: {config.weight_update_frequency}")
    print(f"æ¯epoché‡ç½®æƒé‡: {config.reset_weights_per_epoch}")
    print("=" * 50)
    
    # è¿è¡Œé¢„è®­ç»ƒ
    success = run_pretrain(config)
    
    if success:
        print("\nğŸ¯ é¢„è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("ğŸ’¡ ä¸‹ä¸€æ­¥å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œfine-tuning")
    else:
        print("\nâŒ é¢„è®­ç»ƒå¤±è´¥")
        sys.exit(1)

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SynthRetro-P é¢„è®­ç»ƒæ¨¡å‹å®ç° - åŸºäºå¤šä»»åŠ¡é¢„è®­ç»ƒçš„åŠæ¨¡æ¿é€†åˆæˆæ¨¡å‹

è¿™æ˜¯ä¸€ä¸ªæ–°å¢çš„æ–‡ä»¶ï¼Œåº”è¯¥æ”¾åœ¨ model/ ç›®å½•ä¸‹ï¼Œå‘½åä¸º synthretro_pretrain.py
å®ç°è®¾è®¡æ–¹æ¡ˆä¸­çš„å¤šä»»åŠ¡é¢„è®­ç»ƒæ¶æ„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional
import math

class GMPN(nn.Module):
    """
    å›¾æ¶ˆæ¯ä¼ é€’ç½‘ç»œ - å…±äº«ç¼–ç å™¨
    å®Œå…¨æ²¿ç”¨G2Retroçš„æ ¸å¿ƒç»„ä»¶
    """
    
    def __init__(self, hidden_size=256, embed_size=32, depth=10):
        super(GMPN, self).__init__()
        self.hidden_size = hidden_size
        self.embed_size = embed_size
        self.depth = depth
        
        # åŸå­åµŒå…¥å±‚
        self.atom_embedding = nn.Embedding(200, embed_size)  # æ”¯æŒå¤šç§åŸå­ç±»å‹
        
        # é”®åµŒå…¥å±‚
        self.bond_embedding = nn.Embedding(10, embed_size)   # æ”¯æŒå¤šç§é”®ç±»å‹
        
        # æ¶ˆæ¯ä¼ é€’å±‚
        self.message_layers = nn.ModuleList([
            nn.Linear(embed_size * 2, hidden_size) for _ in range(depth)
        ])
        
        # æ›´æ–°å±‚
        self.update_layers = nn.ModuleList([
            nn.GRU(hidden_size, embed_size, batch_first=True) for _ in range(depth)
        ])
        
        # å…¨å±€æ± åŒ–å±‚
        self.global_pool = nn.Linear(embed_size, hidden_size)
        
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, atom_features, bond_features, adjacency_matrix, batch_indices=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            atom_features: [num_atoms, atom_feature_dim] åŸå­ç‰¹å¾
            bond_features: [num_bonds, bond_feature_dim] é”®ç‰¹å¾  
            adjacency_matrix: [num_atoms, num_atoms] é‚»æ¥çŸ©é˜µ
            batch_indices: [num_atoms] åŸå­å±äºå“ªä¸ªåˆ†å­çš„æ ‡è¯†
            
        Returns:
            atom_embeddings: [num_atoms, embed_size] åŸå­åµŒå…¥
            bond_embeddings: [num_bonds, embed_size] é”®åµŒå…¥
            graph_embeddings: [batch_size, hidden_size] å›¾åµŒå…¥
        """
        # åŸå­ç‰¹å¾åµŒå…¥
        atom_embeddings = self.atom_embedding(atom_features)  # [num_atoms, embed_size]
        
        # æ¶ˆæ¯ä¼ é€’
        for layer_idx in range(self.depth):
            # æ”¶é›†é‚»å±…æ¶ˆæ¯
            messages = []
            for atom_idx in range(atom_embeddings.size(0)):
                neighbors = torch.nonzero(adjacency_matrix[atom_idx]).squeeze(-1)
                if len(neighbors) > 0:
                    neighbor_embeddings = atom_embeddings[neighbors]
                    atom_neighbor_concat = torch.cat([
                        atom_embeddings[atom_idx].unsqueeze(0).repeat(len(neighbors), 1),
                        neighbor_embeddings
                    ], dim=-1)
                    message = self.message_layers[layer_idx](atom_neighbor_concat)
                    message = torch.mean(message, dim=0)  # èšåˆé‚»å±…æ¶ˆæ¯
                else:
                    message = torch.zeros(self.hidden_size, device=atom_embeddings.device)
                messages.append(message)
            
            messages = torch.stack(messages)  # [num_atoms, hidden_size]
            
            # æ›´æ–°åŸå­åµŒå…¥
            messages = messages.unsqueeze(1)  # [num_atoms, 1, hidden_size]
            atom_embeddings = atom_embeddings.unsqueeze(1)  # [num_atoms, 1, embed_size]
            
            updated_embeddings, _ = self.update_layers[layer_idx](messages, atom_embeddings.transpose(0, 1))
            atom_embeddings = updated_embeddings.squeeze(1)  # [num_atoms, embed_size]
            
            atom_embeddings = self.dropout(atom_embeddings)
        
        # ç”Ÿæˆå›¾çº§åµŒå…¥
        if batch_indices is not None:
            # æŒ‰æ‰¹æ¬¡èšåˆ
            batch_size = batch_indices.max().item() + 1
            graph_embeddings = []
            
            for batch_idx in range(batch_size):
                mask = (batch_indices == batch_idx)
                if mask.any():
                    batch_atom_embeddings = atom_embeddings[mask]
                    graph_embedding = torch.mean(batch_atom_embeddings, dim=0)
                    graph_embeddings.append(graph_embedding)
                else:
                    graph_embeddings.append(torch.zeros(self.embed_size, device=atom_embeddings.device))
            
            graph_embeddings = torch.stack(graph_embeddings)  # [batch_size, embed_size]
        else:
            # å…¨å±€å¹³å‡æ± åŒ–
            graph_embeddings = torch.mean(atom_embeddings, dim=0, keepdim=True)  # [1, embed_size]
        
        # æŠ•å½±åˆ°æ›´é«˜ç»´åº¦
        graph_embeddings = self.global_pool(graph_embeddings)  # [batch_size, hidden_size]
        
        # é”®åµŒå…¥ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        bond_embeddings = self.bond_embedding(bond_features) if bond_features is not None else None
        
        return atom_embeddings, bond_embeddings, graph_embeddings


class ReactionCenterHead(nn.Module):
    """
    åŸºç¡€ä»»åŠ¡å¤´ - ååº”ä¸­å¿ƒè¯†åˆ«
    å®Œå…¨ä¿ç•™G2Retroçš„ååº”ä¸­å¿ƒè¯†åˆ«æœºåˆ¶
    """
    
    def __init__(self, hidden_size=256, num_center_types=3):
        super(ReactionCenterHead, self).__init__()
        self.hidden_size = hidden_size
        self.num_center_types = num_center_types
        
        # BF-center (æ–°å½¢æˆé”®) é¢„æµ‹å¤´
        self.bf_center_head = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # BC-center (é”®ç±»å‹å˜åŒ–) é¢„æµ‹å¤´
        self.bc_center_head = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # A-center (åŸå­å¤±å»ç‰‡æ®µ) é¢„æµ‹å¤´
        self.a_center_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
    def forward(self, atom_embeddings, bond_embeddings, edge_indices=None):
        """
        é¢„æµ‹ååº”ä¸­å¿ƒ
        
        Args:
            atom_embeddings: [num_atoms, hidden_size] åŸå­åµŒå…¥
            bond_embeddings: [num_bonds, hidden_size] é”®åµŒå…¥
            edge_indices: [num_bonds, 2] è¾¹çš„åŸå­ç´¢å¼•
            
        Returns:
            bf_scores: [num_bonds] BF-centeråˆ†æ•°
            bc_scores: [num_bonds] BC-centeråˆ†æ•°  
            a_scores: [num_atoms] A-centeråˆ†æ•°
        """
        # A-centeré¢„æµ‹ï¼ˆåŸå­çº§åˆ«ï¼‰
        a_scores = self.a_center_head(atom_embeddings).squeeze(-1)  # [num_atoms]
        
        if bond_embeddings is not None and edge_indices is not None:
            # BF-centerå’ŒBC-centeré¢„æµ‹ï¼ˆé”®çº§åˆ«ï¼‰
            atom1_embeddings = atom_embeddings[edge_indices[:, 0]]  # [num_bonds, hidden_size]
            atom2_embeddings = atom_embeddings[edge_indices[:, 1]]  # [num_bonds, hidden_size]
            
            # æ‹¼æ¥é”®ä¸¤ç«¯çš„åŸå­åµŒå…¥
            bond_pair_embeddings = torch.cat([atom1_embeddings, atom2_embeddings], dim=-1)  # [num_bonds, hidden_size*2]
            
            bf_scores = self.bf_center_head(bond_pair_embeddings).squeeze(-1)  # [num_bonds]
            bc_scores = self.bc_center_head(bond_pair_embeddings).squeeze(-1)  # [num_bonds]
        else:
            bf_scores = torch.zeros(0, device=atom_embeddings.device)
            bc_scores = torch.zeros(0, device=atom_embeddings.device)
        
        return {
            'bf_scores': bf_scores,
            'bc_scores': bc_scores,
            'a_scores': a_scores
        }


class MolecularRecoveryHead(nn.Module):
    """
    åˆ†å­æ¢å¤ä»»åŠ¡å¤´
    åŸºäºMolCLRçš„å¢å¼ºç­–ç•¥ï¼Œé¢„æµ‹è¢«ç ´åçš„åˆ†å­ä¿¡æ¯
    """
    
    def __init__(self, hidden_size=256, num_atom_types=12, num_bond_types=4):
        super(MolecularRecoveryHead, self).__init__()
        self.hidden_size = hidden_size
        self.num_atom_types = num_atom_types
        self.num_bond_types = num_bond_types
        
        # åŸå­ç±»å‹æ¢å¤å¤´ï¼ˆç”¨äºåŸå­æ©ç ï¼‰
        self.atom_recovery_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, num_atom_types)
        )
        
        # é”®ç±»å‹æ¢å¤å¤´ï¼ˆç”¨äºé”®åˆ é™¤ï¼‰
        self.bond_recovery_head = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, num_bond_types)
        )
        
        # å­å›¾æ¢å¤å¤´ï¼ˆç”¨äºå­å›¾ç§»é™¤ï¼‰
        self.subgraph_recovery_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, num_atom_types)
        )
        
    def forward(self, atom_embeddings, masked_indices, augment_type):
        """
        æ¢å¤è¢«ç ´åçš„åˆ†å­ä¿¡æ¯
        
        Args:
            atom_embeddings: [num_atoms, hidden_size] åŸå­åµŒå…¥
            masked_indices: [num_masked] è¢«æ©ç çš„åŸå­/é”®ç´¢å¼•
            augment_type: str å¢å¼ºç±»å‹ ('atom_mask', 'bond_deletion', 'subgraph_removal')
            
        Returns:
            recovery_logits: [num_masked, num_classes] æ¢å¤é¢„æµ‹
        """
        if len(masked_indices) == 0:
            return torch.zeros(0, self.num_atom_types, device=atom_embeddings.device)
        
        if augment_type == 'atom_mask':
            # åŸå­æ©ç æ¢å¤
            masked_embeddings = atom_embeddings[masked_indices]  # [num_masked, hidden_size]
            recovery_logits = self.atom_recovery_head(masked_embeddings)  # [num_masked, num_atom_types]
            
        elif augment_type == 'bond_deletion':
            # é”®åˆ é™¤æ¢å¤ - ç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨ç›¸é‚»åŸå­çš„åµŒå…¥
            if len(masked_indices) > 0:
                # å‡è®¾masked_indicesæ˜¯è¾¹çš„ç´¢å¼•ï¼Œè¿™é‡Œç®€åŒ–ä¸ºä½¿ç”¨åŸå­åµŒå…¥
                masked_embeddings = atom_embeddings[masked_indices % len(atom_embeddings)]
                recovery_logits = self.atom_recovery_head(masked_embeddings)
            else:
                recovery_logits = torch.zeros(0, self.num_atom_types, device=atom_embeddings.device)
                
        elif augment_type == 'subgraph_removal':
            # å­å›¾ç§»é™¤æ¢å¤
            masked_embeddings = atom_embeddings[masked_indices]  # [num_masked, hidden_size]
            recovery_logits = self.subgraph_recovery_head(masked_embeddings)  # [num_masked, num_atom_types]
            
        else:
            raise ValueError(f"æœªçŸ¥çš„å¢å¼ºç±»å‹: {augment_type}")
        
        return recovery_logits


class ProductSynthonContrastiveHead(nn.Module):
    """
    äº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ ä»»åŠ¡å¤´
    æ ¸å¿ƒåˆ›æ–°ï¼šåˆ©ç”¨äº§ç‰©ä¸åˆæˆå­çš„è‡ªç„¶å·®å¼‚è¿›è¡Œå¯¹æ¯”å­¦ä¹ 
    """
    
    def __init__(self, hidden_size=256, projection_dim=128, temperature=0.1):
        super(ProductSynthonContrastiveHead, self).__init__()
        self.hidden_size = hidden_size
        self.projection_dim = projection_dim
        self.temperature = temperature
        
        # æŠ•å½±ç½‘ç»œï¼ˆéµå¾ªMolCLRå’ŒPMSRçš„æ ‡å‡†åšæ³•ï¼‰
        self.projection_head = nn.Sequential(
            nn.Linear(hidden_size, projection_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(projection_dim * 2, projection_dim)
        )
        
    def forward(self, product_features, synthon_features):
        """
        è®¡ç®—äº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ æŸå¤±
        
        Args:
            product_features: [batch_size, hidden_size] äº§ç‰©åˆ†å­å›¾ç‰¹å¾
            synthon_features: [batch_size, hidden_size] åˆæˆå­ç»„åˆç‰¹å¾
            
        Returns:
            contrastive_loss: å¯¹æ¯”å­¦ä¹ æŸå¤±
            similarity_matrix: [batch_size, batch_size] ç›¸ä¼¼åº¦çŸ©é˜µ
        """
        batch_size = product_features.size(0)
        
        # æŠ•å½±åˆ°å¯¹æ¯”å­¦ä¹ ç©ºé—´
        product_proj = self.projection_head(product_features)  # [batch_size, projection_dim]
        synthon_proj = self.projection_head(synthon_features)   # [batch_size, projection_dim]
        
        # L2å½’ä¸€åŒ–
        product_proj = F.normalize(product_proj, p=2, dim=1)
        synthon_proj = F.normalize(synthon_proj, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.mm(product_proj, synthon_proj.t()) / self.temperature  # [batch_size, batch_size]
        
        # æ„å»ºæ­£æ ·æœ¬æ ‡ç­¾ï¼ˆå¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬å¯¹ï¼‰
        labels = torch.arange(batch_size, device=product_features.device)
        
        # è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆNT-XentæŸå¤±ï¼‰
        contrastive_loss = F.cross_entropy(similarity_matrix, labels)
        
        return contrastive_loss, similarity_matrix


class SynthRetroPretrainModel(nn.Module):
    """
    SynthRetro-P å®Œæ•´é¢„è®­ç»ƒæ¨¡å‹
    åŒ…å«å…±äº«ç¼–ç å™¨å’Œä¸‰ä¸ªä»»åŠ¡å¤´
    """
    
    def __init__(self, 
                 hidden_size=256, 
                 embed_size=32, 
                 depth=10,
                 num_atom_types=12,
                 num_bond_types=4,
                 projection_dim=128,
                 temperature=0.1):
        super(SynthRetroPretrainModel, self).__init__()
        
        # å…±äº«ç¼–ç å™¨
        self.shared_encoder = GMPN(hidden_size, embed_size, depth)
        
        # ä¸‰ä¸ªä»»åŠ¡å¤´
        self.reaction_center_head = ReactionCenterHead(hidden_size)
        self.molecular_recovery_head = MolecularRecoveryHead(hidden_size, num_atom_types, num_bond_types)
        self.contrastive_head = ProductSynthonContrastiveHead(hidden_size, projection_dim, temperature)
        
        # æŸå¤±æƒé‡
        self.base_weight = 1.0
        self.recovery_weight = 1.0  
        self.contrastive_weight = 0.1  # è®¾ä¸º0.1é¿å…ä¸»å¯¼è®­ç»ƒè¿‡ç¨‹
        
    def forward(self, batch_data):
        """
        å‰å‘ä¼ æ’­ - å¤šä»»åŠ¡è®­ç»ƒ
        
        Args:
            batch_data: åŒ…å«ä¸‰ä¸ªä»»åŠ¡æ•°æ®çš„æ‰¹æ¬¡
            
        Returns:
            losses: å„ä»»åŠ¡æŸå¤±
            predictions: å„ä»»åŠ¡é¢„æµ‹ç»“æœ
        """
        losses = {}
        predictions = {}
        
        # 1. åŸºç¡€ä»»åŠ¡ï¼ˆååº”ä¸­å¿ƒè¯†åˆ«ï¼‰
        if batch_data.get('base_task') is not None:
            base_task_data = batch_data['base_task']
            
            # ç¼–ç äº§ç‰©åˆ†å­
            atom_emb, bond_emb, graph_emb = self.shared_encoder(
                base_task_data['atom_features'],
                base_task_data.get('bond_features'),
                base_task_data.get('adjacency_matrix'),
                base_task_data.get('batch_indices')
            )
            
            # ååº”ä¸­å¿ƒé¢„æµ‹
            center_preds = self.reaction_center_head(atom_emb, bond_emb, base_task_data.get('edge_indices'))
            predictions['base_task'] = center_preds
            
            # è®¡ç®—åŸºç¡€ä»»åŠ¡æŸå¤±
            if 'reaction_center_labels' in base_task_data:
                base_loss = self.compute_reaction_center_loss(center_preds, base_task_data['reaction_center_labels'])
                losses['base_loss'] = base_loss * self.base_weight
        
        # 2. åˆ†å­æ¢å¤ä»»åŠ¡
        if batch_data.get('recovery_task') is not None:
            recovery_losses = []
            
            for aug_type, aug_data in batch_data['recovery_task'].items():
                if len(aug_data.get('masked_indices', [])) > 0:
                    # ç¼–ç å¢å¼ºåçš„åˆ†å­
                    atom_emb, _, _ = self.shared_encoder(
                        aug_data['augmented_atom_features'],
                        aug_data.get('augmented_bond_features'),
                        aug_data.get('augmented_adjacency_matrix'),
                        aug_data.get('batch_indices')
                    )
                    
                    # æ¢å¤é¢„æµ‹
                    recovery_logits = self.molecular_recovery_head(
                        atom_emb, 
                        aug_data['masked_indices'], 
                        aug_type
                    )
                    
                    # è®¡ç®—æ¢å¤æŸå¤±
                    recovery_loss = F.cross_entropy(recovery_logits, aug_data['target_labels'])
                    recovery_losses.append(recovery_loss)
            
            if recovery_losses:
                losses['recovery_loss'] = torch.mean(torch.stack(recovery_losses)) * self.recovery_weight
        
        # 3. äº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ 
        if batch_data.get('contrastive_task') is not None:
            contrastive_data = batch_data['contrastive_task']
            
            # ç¼–ç äº§ç‰©å’Œåˆæˆå­
            product_emb = self.encode_molecules(contrastive_data['product_features'])
            synthon_emb = self.encode_molecules(contrastive_data['synthon_features'])
            
            # å¯¹æ¯”å­¦ä¹ 
            contrastive_loss, similarity_matrix = self.contrastive_head(product_emb, synthon_emb)
            losses['contrastive_loss'] = contrastive_loss * self.contrastive_weight
            predictions['contrastive_task'] = similarity_matrix
        
        # æ€»æŸå¤±
        total_loss = sum(losses.values())
        losses['total_loss'] = total_loss
        
        return losses, predictions
    
    def encode_molecules(self, molecule_features):
        """ç®€åŒ–çš„åˆ†å­ç¼–ç ï¼ˆå ä½ç¬¦ï¼‰"""
        # è¿™é‡Œåº”è¯¥æ ¹æ®å®é™…çš„åˆ†å­ç‰¹å¾æ ¼å¼è¿›è¡Œç¼–ç 
        # æš‚æ—¶è¿”å›ç‰¹å¾æœ¬èº«
        return molecule_features
    
    def compute_reaction_center_loss(self, predictions, labels):
        """è®¡ç®—ååº”ä¸­å¿ƒè¯†åˆ«æŸå¤±"""
        losses = []
        
        # BF-centeræŸå¤±
        if len(predictions['bf_scores']) > 0 and len(labels.get('bf_centers', [])) > 0:
            bf_loss = F.binary_cross_entropy(predictions['bf_scores'], labels['bf_centers'].float())
            losses.append(bf_loss)
        
        # BC-centeræŸå¤±
        if len(predictions['bc_scores']) > 0 and len(labels.get('bc_centers', [])) > 0:
            bc_loss = F.binary_cross_entropy(predictions['bc_scores'], labels['bc_centers'].float())
            losses.append(bc_loss)
        
        # A-centeræŸå¤±
        if len(predictions['a_scores']) > 0 and len(labels.get('a_centers', [])) > 0:
            a_loss = F.binary_cross_entropy(predictions['a_scores'], labels['a_centers'].float())
            losses.append(a_loss)
        
        return torch.mean(torch.stack(losses)) if losses else torch.tensor(0.0, device=predictions['a_scores'].device)


def create_synthretro_pretrain_model(config):
    """
    åˆ›å»ºSynthRetro-Pé¢„è®­ç»ƒæ¨¡å‹çš„å·¥å‚å‡½æ•°
    
    Args:
        config: æ¨¡å‹é…ç½®å­—å…¸
        
    Returns:
        model: SynthRetro-Pæ¨¡å‹å®ä¾‹
    """
    model = SynthRetroPretrainModel(
        hidden_size=config.get('hidden_size', 256),
        embed_size=config.get('embed_size', 32),
        depth=config.get('depth', 10),
        num_atom_types=config.get('num_atom_types', 12),
        num_bond_types=config.get('num_bond_types', 4),
        projection_dim=config.get('projection_dim', 128),
        temperature=config.get('temperature', 0.1)
    )
    
    return model


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ¨¡å‹é…ç½®
    config = {
        'hidden_size': 256,
        'embed_size': 32,
        'depth': 10,
        'num_atom_types': 12,
        'num_bond_types': 4,
        'projection_dim': 128,
        'temperature': 0.1
    }
    
    # åˆ›å»ºæ¨¡å‹
    model = create_synthretro_pretrain_model(config)
    
    print("ğŸ‰ SynthRetro-Pé¢„è®­ç»ƒæ¨¡å‹åˆ›å»ºæˆåŠŸï¼")
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"ğŸ”§ å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    # æ˜¾ç¤ºæ¨¡å‹æ¶æ„
    print("\nğŸ—ï¸ æ¨¡å‹æ¶æ„:")
    print("  ğŸ§  å…±äº«ç¼–ç å™¨ (GMPN)")
    print("  ğŸ“ åŸºç¡€ä»»åŠ¡å¤´ (ååº”ä¸­å¿ƒè¯†åˆ«)")  
    print("  ğŸ”„ åˆ†å­æ¢å¤å¤´ (MolCLRå¢å¼º)")
    print("  ğŸ¤ å¯¹æ¯”å­¦ä¹ å¤´ (äº§ç‰©-åˆæˆå­å¯¹æ¯”)")

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional
import math

class GMPN(nn.Module):
    """
    å›¾æ¶ˆæ¯ä¼ é€’ç½‘ç»œ - å…±äº«ç¼–ç å™¨
    å®Œå…¨æ²¿ç”¨G2Retroçš„æ ¸å¿ƒç»„ä»¶
    """
    
    def __init__(self, hidden_size=256, embed_size=32, depth=10):
        super(GMPN, self).__init__()
        self.hidden_size = hidden_size
        self.embed_size = embed_size
        self.depth = depth
        
        # åŸå­åµŒå…¥å±‚
        self.atom_embedding = nn.Embedding(200, embed_size)  # æ”¯æŒå¤šç§åŸå­ç±»å‹
        
        # é”®åµŒå…¥å±‚
        self.bond_embedding = nn.Embedding(10, embed_size)   # æ”¯æŒå¤šç§é”®ç±»å‹
        
        # æ¶ˆæ¯ä¼ é€’å±‚
        self.message_layers = nn.ModuleList([
            nn.Linear(embed_size * 2, hidden_size) for _ in range(depth)
        ])
        
        # æ›´æ–°å±‚
        self.update_layers = nn.ModuleList([
            nn.GRU(hidden_size, embed_size, batch_first=True) for _ in range(depth)
        ])
        
        # å…¨å±€æ± åŒ–å±‚
        self.global_pool = nn.Linear(embed_size, hidden_size)
        
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, atom_features, bond_features, adjacency_matrix, batch_indices=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            atom_features: [num_atoms, atom_feature_dim] åŸå­ç‰¹å¾
            bond_features: [num_bonds, bond_feature_dim] é”®ç‰¹å¾  
            adjacency_matrix: [num_atoms, num_atoms] é‚»æ¥çŸ©é˜µ
            batch_indices: [num_atoms] åŸå­å±äºå“ªä¸ªåˆ†å­çš„æ ‡è¯†
            
        Returns:
            atom_embeddings: [num_atoms, embed_size] åŸå­åµŒå…¥
            bond_embeddings: [num_bonds, embed_size] é”®åµŒå…¥
            graph_embeddings: [batch_size, hidden_size] å›¾åµŒå…¥
        """
        # åŸå­ç‰¹å¾åµŒå…¥
        atom_embeddings = self.atom_embedding(atom_features)  # [num_atoms, embed_size]
        
        # æ¶ˆæ¯ä¼ é€’
        for layer_idx in range(self.depth):
            # æ”¶é›†é‚»å±…æ¶ˆæ¯
            messages = []
            for atom_idx in range(atom_embeddings.size(0)):
                neighbors = torch.nonzero(adjacency_matrix[atom_idx]).squeeze(-1)
                if len(neighbors) > 0:
                    neighbor_embeddings = atom_embeddings[neighbors]
                    atom_neighbor_concat = torch.cat([
                        atom_embeddings[atom_idx].unsqueeze(0).repeat(len(neighbors), 1),
                        neighbor_embeddings
                    ], dim=-1)
                    message = self.message_layers[layer_idx](atom_neighbor_concat)
                    message = torch.mean(message, dim=0)  # èšåˆé‚»å±…æ¶ˆæ¯
                else:
                    message = torch.zeros(self.hidden_size, device=atom_embeddings.device)
                messages.append(message)
            
            messages = torch.stack(messages)  # [num_atoms, hidden_size]
            
            # æ›´æ–°åŸå­åµŒå…¥
            messages = messages.unsqueeze(1)  # [num_atoms, 1, hidden_size]
            atom_embeddings = atom_embeddings.unsqueeze(1)  # [num_atoms, 1, embed_size]
            
            updated_embeddings, _ = self.update_layers[layer_idx](messages, atom_embeddings.transpose(0, 1))
            atom_embeddings = updated_embeddings.squeeze(1)  # [num_atoms, embed_size]
            
            atom_embeddings = self.dropout(atom_embeddings)
        
        # ç”Ÿæˆå›¾çº§åµŒå…¥
        if batch_indices is not None:
            # æŒ‰æ‰¹æ¬¡èšåˆ
            batch_size = batch_indices.max().item() + 1
            graph_embeddings = []
            
            for batch_idx in range(batch_size):
                mask = (batch_indices == batch_idx)
                if mask.any():
                    batch_atom_embeddings = atom_embeddings[mask]
                    graph_embedding = torch.mean(batch_atom_embeddings, dim=0)
                    graph_embeddings.append(graph_embedding)
                else:
                    graph_embeddings.append(torch.zeros(self.embed_size, device=atom_embeddings.device))
            
            graph_embeddings = torch.stack(graph_embeddings)  # [batch_size, embed_size]
        else:
            # å…¨å±€å¹³å‡æ± åŒ–
            graph_embeddings = torch.mean(atom_embeddings, dim=0, keepdim=True)  # [1, embed_size]
        
        # æŠ•å½±åˆ°æ›´é«˜ç»´åº¦
        graph_embeddings = self.global_pool(graph_embeddings)  # [batch_size, hidden_size]
        
        # é”®åµŒå…¥ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        bond_embeddings = self.bond_embedding(bond_features) if bond_features is not None else None
        
        return atom_embeddings, bond_embeddings, graph_embeddings


class ReactionCenterHead(nn.Module):
    """
    åŸºç¡€ä»»åŠ¡å¤´ - ååº”ä¸­å¿ƒè¯†åˆ«
    å®Œå…¨ä¿ç•™G2Retroçš„ååº”ä¸­å¿ƒè¯†åˆ«æœºåˆ¶
    """
    
    def __init__(self, hidden_size=256, num_center_types=3):
        super(ReactionCenterHead, self).__init__()
        self.hidden_size = hidden_size
        self.num_center_types = num_center_types
        
        # BF-center (æ–°å½¢æˆé”®) é¢„æµ‹å¤´
        self.bf_center_head = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # BC-center (é”®ç±»å‹å˜åŒ–) é¢„æµ‹å¤´
        self.bc_center_head = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
        # A-center (åŸå­å¤±å»ç‰‡æ®µ) é¢„æµ‹å¤´
        self.a_center_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
        
    def forward(self, atom_embeddings, bond_embeddings, edge_indices=None):
        """
        é¢„æµ‹ååº”ä¸­å¿ƒ
        
        Args:
            atom_embeddings: [num_atoms, hidden_size] åŸå­åµŒå…¥
            bond_embeddings: [num_bonds, hidden_size] é”®åµŒå…¥
            edge_indices: [num_bonds, 2] è¾¹çš„åŸå­ç´¢å¼•
            
        Returns:
            bf_scores: [num_bonds] BF-centeråˆ†æ•°
            bc_scores: [num_bonds] BC-centeråˆ†æ•°  
            a_scores: [num_atoms] A-centeråˆ†æ•°
        """
        # A-centeré¢„æµ‹ï¼ˆåŸå­çº§åˆ«ï¼‰
        a_scores = self.a_center_head(atom_embeddings).squeeze(-1)  # [num_atoms]
        
        if bond_embeddings is not None and edge_indices is not None:
            # BF-centerå’ŒBC-centeré¢„æµ‹ï¼ˆé”®çº§åˆ«ï¼‰
            atom1_embeddings = atom_embeddings[edge_indices[:, 0]]  # [num_bonds, hidden_size]
            atom2_embeddings = atom_embeddings[edge_indices[:, 1]]  # [num_bonds, hidden_size]
            
            # æ‹¼æ¥é”®ä¸¤ç«¯çš„åŸå­åµŒå…¥
            bond_pair_embeddings = torch.cat([atom1_embeddings, atom2_embeddings], dim=-1)  # [num_bonds, hidden_size*2]
            
            bf_scores = self.bf_center_head(bond_pair_embeddings).squeeze(-1)  # [num_bonds]
            bc_scores = self.bc_center_head(bond_pair_embeddings).squeeze(-1)  # [num_bonds]
        else:
            bf_scores = torch.zeros(0, device=atom_embeddings.device)
            bc_scores = torch.zeros(0, device=atom_embeddings.device)
        
        return {
            'bf_scores': bf_scores,
            'bc_scores': bc_scores,
            'a_scores': a_scores
        }


class MolecularRecoveryHead(nn.Module):
    """
    åˆ†å­æ¢å¤ä»»åŠ¡å¤´
    åŸºäºMolCLRçš„å¢å¼ºç­–ç•¥ï¼Œé¢„æµ‹è¢«ç ´åçš„åˆ†å­ä¿¡æ¯
    """
    
    def __init__(self, hidden_size=256, num_atom_types=12, num_bond_types=4):
        super(MolecularRecoveryHead, self).__init__()
        self.hidden_size = hidden_size
        self.num_atom_types = num_atom_types
        self.num_bond_types = num_bond_types
        
        # åŸå­ç±»å‹æ¢å¤å¤´ï¼ˆç”¨äºåŸå­æ©ç ï¼‰
        self.atom_recovery_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, num_atom_types)
        )
        
        # é”®ç±»å‹æ¢å¤å¤´ï¼ˆç”¨äºé”®åˆ é™¤ï¼‰
        self.bond_recovery_head = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, num_bond_types)
        )
        
        # å­å›¾æ¢å¤å¤´ï¼ˆç”¨äºå­å›¾ç§»é™¤ï¼‰
        self.subgraph_recovery_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, num_atom_types)
        )
        
    def forward(self, atom_embeddings, masked_indices, augment_type):
        """
        æ¢å¤è¢«ç ´åçš„åˆ†å­ä¿¡æ¯
        
        Args:
            atom_embeddings: [num_atoms, hidden_size] åŸå­åµŒå…¥
            masked_indices: [num_masked] è¢«æ©ç çš„åŸå­/é”®ç´¢å¼•
            augment_type: str å¢å¼ºç±»å‹ ('atom_mask', 'bond_deletion', 'subgraph_removal')
            
        Returns:
            recovery_logits: [num_masked, num_classes] æ¢å¤é¢„æµ‹
        """
        if len(masked_indices) == 0:
            return torch.zeros(0, self.num_atom_types, device=atom_embeddings.device)
        
        if augment_type == 'atom_mask':
            # åŸå­æ©ç æ¢å¤
            masked_embeddings = atom_embeddings[masked_indices]  # [num_masked, hidden_size]
            recovery_logits = self.atom_recovery_head(masked_embeddings)  # [num_masked, num_atom_types]
            
        elif augment_type == 'bond_deletion':
            # é”®åˆ é™¤æ¢å¤ - ç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨ç›¸é‚»åŸå­çš„åµŒå…¥
            if len(masked_indices) > 0:
                # å‡è®¾masked_indicesæ˜¯è¾¹çš„ç´¢å¼•ï¼Œè¿™é‡Œç®€åŒ–ä¸ºä½¿ç”¨åŸå­åµŒå…¥
                masked_embeddings = atom_embeddings[masked_indices % len(atom_embeddings)]
                recovery_logits = self.atom_recovery_head(masked_embeddings)
            else:
                recovery_logits = torch.zeros(0, self.num_atom_types, device=atom_embeddings.device)
                
        elif augment_type == 'subgraph_removal':
            # å­å›¾ç§»é™¤æ¢å¤
            masked_embeddings = atom_embeddings[masked_indices]  # [num_masked, hidden_size]
            recovery_logits = self.subgraph_recovery_head(masked_embeddings)  # [num_masked, num_atom_types]
            
        else:
            raise ValueError(f"æœªçŸ¥çš„å¢å¼ºç±»å‹: {augment_type}")
        
        return recovery_logits


class ProductSynthonContrastiveHead(nn.Module):
    """
    äº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ ä»»åŠ¡å¤´
    æ ¸å¿ƒåˆ›æ–°ï¼šåˆ©ç”¨äº§ç‰©ä¸åˆæˆå­çš„è‡ªç„¶å·®å¼‚è¿›è¡Œå¯¹æ¯”å­¦ä¹ 
    """
    
    def __init__(self, hidden_size=256, projection_dim=128, temperature=0.1):
        super(ProductSynthonContrastiveHead, self).__init__()
        self.hidden_size = hidden_size
        self.projection_dim = projection_dim
        self.temperature = temperature
        
        # æŠ•å½±ç½‘ç»œï¼ˆéµå¾ªMolCLRå’ŒPMSRçš„æ ‡å‡†åšæ³•ï¼‰
        self.projection_head = nn.Sequential(
            nn.Linear(hidden_size, projection_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(projection_dim * 2, projection_dim)
        )
        
    def forward(self, product_features, synthon_features):
        """
        è®¡ç®—äº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ æŸå¤±
        
        Args:
            product_features: [batch_size, hidden_size] äº§ç‰©åˆ†å­å›¾ç‰¹å¾
            synthon_features: [batch_size, hidden_size] åˆæˆå­ç»„åˆç‰¹å¾
            
        Returns:
            contrastive_loss: å¯¹æ¯”å­¦ä¹ æŸå¤±
            similarity_matrix: [batch_size, batch_size] ç›¸ä¼¼åº¦çŸ©é˜µ
        """
        batch_size = product_features.size(0)
        
        # æŠ•å½±åˆ°å¯¹æ¯”å­¦ä¹ ç©ºé—´
        product_proj = self.projection_head(product_features)  # [batch_size, projection_dim]
        synthon_proj = self.projection_head(synthon_features)   # [batch_size, projection_dim]
        
        # L2å½’ä¸€åŒ–
        product_proj = F.normalize(product_proj, p=2, dim=1)
        synthon_proj = F.normalize(synthon_proj, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.mm(product_proj, synthon_proj.t()) / self.temperature  # [batch_size, batch_size]
        
        # æ„å»ºæ­£æ ·æœ¬æ ‡ç­¾ï¼ˆå¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬å¯¹ï¼‰
        labels = torch.arange(batch_size, device=product_features.device)
        
        # è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆNT-XentæŸå¤±ï¼‰
        contrastive_loss = F.cross_entropy(similarity_matrix, labels)
        
        return contrastive_loss, similarity_matrix


class G2RetroPretrainModel(nn.Module):
    """
    G2Retro-P å®Œæ•´é¢„è®­ç»ƒæ¨¡å‹
    åŒ…å«å…±äº«ç¼–ç å™¨å’Œä¸‰ä¸ªä»»åŠ¡å¤´
    """
    
    def __init__(self, 
                 hidden_size=256, 
                 embed_size=32, 
                 depth=10,
                 num_atom_types=12,
                 num_bond_types=4,
                 projection_dim=128,
                 temperature=0.1):
        super(G2RetroPretrainModel, self).__init__()
        
        # å…±äº«ç¼–ç å™¨
        self.shared_encoder = GMPN(hidden_size, embed_size, depth)
        
        # ä¸‰ä¸ªä»»åŠ¡å¤´
        self.reaction_center_head = ReactionCenterHead(hidden_size)
        self.molecular_recovery_head = MolecularRecoveryHead(hidden_size, num_atom_types, num_bond_types)
        self.contrastive_head = ProductSynthonContrastiveHead(hidden_size, projection_dim, temperature)
        
        # æŸå¤±æƒé‡
        self.base_weight = 1.0
        self.recovery_weight = 1.0  
        self.contrastive_weight = 0.1  # è®¾ä¸º0.1é¿å…ä¸»å¯¼è®­ç»ƒè¿‡ç¨‹
        
    def forward(self, batch_data):
        """
        å‰å‘ä¼ æ’­ - å¤šä»»åŠ¡è®­ç»ƒ
        
        Args:
            batch_data: åŒ…å«ä¸‰ä¸ªä»»åŠ¡æ•°æ®çš„æ‰¹æ¬¡
            
        Returns:
            losses: å„ä»»åŠ¡æŸå¤±
            predictions: å„ä»»åŠ¡é¢„æµ‹ç»“æœ
        """
        losses = {}
        predictions = {}
        
        # 1. åŸºç¡€ä»»åŠ¡ï¼ˆååº”ä¸­å¿ƒè¯†åˆ«ï¼‰
        if batch_data.get('base_task') is not None:
            base_task_data = batch_data['base_task']
            
            # ç¼–ç äº§ç‰©åˆ†å­
            atom_emb, bond_emb, graph_emb = self.shared_encoder(
                base_task_data['atom_features'],
                base_task_data.get('bond_features'),
                base_task_data.get('adjacency_matrix'),
                base_task_data.get('batch_indices')
            )
            
            # ååº”ä¸­å¿ƒé¢„æµ‹
            center_preds = self.reaction_center_head(atom_emb, bond_emb, base_task_data.get('edge_indices'))
            predictions['base_task'] = center_preds
            
            # è®¡ç®—åŸºç¡€ä»»åŠ¡æŸå¤±
            if 'reaction_center_labels' in base_task_data:
                base_loss = self.compute_reaction_center_loss(center_preds, base_task_data['reaction_center_labels'])
                losses['base_loss'] = base_loss * self.base_weight
        
        # 2. åˆ†å­æ¢å¤ä»»åŠ¡
        if batch_data.get('recovery_task') is not None:
            recovery_losses = []
            
            for aug_type, aug_data in batch_data['recovery_task'].items():
                if len(aug_data.get('masked_indices', [])) > 0:
                    # ç¼–ç å¢å¼ºåçš„åˆ†å­
                    atom_emb, _, _ = self.shared_encoder(
                        aug_data['augmented_atom_features'],
                        aug_data.get('augmented_bond_features'),
                        aug_data.get('augmented_adjacency_matrix'),
                        aug_data.get('batch_indices')
                    )
                    
                    # æ¢å¤é¢„æµ‹
                    recovery_logits = self.molecular_recovery_head(
                        atom_emb, 
                        aug_data['masked_indices'], 
                        aug_type
                    )
                    
                    # è®¡ç®—æ¢å¤æŸå¤±
                    recovery_loss = F.cross_entropy(recovery_logits, aug_data['target_labels'])
                    recovery_losses.append(recovery_loss)
            
            if recovery_losses:
                losses['recovery_loss'] = torch.mean(torch.stack(recovery_losses)) * self.recovery_weight
        
        # 3. äº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ 
        if batch_data.get('contrastive_task') is not None:
            contrastive_data = batch_data['contrastive_task']
            
            # ç¼–ç äº§ç‰©å’Œåˆæˆå­
            product_emb = self.encode_molecules(contrastive_data['product_features'])
            synthon_emb = self.encode_molecules(contrastive_data['synthon_features'])
            
            # å¯¹æ¯”å­¦ä¹ 
            contrastive_loss, similarity_matrix = self.contrastive_head(product_emb, synthon_emb)
            losses['contrastive_loss'] = contrastive_loss * self.contrastive_weight
            predictions['contrastive_task'] = similarity_matrix
        
        # æ€»æŸå¤±
        total_loss = sum(losses.values())
        losses['total_loss'] = total_loss
        
        return losses, predictions
    
    def encode_molecules(self, molecule_features):
        """ç®€åŒ–çš„åˆ†å­ç¼–ç ï¼ˆå ä½ç¬¦ï¼‰"""
        # è¿™é‡Œåº”è¯¥æ ¹æ®å®é™…çš„åˆ†å­ç‰¹å¾æ ¼å¼è¿›è¡Œç¼–ç 
        # æš‚æ—¶è¿”å›ç‰¹å¾æœ¬èº«
        return molecule_features
    
    def compute_reaction_center_loss(self, predictions, labels):
        """è®¡ç®—ååº”ä¸­å¿ƒè¯†åˆ«æŸå¤±"""
        losses = []
        
        # BF-centeræŸå¤±
        if len(predictions['bf_scores']) > 0 and len(labels.get('bf_centers', [])) > 0:
            bf_loss = F.binary_cross_entropy(predictions['bf_scores'], labels['bf_centers'].float())
            losses.append(bf_loss)
        
        # BC-centeræŸå¤±
        if len(predictions['bc_scores']) > 0 and len(labels.get('bc_centers', [])) > 0:
            bc_loss = F.binary_cross_entropy(predictions['bc_scores'], labels['bc_centers'].float())
            losses.append(bc_loss)
        
        # A-centeræŸå¤±
        if len(predictions['a_scores']) > 0 and len(labels.get('a_centers', [])) > 0:
            a_loss = F.binary_cross_entropy(predictions['a_scores'], labels['a_centers'].float())
            losses.append(a_loss)
        
        return torch.mean(torch.stack(losses)) if losses else torch.tensor(0.0, device=predictions['a_scores'].device)


def create_g2retro_pretrain_model(config):
    """
    åˆ›å»ºG2Retro-Pé¢„è®­ç»ƒæ¨¡å‹çš„å·¥å‚å‡½æ•°
    
    Args:
        config: æ¨¡å‹é…ç½®å­—å…¸
        
    Returns:
        model: G2Retro-Pæ¨¡å‹å®ä¾‹
    """
    model = G2RetroPretrainModel(
        hidden_size=config.get('hidden_size', 256),
        embed_size=config.get('embed_size', 32),
        depth=config.get('depth', 10),
        num_atom_types=config.get('num_atom_types', 12),
        num_bond_types=config.get('num_bond_types', 4),
        projection_dim=config.get('projection_dim', 128),
        temperature=config.get('temperature', 0.1)
    )
    
    return model


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ¨¡å‹é…ç½®
    config = {
        'hidden_size': 256,
        'embed_size': 32,
        'depth': 10,
        'num_atom_types': 12,
        'num_bond_types': 4,
        'projection_dim': 128,
        'temperature': 0.1
    }
    
    # åˆ›å»ºæ¨¡å‹
    model = create_g2retro_pretrain_model(config)
    
    print("ğŸ‰ G2Retro-Pé¢„è®­ç»ƒæ¨¡å‹åˆ›å»ºæˆåŠŸï¼")
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"ğŸ”§ å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    # æ˜¾ç¤ºæ¨¡å‹æ¶æ„
    print("\nğŸ—ï¸ æ¨¡å‹æ¶æ„:")
    print("  ğŸ§  å…±äº«ç¼–ç å™¨ (GMPN)")
    print("  ğŸ“ åŸºç¡€ä»»åŠ¡å¤´ (ååº”ä¸­å¿ƒè¯†åˆ«)")  
    print("  ğŸ”„ åˆ†å­æ¢å¤å¤´ (MolCLRå¢å¼º)")
    print("  ğŸ¤ å¯¹æ¯”å­¦ä¹ å¤´ (äº§ç‰©-åˆæˆå­å¯¹æ¯”)")
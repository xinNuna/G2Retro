#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
G2Retro-Pé¢„è®­ç»ƒæ•°æ®é›†ç±»
å®Œå…¨ç¬¦åˆè®¾è®¡æ–¹æ¡ˆçš„æ•°æ®å¤„ç†å’Œå¢å¼ºç­–ç•¥

è¿™ä¸ªæ–‡ä»¶åŒ…å«ï¼š
1. G2RetroPDesignAlignedDataset - æ•°æ®é›†ç±»
2. g2retro_design_aligned_collate_fn - æ‰¹å¤„ç†å‡½æ•°
3. MolCLRæ•°æ®å¢å¼ºç­–ç•¥å®ç°
"""

import os
import pickle
import random
import numpy as np
import os
import torch
from torch.utils.data import Dataset, DataLoader
from rdkit import Chem
import copy

# å¯¼å…¥G2Retroæ ¸å¿ƒæ¨¡å—
from mol_tree import MolTree
from vocab import Vocab, common_atom_vocab
from config import device

class G2RetroPDesignAlignedDataset(Dataset):
    """
    å®Œå…¨ç¬¦åˆè®¾è®¡æ–¹æ¡ˆçš„G2Retro-Pé¢„è®­ç»ƒæ•°æ®é›†
    ä¸¥æ ¼æŒ‰ç…§è®¾è®¡æ–¹æ¡ˆçš„æ•°æ®æµç¨‹å¤„ç†ï¼š
    - è¾“å…¥å¤„ç†: åŸºäºatom-mappingä»ååº”æ•°æ®ä¸­æå–äº§ç‰©åˆ†å­å›¾Gpå’Œå¯¹åº”çš„åˆæˆå­ç»„åˆGs
    - æ•°æ®å¢å¼º: å¯¹åŸå§‹äº§ç‰©åˆ†å­å›¾åº”ç”¨MolCLRå¢å¼ºç­–ç•¥ï¼Œç”Ÿæˆè¢«"ç ´å"çš„ç‰ˆæœ¬Gp_aug
    """
    def __init__(self, data_path, vocab_path, max_samples=None, use_small_dataset=False):
        # å¦‚æœè¦ä½¿ç”¨å°æ•°æ®é›†ï¼Œä¿®æ”¹æ–‡ä»¶è·¯å¾„
        if use_small_dataset:
            if 'pretrain_tensors_train.pkl' in data_path:
                data_path = data_path.replace('pretrain_tensors_train.pkl', 'pretrain_tensors_train_small.pkl')
                print(f"ğŸš€ ä½¿ç”¨å°è®­ç»ƒé›†è¿›è¡Œå¿«é€Ÿæµ‹è¯•!")
            elif 'pretrain_tensors_valid.pkl' in data_path:
                data_path = data_path.replace('pretrain_tensors_valid.pkl', 'pretrain_tensors_valid_small.pkl')
                print(f"ğŸš€ ä½¿ç”¨å°éªŒè¯é›†è¿›è¡Œå¿«é€Ÿæµ‹è¯•!")
        
        print(f"åŠ è½½é¢„è®­ç»ƒæ•°æ®: {data_path}")
        if not os.path.exists(data_path):
            if use_small_dataset:
                print(f"âŒ å°æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
                print("è¯·å…ˆè¿è¡Œ python model/create_small_dataset.py æ¥åˆ›å»ºå°æ•°æ®é›†")
                raise FileNotFoundError(f"å°æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            else:
                raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        
        with open(data_path, 'rb') as f:
            self.data = pickle.load(f)
        
        if max_samples:
            self.data = self.data[:max_samples]
            
        print(f"åŠ è½½è¯æ±‡è¡¨: {vocab_path}")
        with open(vocab_path, 'r') as f:
            words = [line.strip() for line in f.readlines()]
        
        # ä½¿ç”¨å®Œæ•´è¯æ±‡è¡¨ï¼ˆè®¾è®¡æ–¹æ¡ˆè¦æ±‚ï¼‰
        self.vocab = Vocab(words)
        # ä½¿ç”¨æ­£ç¡®çš„åŸå­è¯æ±‡è¡¨ï¼ˆè®¾è®¡æ–¹æ¡ˆè¦æ±‚ï¼‰
        self.avocab = common_atom_vocab
        
        print(f"æ•°æ®é›†å¤§å°: {len(self.data)}")
        print(f"åˆ†å­è¯æ±‡è¡¨å¤§å°: {self.vocab.size()} (è®¾è®¡æ–¹æ¡ˆè¦æ±‚ï¼šå®Œæ•´è¯æ±‡è¡¨)")
        print(f"åŸå­è¯æ±‡è¡¨å¤§å°: {self.avocab.size()} (è®¾è®¡æ–¹æ¡ˆè¦æ±‚ï¼šcommon_atom_vocab)")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        """
        è¿”å›è®¾è®¡æ–¹æ¡ˆè¦æ±‚çš„ä¸‰è·¯æ•°æ®ï¼š
        1. åŸå§‹äº§ç‰©å›¾ Gp
        2. å¢å¼ºäº§ç‰©å›¾ Gp_augï¼ˆåº”ç”¨MolCLRç­–ç•¥ï¼‰
        3. åˆæˆå­ç»„åˆ Gs
        """
        item = self.data[idx]
        
        # æ£€æŸ¥æ•°æ®æ ¼å¼å¹¶é€‚é…ä¸åŒçš„é¢„å¤„ç†è¾“å‡º
        if 'product_tree' in item:
            # æ ‡å‡†æ ¼å¼ï¼ˆç›´æ¥é¢„å¤„ç†è¾“å‡ºï¼‰
            product_tree = item['product_tree']
            synthon_trees = item['synthon_trees']
            react_tree = item['react_tree']
        elif 'mol_trees' in item:
            # æ–°æ ¼å¼ï¼ˆæ¥è‡ªé¢„å¤„ç†è„šæœ¬ï¼‰
            mol_trees = item['mol_trees']
            if isinstance(mol_trees, tuple) and len(mol_trees) >= 3:
                product_tree = mol_trees[0]    # äº§ç‰©åˆ†å­æ ‘
                synthon_trees = mol_trees[1]   # åˆæˆå­åˆ†å­æ ‘
                react_tree = mol_trees[2]      # ååº”ç‰©åˆ†å­æ ‘
            else:
                raise ValueError(f"mol_treesæ ¼å¼é”™è¯¯: {type(mol_trees)}, length: {len(mol_trees) if hasattr(mol_trees, '__len__') else 'N/A'}")
        else:
            raise KeyError(f"æœªè¯†åˆ«çš„æ•°æ®æ ¼å¼ï¼Œå¯ç”¨é”®: {list(item.keys())}")
        
        # åˆ›å»ºMolCLRé£æ ¼çš„å¢å¼ºäº§ç‰©æ ‘
        augmented_tree = apply_molclr_graph_augmentation(product_tree)
        
        # å­˜å‚¨é¢„è®­ç»ƒä¿¡æ¯
        if 'pretrain_info' in item:
            # å¦‚æœå·²æœ‰é¢„è®­ç»ƒä¿¡æ¯ï¼Œä½¿ç”¨å®ƒ
            pretrain_info = item['pretrain_info'].copy()
            pretrain_info['augment_type'] = getattr(augmented_tree, 'augment_type', 'unknown')
        else:
            # å¦åˆ™åˆ›å»ºé»˜è®¤ä¿¡æ¯
            pretrain_info = {
                'reaction_id': item.get('reaction_id', idx),
                'product_smiles': item.get('product_smiles', ''),
                'product_orders': item.get('product_orders', None),
                'augment_type': getattr(augmented_tree, 'augment_type', 'unknown')
            }
        
        return {
            'product_tree': product_tree,      # Gpï¼šåŸå§‹äº§ç‰©å›¾
            'augmented_tree': augmented_tree,  # Gp_augï¼šå¢å¼ºäº§ç‰©å›¾
            'synthon_trees': synthon_trees,    # Gsï¼šåˆæˆå­ç»„åˆ
            'react_tree': react_tree,          # ååº”ç‰©ï¼ˆç”¨äºåŸºç¡€ä»»åŠ¡ï¼‰
            'pretrain_info': pretrain_info,
            # ä¸ºMolCLRæ©ç æ·»åŠ æ ‘å¯¹è±¡
            'prod_trees': [product_tree],
            'aug_trees': [augmented_tree],     # åŒ…å«æ©ç ä¿¡æ¯çš„å¢å¼ºæ ‘
            'synthon_tree_objects': synthon_trees if isinstance(synthon_trees, list) else [synthon_trees]
        }

def apply_molclr_graph_augmentation(tree, augmentation_prob=0.1):
    """
    åŸºäºMolCLRåŸå§‹å®ç°çš„å›¾å¢å¼º
    å‚è€ƒ: "Molecular Contrastive Learning of Representations via Graph Neural Networks"
    
    é‡è¦ï¼šMolCLRçš„å¢å¼ºåº”è¯¥åœ¨GNNç‰¹å¾å±‚é¢è¿›è¡Œï¼Œè€Œä¸æ˜¯ä¿®æ”¹åŒ–å­¦ç»“æ„
    
    ä¸‰ç§å¢å¼ºç­–ç•¥:
    1. åŸå­æ©ç  (Atom Masking): åœ¨GNNä¸­å°†æŸäº›åŸå­ç‰¹å¾æ›¿æ¢ä¸ºæ©ç token
    2. é”®åˆ é™¤ (Bond Deletion): åœ¨GNNä¸­åˆ é™¤æŸäº›è¾¹çš„ç‰¹å¾
    3. å­å›¾ç§»é™¤ (Subgraph Removal): åœ¨GNNä¸­æ©ç è¿é€šå­å›¾çš„ç‰¹å¾
    """
    try:
        # ä¿å­˜åŸå§‹åˆ†å­æ ‘çš„å‰¯æœ¬
        original_tree = copy.deepcopy(tree)
        
        if tree.mol is None:
            return original_tree
        
        # éšæœºé€‰æ‹©ä¸€ç§å¢å¼ºç­–ç•¥
        augmentation_type = random.choice(['atom_masking', 'bond_deletion', 'subgraph_removal'])
        
        # ä¸ºMolTreeæ·»åŠ æ©ç ä¿¡æ¯ï¼Œè®©GNNåœ¨ç¼–ç æ—¶ä½¿ç”¨
        augmented_tree = copy.deepcopy(tree)
        
        if augmentation_type == 'atom_masking':
            # åŸå­æ©ç ï¼šæ ‡è®°å“ªäº›åŸå­éœ€è¦åœ¨GNNä¸­æ©ç 
            num_atoms = tree.mol.GetNumAtoms()
            if num_atoms > 0:
                num_mask = max(1, int(num_atoms * augmentation_prob))
                masked_atom_indices = random.sample(range(num_atoms), min(num_mask, num_atoms))
                
                # åœ¨MolTreeä¸Šæ·»åŠ æ©ç æ ‡è®°ï¼Œä¾›GNNä½¿ç”¨
                augmented_tree.atom_masks = torch.zeros(num_atoms, dtype=torch.bool)
                for idx in masked_atom_indices:
                    augmented_tree.atom_masks[idx] = True
                    
        elif augmentation_type == 'bond_deletion':
            # é”®åˆ é™¤ï¼šæ ‡è®°å“ªäº›é”®éœ€è¦åœ¨GNNä¸­åˆ é™¤
            num_bonds = tree.mol.GetNumBonds()
            if num_bonds > 0:
                num_delete = max(1, int(num_bonds * augmentation_prob))
                deleted_bond_indices = random.sample(range(num_bonds), min(num_delete, num_bonds))
                
                # åœ¨MolTreeä¸Šæ·»åŠ é”®æ©ç æ ‡è®°
                augmented_tree.bond_masks = torch.zeros(num_bonds, dtype=torch.bool)
                for idx in deleted_bond_indices:
                    augmented_tree.bond_masks[idx] = True
                    
        else:  # subgraph_removal
            # å­å›¾ç§»é™¤ï¼šæ ‡è®°è¿é€šçš„åŸå­å­é›†éœ€è¦åœ¨GNNä¸­æ©ç 
            num_atoms = tree.mol.GetNumAtoms()
            if num_atoms > 1:
                # é€‰æ‹©ä¸€ä¸ªèµ·å§‹åŸå­
                start_atom = random.randint(0, num_atoms - 1)
                
                # æ‰¾åˆ°è¿é€šçš„åŸå­å­é›†ï¼ˆBFSï¼‰
                mol = tree.mol
                visited = set()
                to_visit = [start_atom]
                max_subgraph_size = max(1, int(num_atoms * augmentation_prob))
                
                while to_visit and len(visited) < max_subgraph_size:
                    current = to_visit.pop(0)
                    if current in visited:
                        continue
                    visited.add(current)
                    
                    # æ·»åŠ é‚»å±…åŸå­
                    atom = mol.GetAtomWithIdx(current)
                    for neighbor in atom.GetNeighbors():
                        neighbor_idx = neighbor.GetIdx()
                        if neighbor_idx not in visited and len(visited) < max_subgraph_size:
                            to_visit.append(neighbor_idx)
                
                # åœ¨MolTreeä¸Šæ·»åŠ å­å›¾æ©ç æ ‡è®°
                augmented_tree.subgraph_masks = torch.zeros(num_atoms, dtype=torch.bool)
                for idx in visited:
                    augmented_tree.subgraph_masks[idx] = True
        
        # è®¾ç½®å¢å¼ºä¿¡æ¯
        augmented_tree.augmented = True
        augmented_tree.augment_type = augmentation_type
        
        # ç¡®ä¿å¢å¼ºçš„åˆ†å­æ ‘ä»æœ‰changeå±æ€§ï¼ˆç”¨äºåŸºç¡€ä»»åŠ¡ï¼‰
        if not hasattr(augmented_tree, 'change'):
            augmented_tree.change = getattr(tree, 'change', ([], [], [], [], [], []))
        
        return augmented_tree
        
    except Exception as e:
        print(f"MolCLRå›¾å¢å¼ºé”™è¯¯: {e}")
        return original_tree

# æ³¨æ„ï¼šMolCLRçš„å¢å¼ºç°åœ¨åœ¨GNNç‰¹å¾å±‚é¢è¿›è¡Œï¼Œä¸å†éœ€è¦apply_atom_maskingå‡½æ•°

# æ³¨æ„ï¼šMolCLRçš„å¢å¼ºç°åœ¨åœ¨GNNç‰¹å¾å±‚é¢è¿›è¡Œï¼Œä¸å†éœ€è¦åŒ–å­¦ç»“æ„ä¿®æ”¹å‡½æ•°

def g2retro_design_aligned_collate_fn(batch, vocab, avocab):
    """
    æ‰¹å¤„ç†å‡½æ•°ï¼šæŒ‰ç…§è®¾è®¡æ–¹æ¡ˆå‡†å¤‡ä¸‰è·¯æ•°æ®æµ
    
    æ•°æ®æµç¨‹ï¼ˆå®Œå…¨å¯¹é½è®¾è®¡æ–¹æ¡ˆï¼‰ï¼š
    1. åŸå§‹äº§ç‰©å›¾ Gp â†’ prod_tensors
    2. å¢å¼ºäº§ç‰©å›¾ Gp_aug â†’ aug_tensors  
    3. åˆæˆå­ç»„åˆ Gs â†’ synthon_tensors
    """
    try:
        batch_size = len(batch)
        print(f"æ‰¹å¤„ç†å‡½æ•°ï¼šå¤„ç† {batch_size} ä¸ªæ ·æœ¬")
        
        # æ”¶é›†æ‰€æœ‰åˆ†å­æ ‘
        prod_trees = []      # åŸå§‹äº§ç‰©æ ‘
        aug_trees = []       # å¢å¼ºäº§ç‰©æ ‘
        synthon_trees = []   # åˆæˆå­æ ‘
        react_trees = []     # ååº”ç‰©æ ‘
        
        # æ”¶é›†å…¶ä»–ä¿¡æ¯
        pretrain_infos = []
        augmented_data_batch = []
        
        # éªŒè¯æ‰¹æ¬¡æ•°æ®
        valid_batch = []
        for item in batch:
            if all(key in item for key in ['product_tree', 'augmented_tree', 'synthon_trees', 'react_tree']):
                valid_batch.append(item)
            else:
                print(f"è­¦å‘Šï¼šè·³è¿‡ä¸å®Œæ•´çš„æ•°æ®é¡¹")
        
        if not valid_batch:
            print("é”™è¯¯ï¼šæ‰¹æ¬¡ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
            return None
            
        # å¤„ç†æ¯ä¸ªæ ·æœ¬
        for item in valid_batch:
            # åŸå§‹äº§ç‰©æ ‘
            prod_trees.append(item['product_tree'])
            
            # å¢å¼ºäº§ç‰©æ ‘
            aug_trees.append(item['augmented_tree'])
            
            # åˆæˆå­æ ‘ï¼ˆå¯èƒ½æœ‰å¤šä¸ªï¼‰
            synthon_list = item['synthon_trees']
            if isinstance(synthon_list, list) and len(synthon_list) > 0:
                # åˆå¹¶å¤šä¸ªåˆæˆå­ä¸ºä¸€ä¸ªç»„åˆåˆ†å­æ ‘
                combined_synthon = synthon_list[0]  # ç®€åŒ–ï¼šæš‚æ—¶åªç”¨ç¬¬ä¸€ä¸ª
                synthon_trees.append(combined_synthon)
            else:
                synthon_trees.append(synthon_list)
            
            # ååº”ç‰©æ ‘
            react_trees.append(item['react_tree'])
            
            # é¢„è®­ç»ƒä¿¡æ¯
            pretrain_infos.append(item.get('pretrain_info', {}))
            
            # å¢å¼ºæ•°æ®ä¿¡æ¯ï¼ˆç”¨äºåˆ†å­æ¢å¤ä»»åŠ¡ï¼‰
            augmented_data = {
                'augment_type': item['augmented_tree'].augment_type if hasattr(item['augmented_tree'], 'augment_type') else 'unknown',
                'masked_atoms': getattr(item['augmented_tree'], 'masked_atoms', []),
                'original_atoms': getattr(item['augmented_tree'], 'original_atoms', {}),
                'deleted_bonds': getattr(item['augmented_tree'], 'deleted_bonds', []),
                'removed_atoms': getattr(item['augmented_tree'], 'removed_atoms', [])
            }
            augmented_data_batch.append(augmented_data)
        
        # å¼ é‡åŒ–å¤„ç†
        # MolTree.tensorizeè¿”å›(trees, tensors)å…ƒç»„ï¼Œæˆ‘ä»¬éœ€è¦tensorséƒ¨åˆ†
        mol_batch_prod, prod_tensors = MolTree.tensorize(
            prod_trees, vocab, avocab, 
            use_feature=True, product=True
        )
        
        mol_batch_aug, aug_tensors = MolTree.tensorize(
            aug_trees, vocab, avocab, 
            use_feature=True, product=True
        )
        
        mol_batch_synthon, synthon_tensors = MolTree.tensorize(
            synthon_trees, vocab, avocab, 
            use_feature=True, product=False
        )
        
        mol_batch_react, react_tensors = MolTree.tensorize(
            react_trees, vocab, avocab, 
            use_feature=True, product=False
        )
        
        # æ„å»ºæ‰¹æ¬¡å­—å…¸
        batch_dict = {
            'batch_size': len(valid_batch),
            'prod_tensors': prod_tensors,      # Gpï¼šåŸå§‹äº§ç‰©å›¾
            'aug_tensors': aug_tensors,        # Gp_augï¼šå¢å¼ºäº§ç‰©å›¾
            'synthon_tensors': synthon_tensors, # Gsï¼šåˆæˆå­ç»„åˆ
            'react_tensors': react_tensors,    # ååº”ç‰©ï¼ˆåŸºç¡€ä»»åŠ¡ç”¨ï¼‰
            'prod_trees': prod_trees,          # äº§ç‰©åˆ†å­æ ‘ï¼ˆåŸºç¡€ä»»åŠ¡éœ€è¦ï¼‰
            'synthon_trees': synthon_trees,    # åˆæˆå­åˆ†å­æ ‘
            'react_trees': react_trees,        # ååº”ç‰©åˆ†å­æ ‘
            'augmented_trees': aug_trees,      # å¢å¼ºåˆ†å­æ ‘
            'augmented_data': augmented_data_batch,  # å¢å¼ºæ•°æ®ä¿¡æ¯
            'pretrain_infos': pretrain_infos,  # é¢„è®­ç»ƒä¿¡æ¯ï¼ˆåŒ…å«product_ordersï¼‰
            # ä¸ºMolCLRæ©ç æ·»åŠ MolTreeå¯¹è±¡
            'aug_trees': aug_trees  # åŒ…å«æ©ç ä¿¡æ¯çš„å¢å¼ºæ ‘å¯¹è±¡
        }
        
        return batch_dict
        
    except Exception as e:
        print(f"æ‰¹å¤„ç†å‡½æ•°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None

def create_dataloaders(train_data_path, val_data_path, vocab_path, args, max_train_samples=None, max_val_samples=None, use_small_dataset=False):
    """
    åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨çš„è¾…åŠ©å‡½æ•°
    
    Args:
        train_data_path: è®­ç»ƒæ•°æ®è·¯å¾„
        val_data_path: éªŒè¯æ•°æ®è·¯å¾„
        vocab_path: è¯æ±‡è¡¨è·¯å¾„
        args: å‚æ•°å¯¹è±¡
        max_train_samples: æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰
        max_val_samples: æœ€å¤§éªŒè¯æ ·æœ¬æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰
        use_small_dataset: æ˜¯å¦ä½¿ç”¨å°æ•°æ®é›†è¿›è¡Œå¿«é€Ÿæµ‹è¯•
    
    Returns:
        train_dataset, val_dataset, train_loader, val_loader
    """
    # åˆ›å»ºæ•°æ®é›†
    print("\nåˆ›å»ºæ•°æ®é›†...")
    if use_small_dataset:
        print("ğŸš€ å¯ç”¨å°æ•°æ®é›†æ¨¡å¼ - å¿«é€Ÿæµ‹è¯•!")
    train_dataset = G2RetroPDesignAlignedDataset(train_data_path, vocab_path, max_samples=max_train_samples, use_small_dataset=use_small_dataset)
    val_dataset = G2RetroPDesignAlignedDataset(val_data_path, vocab_path, max_samples=max_val_samples, use_small_dataset=use_small_dataset)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=lambda batch: g2retro_design_aligned_collate_fn(batch, train_dataset.vocab, train_dataset.avocab),
        num_workers=0,
        drop_last=True  # ç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸€è‡´ï¼ˆå¯¹æ¯”å­¦ä¹ éœ€è¦ï¼‰
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=lambda batch: g2retro_design_aligned_collate_fn(batch, val_dataset.vocab, val_dataset.avocab),
        num_workers=0
    )
    
    print(f"âœ“ è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"âœ“ éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    print(f"âœ“ è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"âœ“ éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    
    return train_dataset, val_dataset, train_loader, val_loader

if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®é›†
    print("æµ‹è¯•G2Retro-Pæ•°æ®é›†...")
    
    # æµ‹è¯•è·¯å¾„
    test_data_path = '../data/pretrain/pretrain_tensors_valid.pkl'
    vocab_path = '../data/pretrain/vocab_train.txt'
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = G2RetroPDesignAlignedDataset(test_data_path, vocab_path, max_samples=5)
    
    # æµ‹è¯•å•ä¸ªæ ·æœ¬
    print("\næµ‹è¯•å•ä¸ªæ ·æœ¬...")
    sample = dataset[0]
    print(f"æ ·æœ¬é”®: {list(sample.keys())}")
    
    # æµ‹è¯•æ‰¹å¤„ç†
    print("\næµ‹è¯•æ‰¹å¤„ç†...")
    batch = [dataset[i] for i in range(min(2, len(dataset)))]
    batch_data = g2retro_design_aligned_collate_fn(batch, dataset.vocab, dataset.avocab)
    
    if batch_data:
        print(f"æ‰¹å¤„ç†æˆåŠŸï¼")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_data['batch_size']}")
        print(f"æ•°æ®é”®: {list(batch_data.keys())}")
    else:
        print(f"æ‰¹å¤„ç†å¤±è´¥ï¼")
    
    print("\næ•°æ®é›†æµ‹è¯•å®Œæˆï¼")
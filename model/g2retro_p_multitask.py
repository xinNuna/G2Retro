#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
G2Retro-På¤šä»»åŠ¡é¢„è®­ç»ƒæ¨¡å‹ - å®Œæ•´çš„ä¸‰ä»»åŠ¡å¹¶è¡Œæ¶æ„

è¿™ä¸ªæ¨¡å—å®ç°äº†G2Retro-Pçš„å®Œæ•´é¢„è®­ç»ƒæ¶æ„ï¼ŒåŒ…å«ä¸‰ä¸ªå¹¶è¡Œä»»åŠ¡å¤´ï¼š
1. åŸºç¡€ä»»åŠ¡å¤´ï¼šååº”ä¸­å¿ƒè¯†åˆ«ï¼ˆå¤ç”¨G2Retroçš„MolCenterï¼‰
2. åˆ†å­æ¢å¤å¤´ï¼šMolCLRé£æ ¼çš„åˆ†å­å¯¹æ¯”å­¦ä¹ 
3. äº§ç‰©-åˆæˆå­å¯¹æ¯”å¤´ï¼šå­¦ä¹ é€†åˆæˆæ˜ å°„å…³ç³»

å‚è€ƒæ–‡çŒ®ï¼š
- G2Retro: Graph-Guided Molecule Generation for Retrosynthesis Prediction
- PMSR: Learning Chemical Rules of Retrosynthesis with Pre-training
- MolCLR: Molecular Contrastive Learning of Representations via Graph Neural Networks

è®¾è®¡ç†å¿µï¼š
é€šè¿‡å¤šä»»åŠ¡é¢„è®­ç»ƒè®©æ¨¡å‹åŒæ—¶å­¦ä¹ ï¼š
- åŒ–å­¦ååº”çš„åŸºæœ¬è§„å¾‹ï¼ˆååº”ä¸­å¿ƒè¯†åˆ«ï¼‰
- åˆ†å­è¡¨ç¤ºçš„é²æ£’æ€§ï¼ˆåˆ†å­æ¢å¤ï¼‰
- é€†åˆæˆçš„æ˜ å°„å…³ç³»ï¼ˆäº§ç‰©-åˆæˆå­å¯¹æ¯”ï¼‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Union
import warnings

# å¯¼å…¥G2Retroç°æœ‰ç»„ä»¶
try:
    from mol_tree import MolTree
    from vocab import Vocab
    from config import device
except ImportError:
    warnings.warn("æ— æ³•å¯¼å…¥G2Retroç»„ä»¶ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½å—é™")

# å¯¼å…¥æˆ‘ä»¬å®ç°çš„ä»»åŠ¡å¤´
from molecule_recovery_head import MoleculeRecoveryHead
from product_synthon_contrastive_head import ProductSynthonContrastiveHead

class G2RetroPMultiTaskModel(nn.Module):
    """
    G2Retro-På¤šä»»åŠ¡é¢„è®­ç»ƒæ¨¡å‹
    
    é›†æˆä¸‰ä¸ªå¹¶è¡Œä»»åŠ¡å¤´çš„å®Œæ•´é¢„è®­ç»ƒæ¶æ„ï¼š
    1. åŸºç¡€ä»»åŠ¡ï¼šååº”ä¸­å¿ƒè¯†åˆ«ï¼ˆå¤ç”¨G2Retro MolCenterï¼‰
    2. åˆ†å­æ¢å¤ï¼šMolCLRå¯¹æ¯”å­¦ä¹ 
    3. äº§ç‰©-åˆæˆå­å¯¹æ¯”ï¼šé€†åˆæˆæ˜ å°„å­¦ä¹ 
    """
    
    def __init__(self,
                 # åˆ†å­å›¾ç¼–ç å™¨é…ç½®
                 vocab: Vocab,
                 atom_vocab: List[str],
                 encoder_hidden_dim: int = 300,
                 encoder_depth: int = 3,
                 
                 # ä»»åŠ¡å¤´é…ç½®
                 recovery_config: Optional[Dict] = None,
                 contrastive_config: Optional[Dict] = None,
                 
                 # å¤šä»»åŠ¡å­¦ä¹ é…ç½®
                 task_weights: Optional[Dict[str, float]] = None,
                 temperature_schedule: str = "static",  # static, cosine, linear
                 
                 # è®­ç»ƒé…ç½®
                 dropout: float = 0.1):
        """
        åˆå§‹åŒ–G2Retro-På¤šä»»åŠ¡é¢„è®­ç»ƒæ¨¡å‹
        
        Args:
            vocab: åˆ†å­æ ‘è¯æ±‡è¡¨
            atom_vocab: åŸå­è¯æ±‡è¡¨
            encoder_hidden_dim: ç¼–ç å™¨éšè—ç»´åº¦
            encoder_depth: ç¼–ç å™¨æ·±åº¦
            recovery_config: åˆ†å­æ¢å¤ä»»åŠ¡å¤´é…ç½®
            contrastive_config: äº§ç‰©-åˆæˆå­å¯¹æ¯”ä»»åŠ¡å¤´é…ç½®
            task_weights: ä»»åŠ¡æƒé‡å­—å…¸
            temperature_schedule: æ¸©åº¦è°ƒåº¦ç­–ç•¥
            dropout: Dropoutæ¯”ä¾‹
        """
        super(G2RetroPMultiTaskModel, self).__init__()
        
        self.vocab = vocab
        self.atom_vocab = atom_vocab
        self.encoder_hidden_dim = encoder_hidden_dim
        self.vocab_size = len(vocab)
        self.atom_vocab_size = len(atom_vocab)
        
        # é»˜è®¤ä»»åŠ¡æƒé‡
        self.task_weights = task_weights or {
            'base_task': 1.0,           # åŸºç¡€ä»»åŠ¡ï¼ˆååº”ä¸­å¿ƒè¯†åˆ«ï¼‰
            'molecular_recovery': 1.0,   # åˆ†å­æ¢å¤ä»»åŠ¡
            'product_synthon': 1.0       # äº§ç‰©-åˆæˆå­å¯¹æ¯”ä»»åŠ¡
        }
        
        self.temperature_schedule = temperature_schedule
        self.current_epoch = 0
        
        # === 1. åˆ†å­å›¾ç¼–ç å™¨ ===
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„å›¾ç¼–ç å™¨ï¼Œå®é™…åº”è¯¥å¤ç”¨G2Retroçš„å›¾ç¼–ç å™¨
        self.molecule_encoder = self._build_molecule_encoder()
        
        # === 2. åŸºç¡€ä»»åŠ¡å¤´ï¼ˆååº”ä¸­å¿ƒè¯†åˆ«ï¼‰===
        # å¤ç”¨G2Retroçš„MolCenterç»„ä»¶
        self.base_task_head = self._build_base_task_head()
        
        # === 3. åˆ†å­æ¢å¤ä»»åŠ¡å¤´ ===
        recovery_config = recovery_config or {}
        self.molecular_recovery_head = MoleculeRecoveryHead(
            input_dim=encoder_hidden_dim,
            projection_dim=recovery_config.get('projection_dim', 128),
            hidden_dim=recovery_config.get('hidden_dim', 512),
            temperature=recovery_config.get('temperature', 0.1),
            dropout=dropout
        )
        
        # === 4. äº§ç‰©-åˆæˆå­å¯¹æ¯”ä»»åŠ¡å¤´ ===
        contrastive_config = contrastive_config or {}
        self.product_synthon_head = ProductSynthonContrastiveHead(
            product_input_dim=encoder_hidden_dim,
            synthon_input_dim=encoder_hidden_dim,
            projection_dim=contrastive_config.get('projection_dim', 256),
            hidden_dim=contrastive_config.get('hidden_dim', 512),
            temperature=contrastive_config.get('temperature', 0.07),
            dropout=dropout,
            fusion_method=contrastive_config.get('fusion_method', 'attention')
        )
        
        # æƒé‡åˆå§‹åŒ–
        self._init_weights()
    
    def _build_molecule_encoder(self) -> nn.Module:
        """
        æ„å»ºåˆ†å­å›¾ç¼–ç å™¨
        
        åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥å¤ç”¨G2Retroçš„å›¾ç¥ç»ç½‘ç»œç¼–ç å™¨ã€‚
        ç›®å‰ä½¿ç”¨ç®€åŒ–çš„MLPä½œä¸ºå ä½ç¬¦ã€‚
        """
        # å ä½ç¬¦ç¼–ç å™¨ - å®é™…åº”è¯¥ä½¿ç”¨G2Retroçš„GNNç¼–ç å™¨
        return nn.Sequential(
            nn.Linear(self.vocab_size + self.atom_vocab_size, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, self.encoder_hidden_dim),
            nn.ReLU()
        )
    
    def _build_base_task_head(self) -> nn.Module:
        """
        æ„å»ºåŸºç¡€ä»»åŠ¡å¤´ï¼ˆååº”ä¸­å¿ƒè¯†åˆ«ï¼‰
        
        åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥å¤ç”¨G2Retroçš„MolCenterç»„ä»¶ã€‚
        """
        # å ä½ç¬¦ - å®é™…åº”è¯¥ä½¿ç”¨G2Retroçš„MolCenter
        return nn.Sequential(
            nn.Linear(self.encoder_hidden_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, self.atom_vocab_size)  # é¢„æµ‹åŸå­çº§åˆ«çš„ååº”ä¸­å¿ƒ
        )
    
    def _init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
    
    def encode_molecule(self, mol_tree: Union[MolTree, torch.Tensor]) -> torch.Tensor:
        """
        ç¼–ç åˆ†å­ä¸ºå‘é‡è¡¨ç¤º
        
        Args:
            mol_tree: åˆ†å­æ ‘å¯¹è±¡æˆ–ç‰¹å¾å¼ é‡
            
        Returns:
            åˆ†å­åµŒå…¥å‘é‡ [hidden_dim]
        """
        if isinstance(mol_tree, torch.Tensor):
            # å¦‚æœè¾“å…¥å·²ç»æ˜¯å¼ é‡ï¼Œç›´æ¥ä½¿ç”¨
            mol_features = mol_tree
        else:
            # å¦‚æœæ˜¯MolTreeå¯¹è±¡ï¼Œéœ€è¦æå–ç‰¹å¾
            # è¿™é‡Œä½¿ç”¨å ä½ç¬¦ï¼Œå®é™…åº”è¯¥è°ƒç”¨G2Retroçš„ç‰¹å¾æå–æ–¹æ³•
            mol_features = torch.randn(self.vocab_size + self.atom_vocab_size)
        
        # é€šè¿‡ç¼–ç å™¨è·å¾—åˆ†å­åµŒå…¥
        mol_embedding = self.molecule_encoder(mol_features)
        
        return mol_embedding
    
    def forward(self, batch_data: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­ - æ‰§è¡Œä¸‰ä¸ªå¹¶è¡Œä»»åŠ¡
        
        Args:
            batch_data: æ‰¹æ¬¡æ•°æ®å­—å…¸ï¼ŒåŒ…å«ï¼š
                - 'product_trees': äº§ç‰©åˆ†å­æ ‘åˆ—è¡¨
                - 'synthon_trees': åˆæˆå­æ ‘åˆ—è¡¨
                - 'augmented_data': å¢å¼ºæ•°æ®å­—å…¸
                - 'reaction_centers': ååº”ä¸­å¿ƒæ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
                
        Returns:
            åŒ…å«æ‰€æœ‰ä»»åŠ¡ç»“æœçš„å­—å…¸
        """
        batch_size = len(batch_data['product_trees'])
        device = next(self.parameters()).device
        
        # === 1. åˆ†å­ç¼–ç é˜¶æ®µ ===
        
        # ç¼–ç äº§ç‰©åˆ†å­
        product_embeddings = []
        for mol_tree in batch_data['product_trees']:
            # è¿™é‡Œä½¿ç”¨éšæœºåµŒå…¥ä½œä¸ºå ä½ç¬¦ï¼Œå®é™…åº”è¯¥è°ƒç”¨çœŸå®çš„ç¼–ç æ–¹æ³•
            mol_emb = torch.randn(self.encoder_hidden_dim).to(device)
            product_embeddings.append(mol_emb)
        product_embeddings = torch.stack(product_embeddings)  # [batch_size, hidden_dim]
        
        # ç¼–ç åˆæˆå­
        synthon_embeddings = []
        max_synthons = 0
        for synthon_list in batch_data['synthon_trees']:
            if synthon_list:
                batch_synthons = []
                for synthon in synthon_list:
                    # åŒæ ·ä½¿ç”¨å ä½ç¬¦
                    synthon_emb = torch.randn(self.encoder_hidden_dim).to(device)
                    batch_synthons.append(synthon_emb)
                synthon_embeddings.append(torch.stack(batch_synthons))
                max_synthons = max(max_synthons, len(batch_synthons))
            else:
                # ç©ºåˆæˆå­åˆ—è¡¨çš„æƒ…å†µ
                synthon_embeddings.append(torch.zeros(1, self.encoder_hidden_dim).to(device))
                max_synthons = max(max_synthons, 1)
        
        # å¡«å……ä¸ºç›¸åŒé•¿åº¦
        padded_synthon_embeddings = []
        synthon_masks = []
        for synthon_batch in synthon_embeddings:
            current_len = synthon_batch.shape[0]
            if current_len < max_synthons:
                # å¡«å……
                padding = torch.zeros(max_synthons - current_len, self.encoder_hidden_dim).to(device)
                padded_synthon = torch.cat([synthon_batch, padding], dim=0)
                mask = torch.cat([torch.ones(current_len), torch.zeros(max_synthons - current_len)])
            else:
                padded_synthon = synthon_batch
                mask = torch.ones(current_len)
            
            padded_synthon_embeddings.append(padded_synthon)
            synthon_masks.append(mask.bool())
        
        synthon_embeddings_tensor = torch.stack(padded_synthon_embeddings)  # [batch_size, max_synthons, hidden_dim]
        synthon_masks_tensor = torch.stack(synthon_masks).to(device)  # [batch_size, max_synthons]
        
        # === 2. ä»»åŠ¡æ‰§è¡Œé˜¶æ®µ ===
        
        results = {}
        
        # ä»»åŠ¡1ï¼šåŸºç¡€ä»»åŠ¡ï¼ˆååº”ä¸­å¿ƒè¯†åˆ«ï¼‰
        if 'base_task' in self.task_weights and self.task_weights['base_task'] > 0:
            base_logits = self.base_task_head(product_embeddings)  # [batch_size, atom_vocab_size]
            
            # è®¡ç®—æŸå¤±ï¼ˆå¦‚æœæœ‰æ ‡ç­¾ï¼‰
            if 'reaction_centers' in batch_data:
                base_loss = F.cross_entropy(base_logits, batch_data['reaction_centers'])
            else:
                base_loss = torch.tensor(0.0, device=device)
            
            results['base_task'] = {
                'logits': base_logits,
                'loss': base_loss
            }
        
        # ä»»åŠ¡2ï¼šåˆ†å­æ¢å¤ï¼ˆMolCLRå¯¹æ¯”å­¦ä¹ ï¼‰
        if 'molecular_recovery' in self.task_weights and self.task_weights['molecular_recovery'] > 0:
            # ç”Ÿæˆå¢å¼ºçš„åˆ†å­åµŒå…¥ï¼ˆè¿™é‡Œä½¿ç”¨éšæœºå™ªå£°æ¨¡æ‹Ÿï¼‰
            augmented_embeddings = product_embeddings + torch.randn_like(product_embeddings) * 0.1
            
            recovery_results = self.molecular_recovery_head(product_embeddings, augmented_embeddings)
            results['molecular_recovery'] = recovery_results
        
        # ä»»åŠ¡3ï¼šäº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ 
        if 'product_synthon' in self.task_weights and self.task_weights['product_synthon'] > 0:
            contrastive_results = self.product_synthon_head(
                product_embeddings, 
                synthon_embeddings_tensor, 
                synthon_masks_tensor
            )
            results['product_synthon'] = contrastive_results
        
        # === 3. å¤šä»»åŠ¡æŸå¤±æ•´åˆ ===
        
        total_loss = torch.tensor(0.0, device=device)
        loss_details = {}
        
        for task_name, task_weight in self.task_weights.items():
            if task_name in results and task_weight > 0:
                task_loss = results[task_name]['loss']
                weighted_loss = task_weight * task_loss
                total_loss += weighted_loss
                
                loss_details[f'{task_name}_loss'] = task_loss.item()
                loss_details[f'{task_name}_weighted_loss'] = weighted_loss.item()
        
        results['total_loss'] = total_loss
        results['loss_details'] = loss_details
        
        return results
    
    def update_temperature_schedule(self, epoch: int, max_epochs: int):
        """
        æ›´æ–°æ¸©åº¦è°ƒåº¦
        
        Args:
            epoch: å½“å‰è½®æ•°
            max_epochs: æœ€å¤§è½®æ•°
        """
        self.current_epoch = epoch
        
        if self.temperature_schedule == "cosine":
            # ä½™å¼¦é€€ç«æ¸©åº¦è°ƒåº¦
            progress = epoch / max_epochs
            for head in [self.molecular_recovery_head, self.product_synthon_head]:
                original_temp = 0.1 if hasattr(head, 'temperature') else 0.07
                head.temperature = original_temp * (0.5 * (1 + np.cos(np.pi * progress)))
        
        elif self.temperature_schedule == "linear":
            # çº¿æ€§æ¸©åº¦è°ƒåº¦
            progress = epoch / max_epochs
            for head in [self.molecular_recovery_head, self.product_synthon_head]:
                original_temp = 0.1 if hasattr(head, 'temperature') else 0.07
                head.temperature = original_temp * (1 - 0.5 * progress)
    
    def set_task_weights(self, new_weights: Dict[str, float]):
        """
        åŠ¨æ€è°ƒæ•´ä»»åŠ¡æƒé‡
        
        Args:
            new_weights: æ–°çš„ä»»åŠ¡æƒé‡å­—å…¸
        """
        self.task_weights.update(new_weights)
    
    def get_molecular_embeddings(self, mol_trees: List[MolTree]) -> torch.Tensor:
        """
        è·å–åˆ†å­åµŒå…¥ï¼ˆç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼‰
        
        Args:
            mol_trees: åˆ†å­æ ‘åˆ—è¡¨
            
        Returns:
            åˆ†å­åµŒå…¥çŸ©é˜µ [num_molecules, hidden_dim]
        """
        self.eval()
        embeddings = []
        
        with torch.no_grad():
            for mol_tree in mol_trees:
                mol_emb = self.encode_molecule(mol_tree)
                embeddings.append(mol_emb)
        
        return torch.stack(embeddings)
    
    def save_pretrained_weights(self, save_path: str):
        """
        ä¿å­˜é¢„è®­ç»ƒæƒé‡
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
        """
        torch.save({
            'model_state_dict': self.state_dict(),
            'vocab': self.vocab,
            'atom_vocab': self.atom_vocab,
            'task_weights': self.task_weights,
            'encoder_hidden_dim': self.encoder_hidden_dim
        }, save_path)
        
        print(f"âœ… é¢„è®­ç»ƒæƒé‡å·²ä¿å­˜åˆ°: {save_path}")
    
    @classmethod
    def load_pretrained_weights(cls, load_path: str, **kwargs):
        """
        åŠ è½½é¢„è®­ç»ƒæƒé‡
        
        Args:
            load_path: åŠ è½½è·¯å¾„
            **kwargs: é¢å¤–çš„æ¨¡å‹å‚æ•°
            
        Returns:
            åŠ è½½æƒé‡åçš„æ¨¡å‹
        """
        checkpoint = torch.load(load_path, map_location='cpu')
        
        # ä»checkpointæ¢å¤é…ç½®
        vocab = checkpoint['vocab']
        atom_vocab = checkpoint['atom_vocab']
        task_weights = checkpoint.get('task_weights', None)
        encoder_hidden_dim = checkpoint.get('encoder_hidden_dim', 300)
        
        # åˆ›å»ºæ¨¡å‹
        model = cls(
            vocab=vocab,
            atom_vocab=atom_vocab,
            encoder_hidden_dim=encoder_hidden_dim,
            task_weights=task_weights,
            **kwargs
        )
        
        # åŠ è½½æƒé‡
        model.load_state_dict(checkpoint['model_state_dict'])
        
        print(f"âœ… é¢„è®­ç»ƒæƒé‡å·²ä» {load_path} åŠ è½½")
        
        return model


def create_g2retro_p_model(config: Dict) -> G2RetroPMultiTaskModel:
    """
    å·¥å‚å‡½æ•°ï¼šæ ¹æ®é…ç½®åˆ›å»ºG2Retro-Pæ¨¡å‹
    
    Args:
        config: é…ç½®å­—å…¸
        
    Returns:
        é…ç½®å¥½çš„G2Retro-Pæ¨¡å‹
    """
    # è¿™é‡Œéœ€è¦å®é™…çš„è¯æ±‡è¡¨ï¼Œæš‚æ—¶ä½¿ç”¨å ä½ç¬¦
    vocab = config.get('vocab', None)
    atom_vocab = config.get('atom_vocab', ['C', 'N', 'O', 'S', 'P', 'F', 'Cl', 'Br', 'I'])
    
    if vocab is None:
        # åˆ›å»ºå ä½ç¬¦è¯æ±‡è¡¨
        class PlaceholderVocab:
            def __init__(self, size):
                self.size = size
            def __len__(self):
                return self.size
        vocab = PlaceholderVocab(100)
    
    return G2RetroPMultiTaskModel(
        vocab=vocab,
        atom_vocab=atom_vocab,
        encoder_hidden_dim=config.get('encoder_hidden_dim', 300),
        encoder_depth=config.get('encoder_depth', 3),
        recovery_config=config.get('recovery_config', {}),
        contrastive_config=config.get('contrastive_config', {}),
        task_weights=config.get('task_weights', {}),
        temperature_schedule=config.get('temperature_schedule', 'static'),
        dropout=config.get('dropout', 0.1)
    )


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸš€ æµ‹è¯•G2Retro-På¤šä»»åŠ¡é¢„è®­ç»ƒæ¨¡å‹...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºé…ç½®
    config = {
        'encoder_hidden_dim': 300,
        'recovery_config': {
            'projection_dim': 128,
            'temperature': 0.1
        },
        'contrastive_config': {
            'projection_dim': 256,
            'temperature': 0.07,
            'fusion_method': 'attention'
        },
        'task_weights': {
            'base_task': 1.0,
            'molecular_recovery': 1.0,
            'product_synthon': 1.0
        }
    }
    
    # åˆ›å»ºæ¨¡å‹
    model = create_g2retro_p_model(config).to(device)
    
    print(f"âœ… G2Retro-Pæ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"   æ€»å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # æ¨¡æ‹Ÿæ‰¹æ¬¡æ•°æ®
    batch_size = 8
    batch_data = {
        'product_trees': [None] * batch_size,  # å ä½ç¬¦
        'synthon_trees': [[None, None] for _ in range(batch_size)],  # æ¯ä¸ªäº§ç‰©å¯¹åº”2ä¸ªåˆæˆå­
        'augmented_data': {},
        'reaction_centers': torch.randint(0, 9, (batch_size,)).to(device)
    }
    
    print(f"\nğŸ§ª æµ‹è¯•å¤šä»»åŠ¡å‰å‘ä¼ æ’­...")
    
    # å‰å‘ä¼ æ’­
    results = model(batch_data)
    
    print(f"âœ… æ€»æŸå¤±: {results['total_loss'].item():.4f}")
    print(f"âœ… æŸå¤±è¯¦æƒ…: {results['loss_details']}")
    
    # æµ‹è¯•å„ä¸ªä»»åŠ¡å¤´
    for task_name in ['base_task', 'molecular_recovery', 'product_synthon']:
        if task_name in results:
            task_result = results[task_name]
            print(f"   {task_name}: Loss={task_result['loss'].item():.4f}")
            if 'accuracy' in task_result:
                print(f"                     Acc={task_result['accuracy'].item():.4f}")
    
    # æµ‹è¯•æ¸©åº¦è°ƒåº¦
    print(f"\nğŸŒ¡ï¸  æµ‹è¯•æ¸©åº¦è°ƒåº¦...")
    original_temp = model.molecular_recovery_head.temperature
    model.update_temperature_schedule(epoch=50, max_epochs=100)
    new_temp = model.molecular_recovery_head.temperature
    print(f"   åˆ†å­æ¢å¤æ¸©åº¦: {original_temp:.4f} -> {new_temp:.4f}")
    
    # æµ‹è¯•æƒé‡è°ƒæ•´
    print(f"\nâš–ï¸  æµ‹è¯•åŠ¨æ€æƒé‡è°ƒæ•´...")
    model.set_task_weights({'molecular_recovery': 2.0, 'product_synthon': 0.5})
    print(f"   æ›´æ–°åçš„ä»»åŠ¡æƒé‡: {model.task_weights}")
    
    print("ğŸ‰ G2Retro-På¤šä»»åŠ¡æ¨¡å‹æµ‹è¯•å®Œæˆï¼") 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ ä»»åŠ¡å¤´ - G2Retro-Pé¢„è®­ç»ƒæ¶æ„æ ¸å¿ƒç»„ä»¶

è¿™ä¸ªæ¨¡å—å®ç°äº†äº§ç‰©åˆ†å­ä¸åˆæˆå­ç»„åˆä¹‹é—´çš„å¯¹æ¯”å­¦ä¹ ï¼Œå­¦ä¹ ååº”çš„é€†å‘æ˜ å°„å…³ç³»ã€‚
é€šè¿‡å¯¹æ¯”å­¦ä¹ ä½¿å¾—çœŸå®çš„äº§ç‰©-åˆæˆå­é…å¯¹åœ¨è¡¨ç¤ºç©ºé—´ä¸­æ›´ç›¸ä¼¼ï¼Œè€Œé”™è¯¯é…å¯¹æ›´ä¸ç›¸ä¼¼ã€‚

å‚è€ƒæ–‡çŒ®ï¼š
- PMSR: Learning Chemical Rules of Retrosynthesis with Pre-training  
- MolCLR: Molecular Contrastive Learning of Representations via Graph Neural Networks
- RetroExplainer: Retrosynthesis prediction with an interpretable deep-learning framework

æ ¸å¿ƒæ€æƒ³ï¼š
1. çœŸå®çš„äº§ç‰©-åˆæˆå­é…å¯¹åº”è¯¥åœ¨è¡¨ç¤ºç©ºé—´ä¸­ç›¸ä¼¼ï¼ˆæ­£æ ·æœ¬ï¼‰
2. é”™è¯¯çš„äº§ç‰©-åˆæˆå­é…å¯¹åº”è¯¥ä¸ç›¸ä¼¼ï¼ˆè´Ÿæ ·æœ¬ï¼‰
3. é€šè¿‡InfoNCEæŸå¤±å­¦ä¹ ååº”çš„é€†å‘åŒ–å­¦è§„å¾‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Dict, Optional, List, Union
import math

class ProductSynthonContrastiveHead(nn.Module):
    """
    äº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ ä»»åŠ¡å¤´
    
    å­¦ä¹ äº§ç‰©åˆ†å­ä¸åˆæˆå­ç»„åˆä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œè¿™æ˜¯é€†åˆæˆé¢„æµ‹çš„æ ¸å¿ƒä»»åŠ¡ã€‚
    é€šè¿‡å¯¹æ¯”å­¦ä¹ ä½¿æ¨¡å‹ç†è§£"è¿™ä¸ªäº§ç‰©åº”è¯¥ç”±å“ªäº›åˆæˆå­ç»„æˆ"çš„åŒ–å­¦è§„å¾‹ã€‚
    """
    
    def __init__(self, 
                 product_input_dim: int = 300,      # äº§ç‰©åˆ†å­å›¾åµŒå…¥ç»´åº¦
                 synthon_input_dim: int = 300,      # åˆæˆå­å›¾åµŒå…¥ç»´åº¦
                 projection_dim: int = 256,         # æŠ•å½±ç©ºé—´ç»´åº¦
                 hidden_dim: int = 512,            # éšè—å±‚ç»´åº¦
                 temperature: float = 0.07,         # InfoNCEæ¸©åº¦å‚æ•°
                 dropout: float = 0.1,
                 fusion_method: str = "attention"):  # åˆæˆå­èåˆæ–¹æ³•
        """
        åˆå§‹åŒ–äº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ ä»»åŠ¡å¤´
        
        Args:
            product_input_dim: äº§ç‰©åˆ†å­å›¾åµŒå…¥ç»´åº¦
            synthon_input_dim: åˆæˆå­å›¾åµŒå…¥ç»´åº¦  
            projection_dim: æŠ•å½±åçš„è¡¨ç¤ºç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            temperature: InfoNCEæŸå¤±çš„æ¸©åº¦å‚æ•°
            dropout: Dropoutæ¯”ä¾‹
            fusion_method: åˆæˆå­èåˆæ–¹æ³• ('attention', 'mean', 'max')
        """
        super(ProductSynthonContrastiveHead, self).__init__()
        
        self.product_input_dim = product_input_dim
        self.synthon_input_dim = synthon_input_dim
        self.projection_dim = projection_dim
        self.temperature = temperature
        self.fusion_method = fusion_method
        
        # äº§ç‰©åˆ†å­æŠ•å½±å¤´ - å‚è€ƒMolCLRæ¶æ„
        self.product_projector = nn.Sequential(
            nn.Linear(product_input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, projection_dim),
            nn.BatchNorm1d(projection_dim)
        )
        
        # åˆæˆå­æŠ•å½±å¤´ - å¯¹ç§°è®¾è®¡
        self.synthon_projector = nn.Sequential(
            nn.Linear(synthon_input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim), 
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, projection_dim),
            nn.BatchNorm1d(projection_dim)
        )
        
        # G2Retro-Pè®¾è®¡ï¼šä¸éœ€è¦åˆæˆå­èåˆæ¨¡å—
        # åˆæˆå­ç»„åˆå·²ç»åœ¨æ•°æ®é¢„å¤„ç†é˜¶æ®µå®Œæˆï¼Œè¿™é‡Œåªå¤„ç†å•ä¸€çš„åˆæˆå­ç»„åˆåˆ†å­å›¾
        # fusion_methodå‚æ•°ä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼Œä½†å®é™…ä¸ä½¿ç”¨
        if fusion_method not in ["attention", "mean", "max"]:
            raise ValueError(f"æœªæ”¯æŒçš„èåˆæ–¹æ³•: {fusion_method}")
        self.fusion_method = fusion_method  # ä¿å­˜ç”¨äºæ—¥å¿—è®°å½•
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡ - å‚è€ƒMolCLRåˆå§‹åŒ–ç­–ç•¥"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def fuse_synthon_embeddings(self, 
                               synthon_embeddings: torch.Tensor,
                               synthon_masks: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        èåˆå¤šä¸ªåˆæˆå­çš„è¡¨ç¤ºä¸ºå•ä¸€è¡¨ç¤º
        
        Args:
            synthon_embeddings: åˆæˆå­åµŒå…¥ [batch_size, max_synthons, projection_dim]
            synthon_masks: åˆæˆå­æ©ç  [batch_size, max_synthons] (å¯é€‰)
            
        Returns:
            èåˆåçš„åˆæˆå­è¡¨ç¤º [batch_size, projection_dim]
        """
        batch_size, max_synthons, embedding_dim = synthon_embeddings.shape
        
        if self.fusion_method == "attention":
            # æ·»åŠ ä½ç½®ç¼–ç 
            pos_encoding = self.position_encoding[:max_synthons].unsqueeze(0).expand(batch_size, -1, -1)
            synthon_embeddings = synthon_embeddings + pos_encoding
            
            # è‡ªæ³¨æ„åŠ›èåˆ
            if synthon_masks is not None:
                # è½¬æ¢maskæ ¼å¼ï¼šTrueè¡¨ç¤ºæœ‰æ•ˆä½ç½®ï¼ŒFalseè¡¨ç¤ºå¡«å……ä½ç½®
                attention_mask = ~synthon_masks  # MultiheadAttentionæœŸæœ›Trueè¡¨ç¤ºéœ€è¦maskçš„ä½ç½®
            else:
                attention_mask = None
            
            # è½¬æ¢ä¸ºåºåˆ—ä¼˜å…ˆæ ¼å¼ (seq_len, batch_size, embed_dim)
            synthon_embeddings_t = synthon_embeddings.transpose(0, 1)  # [max_synthons, batch_size, projection_dim]
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªåˆæˆå­ä½œä¸ºqueryï¼Œæ‰€æœ‰åˆæˆå­ä½œä¸ºkeyå’Œvalue
            query = synthon_embeddings_t[0:1, :, :]  # [1, batch_size, projection_dim]
            
            attended_output, attention_weights = self.synthon_attention(
                query=query,
                key=synthon_embeddings_t,
                value=synthon_embeddings_t,
                key_padding_mask=attention_mask
            )
            
            # è¿”å›æ³¨æ„åŠ›åŠ æƒçš„è¡¨ç¤ºï¼Œè½¬æ¢å›batchä¼˜å…ˆæ ¼å¼
            fused_embedding = attended_output.squeeze(0)  # [batch_size, projection_dim]
            
        elif self.fusion_method == "mean":
            # å‡å€¼èšåˆ
            if synthon_masks is not None:
                # maskedå‡å€¼
                masked_embeddings = synthon_embeddings * synthon_masks.unsqueeze(-1)
                valid_counts = synthon_masks.sum(dim=1, keepdim=True).float()
                valid_counts = torch.clamp(valid_counts, min=1.0)  # é¿å…é™¤é›¶
                fused_embedding = masked_embeddings.sum(dim=1) / valid_counts
            else:
                fused_embedding = synthon_embeddings.mean(dim=1)
                
        elif self.fusion_method == "max":
            # æœ€å¤§å€¼èšåˆ
            if synthon_masks is not None:
                # å¯¹æ— æ•ˆä½ç½®è®¾ç½®å¾ˆå°çš„å€¼
                masked_embeddings = synthon_embeddings.clone()
                masked_embeddings[~synthon_masks] = float('-inf')
                fused_embedding = masked_embeddings.max(dim=1)[0]
            else:
                fused_embedding = synthon_embeddings.max(dim=1)[0]
        
        return fused_embedding
    
    def forward(self, 
                product_embeddings: torch.Tensor,
                synthon_embeddings: torch.Tensor,
                pretrain_infos: List[Dict],
                synthon_masks: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, float]:
        """
        å‰å‘ä¼ æ’­ - è®¡ç®—äº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ æŸå¤±
        
        Args:
            product_embeddings: äº§ç‰©åˆ†å­å›¾åµŒå…¥ [batch_size, product_input_dim]
            synthon_embeddings: åˆæˆå­å›¾åµŒå…¥ 
                              - 2D: [batch_size, synthon_input_dim] (å•åˆæˆå­æˆ–å·²èåˆ)
                              - 3D: [batch_size, num_synthons, synthon_input_dim] (å¤šåˆæˆå­)
            pretrain_infos: é¢„è®­ç»ƒä¿¡æ¯åˆ—è¡¨
            synthon_masks: åˆæˆå­æœ‰æ•ˆæ€§æ©ç  [batch_size, max_synthons] (å¯é€‰)
            
        Returns:
            (loss, accuracy): æŸå¤±å€¼å’Œå‡†ç¡®ç‡
        """
        batch_size = product_embeddings.size(0)
        device = product_embeddings.device
        
        # æŠ•å½±äº§ç‰©åµŒå…¥
        product_proj = self.product_projector(product_embeddings)  # [batch_size, projection_dim]
        
        # å¤„ç†åˆæˆå­åµŒå…¥
        # G2Retro-Pè®¾è®¡ï¼šåˆæˆå­ç»„åˆå·²ç»åœ¨é¢„å¤„ç†é˜¶æ®µç»„åˆä¸ºå•ä¸€åˆ†å­å›¾
        # GMPNç¼–ç å™¨æ€»æ˜¯è¾“å‡º2Då¼ é‡ [batch_size, hidden_size]
        if synthon_embeddings.dim() != 2:
            raise ValueError(f"æœŸæœ›åˆæˆå­åµŒå…¥ä¸º2Då¼ é‡ [batch_size, hidden_size]ï¼Œä½†å¾—åˆ° {synthon_embeddings.shape}")
        
        synthon_proj = self.synthon_projector(synthon_embeddings)  # [batch_size, projection_dim]
        
        # L2å½’ä¸€åŒ–ï¼ˆå¯¹æ¯”å­¦ä¹ çš„æ ‡å‡†åšæ³•ï¼‰
        product_proj = F.normalize(product_proj, p=2, dim=1)
        synthon_proj = F.normalize(synthon_proj, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.mm(product_proj, synthon_proj.t()) / self.temperature
        
        # è®¡ç®—InfoNCEæŸå¤±
        contrastive_loss = self._compute_infonce_loss(product_proj, synthon_proj)
        
        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = self._compute_accuracy(similarity_matrix)
        
        return contrastive_loss, accuracy.item()
    
    def _compute_infonce_loss(self, 
                             product_proj: torch.Tensor, 
                             synthon_proj: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—äº§ç‰©-åˆæˆå­InfoNCEå¯¹æ¯”å­¦ä¹ æŸå¤±
        
        æŸå¤±å‡½æ•°æ—¨åœ¨ä½¿çœŸå®çš„äº§ç‰©-åˆæˆå­é…å¯¹ç›¸ä¼¼åº¦æ›´é«˜ï¼Œ
        è€Œéšæœºé…å¯¹çš„ç›¸ä¼¼åº¦æ›´ä½ã€‚
        
        Args:
            product_proj: å½’ä¸€åŒ–çš„äº§ç‰©æŠ•å½±è¡¨ç¤º [batch_size, projection_dim]
            synthon_proj: å½’ä¸€åŒ–çš„åˆæˆå­æŠ•å½±è¡¨ç¤º [batch_size, projection_dim]
            
        Returns:
            InfoNCEæŸå¤±å€¼
        """
        batch_size = product_proj.size(0)
        device = product_proj.device
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        # è¡Œï¼šäº§ç‰©ï¼Œåˆ—ï¼šåˆæˆå­
        similarity_matrix = torch.mm(product_proj, synthon_proj.t()) / self.temperature
        
        # åˆ›å»ºæ ‡ç­¾ï¼šå¯¹è§’çº¿å…ƒç´ æ˜¯æ­£æ ·æœ¬é…å¯¹
        labels = torch.arange(batch_size, device=device)
        
        # è®¡ç®—ä¸¤ä¸ªæ–¹å‘çš„äº¤å‰ç†µæŸå¤±
        # 1. äº§ç‰©åˆ°åˆæˆå­ï¼šç»™å®šäº§ç‰©ï¼Œæ‰¾åˆ°å¯¹åº”çš„åˆæˆå­
        loss_product_to_synthon = F.cross_entropy(similarity_matrix, labels)
        
        # 2. åˆæˆå­åˆ°äº§ç‰©ï¼šç»™å®šåˆæˆå­ï¼Œæ‰¾åˆ°å¯¹åº”çš„äº§ç‰©
        loss_synthon_to_product = F.cross_entropy(similarity_matrix.t(), labels)
        
        # å¯¹ç§°InfoNCEæŸå¤±
        total_loss = (loss_product_to_synthon + loss_synthon_to_product) / 2
        
        return total_loss
    
    def _compute_accuracy(self, similarity_matrix: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—top-1å‡†ç¡®ç‡ - äº§ç‰©-åˆæˆå­åŒ¹é…å‡†ç¡®ç‡
        
        Args:
            similarity_matrix: ç›¸ä¼¼åº¦çŸ©é˜µ [batch_size, batch_size]
            
        Returns:
            å¹³å‡å‡†ç¡®ç‡
        """
        batch_size = similarity_matrix.size(0)
        
        # äº§ç‰©åˆ°åˆæˆå­çš„top-1å‡†ç¡®ç‡
        _, top_indices_p2s = torch.topk(similarity_matrix, k=1, dim=1)
        correct_p2s = (top_indices_p2s.squeeze() == torch.arange(batch_size, device=similarity_matrix.device)).float()
        
        # åˆæˆå­åˆ°äº§ç‰©çš„top-1å‡†ç¡®ç‡  
        _, top_indices_s2p = torch.topk(similarity_matrix.t(), k=1, dim=1)
        correct_s2p = (top_indices_s2p.squeeze() == torch.arange(batch_size, device=similarity_matrix.device)).float()
        
        # å¹³å‡å‡†ç¡®ç‡
        accuracy = (correct_p2s.mean() + correct_s2p.mean()) / 2
        
        return accuracy
    
    def predict_synthons_for_product(self, 
                                   product_embedding: torch.Tensor,
                                   candidate_synthon_embeddings: torch.Tensor,
                                   top_k: int = 5) -> Dict[str, torch.Tensor]:
        """
        ç»™å®šäº§ç‰©åˆ†å­ï¼Œé¢„æµ‹æœ€å¯èƒ½çš„åˆæˆå­ç»„åˆ
        
        Args:
            product_embedding: å•ä¸ªäº§ç‰©åµŒå…¥ [1, product_input_dim]
            candidate_synthon_embeddings: å€™é€‰åˆæˆå­åµŒå…¥ [num_candidates, synthon_input_dim]
            top_k: è¿”å›top-kç»“æœ
            
        Returns:
            åŒ…å«é¢„æµ‹ç»“æœçš„å­—å…¸
        """
        self.eval()
        with torch.no_grad():
            # æŠ•å½±å¹¶å½’ä¸€åŒ–
            product_proj = F.normalize(self.product_projector(product_embedding), p=2, dim=1)
            
            if candidate_synthon_embeddings.dim() == 2:
                synthon_proj = F.normalize(self.synthon_projector(candidate_synthon_embeddings), p=2, dim=1)
            else:
                # å¤„ç†å¤šåˆæˆå­å€™é€‰
                batch_size, max_synthons, synthon_dim = candidate_synthon_embeddings.shape
                synthon_flat = candidate_synthon_embeddings.view(-1, synthon_dim)
                synthon_proj_flat = self.synthon_projector(synthon_flat)
                synthon_proj_3d = synthon_proj_flat.view(batch_size, max_synthons, self.projection_dim)
                synthon_proj = F.normalize(self.fuse_synthon_embeddings(synthon_proj_3d), p=2, dim=1)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = torch.mm(product_proj, synthon_proj.t()).squeeze(0)
            
            # è·å–top-kç»“æœ
            top_similarities, top_indices = torch.topk(similarities, k=min(top_k, len(similarities)))
            
            return {
                'top_indices': top_indices,
                'top_similarities': top_similarities,
                'all_similarities': similarities
            }


def create_product_synthon_contrastive_head(config: Dict) -> ProductSynthonContrastiveHead:
    """
    å·¥å‚å‡½æ•°ï¼šæ ¹æ®é…ç½®åˆ›å»ºäº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ ä»»åŠ¡å¤´
    
    Args:
        config: é…ç½®å­—å…¸
        
    Returns:
        é…ç½®å¥½çš„å¯¹æ¯”å­¦ä¹ ä»»åŠ¡å¤´
    """
    return ProductSynthonContrastiveHead(
        product_input_dim=config.get('product_input_dim', 300),
        synthon_input_dim=config.get('synthon_input_dim', 300),
        projection_dim=config.get('projection_dim', 256),
        hidden_dim=config.get('hidden_dim', 512),
        temperature=config.get('temperature', 0.07),
        dropout=config.get('dropout', 0.1),
        fusion_method=config.get('fusion_method', 'attention')
    )


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•äº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ ä»»åŠ¡å¤´...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¨¡å‹
    contrastive_head = ProductSynthonContrastiveHead(
        product_input_dim=300,
        synthon_input_dim=300,
        projection_dim=256,
        hidden_dim=512,
        temperature=0.07,
        fusion_method='attention'
    ).to(device)
    
    print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"   å‚æ•°æ€»æ•°: {sum(p.numel() for p in contrastive_head.parameters()):,}")
    
    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 16
    max_synthons = 4
    
    # äº§ç‰©åµŒå…¥
    product_embeddings = torch.randn(batch_size, 300).to(device)
    
    # åˆæˆå­åµŒå…¥ï¼ˆå¤šåˆæˆå­æƒ…å†µï¼‰
    synthon_embeddings = torch.randn(batch_size, max_synthons, 300).to(device)
    
    # åˆæˆå­æ©ç ï¼ˆæ¨¡æ‹Ÿä¸åŒååº”æœ‰ä¸åŒæ•°é‡çš„åˆæˆå­ï¼‰
    synthon_masks = torch.ones(batch_size, max_synthons, dtype=torch.bool).to(device)
    for i in range(batch_size):
        num_valid = torch.randint(1, max_synthons + 1, (1,)).item()
        synthon_masks[i, num_valid:] = False
    
    print(f"\nğŸ§ª æµ‹è¯•å‰å‘ä¼ æ’­...")
    
    # å‰å‘ä¼ æ’­
    results = contrastive_head(product_embeddings, synthon_embeddings, synthon_masks)
    
    print(f"âœ… å¯¹æ¯”å­¦ä¹ æŸå¤±: {results['loss'].item():.4f}")
    print(f"âœ… Top-1å‡†ç¡®ç‡: {results['accuracy'].item():.4f}")
    print(f"âœ… äº§ç‰©æŠ•å½±å½¢çŠ¶: {results['product_proj'].shape}")
    print(f"âœ… åˆæˆå­æŠ•å½±å½¢çŠ¶: {results['synthon_proj'].shape}")
    print(f"âœ… ç›¸ä¼¼åº¦çŸ©é˜µå½¢çŠ¶: {results['similarity_matrix'].shape}")
    
    # æµ‹è¯•é¢„æµ‹åŠŸèƒ½
    print(f"\nğŸ” æµ‹è¯•åˆæˆå­é¢„æµ‹...")
    
    # æ¨¡æ‹Ÿå€™é€‰åˆæˆå­
    num_candidates = 50
    candidate_synthons = torch.randn(num_candidates, 300).to(device)
    
    # é¢„æµ‹
    prediction_results = contrastive_head.predict_synthons_for_product(
        product_embeddings[:1], 
        candidate_synthons,
        top_k=5
    )
    
    print(f"âœ… Top-5ç›¸ä¼¼åº¦: {prediction_results['top_similarities']}")
    print(f"âœ… Top-5ç´¢å¼•: {prediction_results['top_indices']}")
    
    # æµ‹è¯•ä¸åŒèåˆæ–¹æ³•
    print(f"\nğŸ”„ æµ‹è¯•ä¸åŒåˆæˆå­èåˆæ–¹æ³•...")
    
    for fusion_method in ['attention', 'mean', 'max']:
        test_head = ProductSynthonContrastiveHead(
            fusion_method=fusion_method
        ).to(device)
        
        with torch.no_grad():
            test_results = test_head(product_embeddings, synthon_embeddings, synthon_masks)
        
        print(f"  {fusion_method:10s}: Loss={test_results['loss'].item():.4f}, "
              f"Acc={test_results['accuracy'].item():.4f}")
    
    print("ğŸ‰ äº§ç‰©-åˆæˆå­å¯¹æ¯”å­¦ä¹ ä»»åŠ¡å¤´æµ‹è¯•å®Œæˆï¼") 
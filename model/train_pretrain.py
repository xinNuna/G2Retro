#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SynthRetro-Pé¢„è®­ç»ƒè®­ç»ƒè„šæœ¬

è¿™æ˜¯ä¸€ä¸ªæ–°å¢çš„æ–‡ä»¶ï¼Œåº”è¯¥æ”¾åœ¨ model/ ç›®å½•ä¸‹ï¼Œå‘½åä¸º train_pretrain.py
å®ç°å¤šä»»åŠ¡é¢„è®­ç»ƒçš„å®Œæ•´è®­ç»ƒæµç¨‹ï¼ŒåŒ…å«æŸå¤±å¯è§†åŒ–
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import os
import time
import json
import argparse
from datetime import datetime
import numpy as np
from collections import defaultdict
import gc
import matplotlib
matplotlib.use('Agg')  # æ— GUIåç«¯ï¼Œé€‚åˆæœåŠ¡å™¨ç¯å¢ƒ
import matplotlib.pyplot as plt
import seaborn as sns

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from synthretro_pretrain import create_synthretro_pretrain_model
from pretrain_dataloader import create_pretrain_dataloaders

class PretrainTrainer:
    """SynthRetro-Pé¢„è®­ç»ƒè®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')
        print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = create_synthretro_pretrain_model(config['model'])
        self.model.to(self.device)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=config['training']['learning_rate'],
            weight_decay=config['training']['weight_decay']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer,
            step_size=config['training']['lr_step_size'],
            gamma=config['training']['lr_gamma']
        )
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_loss = float('inf')
        self.train_losses = []
        self.val_losses = []
        
        # æŸå¤±å†å²è®°å½•ï¼ˆè¯¦ç»†ï¼‰
        self.loss_history = {
            'epoch': [],
            'train_total': [],
            'train_base': [],
            'train_recovery': [],
            'train_contrastive': [],
            'val_total': [],
            'val_base': [],
            'val_recovery': [],
            'val_contrastive': [],
            'learning_rate': []
        }
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = config['training']['output_dir']
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åˆ›å»ºplotsç›®å½•
        self.plots_dir = os.path.join(self.output_dir, 'plots')
        os.makedirs(self.plots_dir, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        config_path = os.path.join(self.output_dir, 'config.json')
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        epoch_losses = defaultdict(list)
        
        for batch_idx, batch in enumerate(train_loader):
            if batch is None:
                continue
                
            try:
                # æ•°æ®ç§»åˆ°GPU
                batch = self.move_batch_to_device(batch)
                
                # å‰å‘ä¼ æ’­
                self.optimizer.zero_grad()
                losses, predictions = self.model(batch)
                
                # åå‘ä¼ æ’­
                total_loss = losses['total_loss']
                total_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                
                # è®°å½•æŸå¤±
                for loss_name, loss_value in losses.items():
                    if isinstance(loss_value, torch.Tensor):
                        epoch_losses[loss_name].append(loss_value.item())
                
                # æ‰“å°è¿›åº¦
                if batch_idx % self.config['training']['log_interval'] == 0:
                    print(f"  æ‰¹æ¬¡ {batch_idx}/{len(train_loader)}: "
                          f"æ€»æŸå¤±={total_loss.item():.4f}")
                
                # å†…å­˜æ¸…ç†
                del losses, predictions, batch
                torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"âš ï¸ æ‰¹æ¬¡ {batch_idx} è®­ç»ƒå¤±è´¥: {e}")
                continue
        
        # è®¡ç®—epochå¹³å‡æŸå¤±
        avg_losses = {}
        for loss_name, loss_list in epoch_losses.items():
            if loss_list:
                avg_losses[loss_name] = np.mean(loss_list)
        
        return avg_losses
    
    def validate_epoch(self, val_loader):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        epoch_losses = defaultdict(list)
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(val_loader):
                if batch is None:
                    continue
                    
                try:
                    # æ•°æ®ç§»åˆ°GPU
                    batch = self.move_batch_to_device(batch)
                    
                    # å‰å‘ä¼ æ’­
                    losses, predictions = self.model(batch)
                    
                    # è®°å½•æŸå¤±
                    for loss_name, loss_value in losses.items():
                        if isinstance(loss_value, torch.Tensor):
                            epoch_losses[loss_name].append(loss_value.item())
                    
                    
                except Exception as e:
                    print(f"âš ï¸ éªŒè¯æ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                    continue
        
        # è®¡ç®—epochå¹³å‡æŸå¤±
        avg_losses = {}
        for loss_name, loss_list in epoch_losses.items():
            if loss_list:
                avg_losses[loss_name] = np.mean(loss_list)
        
        return avg_losses
    
    def plot_loss_curves(self):
        """ç»˜åˆ¶å¹¶ä¿å­˜æŸå¤±æ›²çº¿å›¾"""
        if len(self.loss_history['epoch']) < 2:
            return
        
        # è®¾ç½®ç»˜å›¾é£æ ¼
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        epochs = self.loss_history['epoch']
        
        # 1. æ€»æŸå¤±å¯¹æ¯”
        ax1 = axes[0, 0]
        ax1.plot(epochs, self.loss_history['train_total'], 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
        ax1.plot(epochs, self.loss_history['val_total'], 'r-', label='éªŒè¯æŸå¤±', linewidth=2)
        ax1.set_title('æ€»æŸå¤±å˜åŒ–æ›²çº¿', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('æŸå¤±å€¼')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. å„ä»»åŠ¡è®­ç»ƒæŸå¤±
        ax2 = axes[0, 1]
        if self.loss_history['train_base']:
            ax2.plot(epochs, self.loss_history['train_base'], 'g-', label='åŸºç¡€ä»»åŠ¡', linewidth=2)
        if self.loss_history['train_recovery']:
            ax2.plot(epochs, self.loss_history['train_recovery'], 'm-', label='åˆ†å­æ¢å¤', linewidth=2)
        if self.loss_history['train_contrastive']:
            ax2.plot(epochs, self.loss_history['train_contrastive'], 'c-', label='å¯¹æ¯”å­¦ä¹ ', linewidth=2)
        ax2.set_title('è®­ç»ƒæŸå¤±åˆ†è§£', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('æŸå¤±å€¼')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. å„ä»»åŠ¡éªŒè¯æŸå¤±
        ax3 = axes[1, 0]
        if self.loss_history['val_base']:
            ax3.plot(epochs, self.loss_history['val_base'], 'g--', label='åŸºç¡€ä»»åŠ¡', linewidth=2)
        if self.loss_history['val_recovery']:
            ax3.plot(epochs, self.loss_history['val_recovery'], 'm--', label='åˆ†å­æ¢å¤', linewidth=2)
        if self.loss_history['val_contrastive']:
            ax3.plot(epochs, self.loss_history['val_contrastive'], 'c--', label='å¯¹æ¯”å­¦ä¹ ', linewidth=2)
        ax3.set_title('éªŒè¯æŸå¤±åˆ†è§£', fontsize=14, fontweight='bold')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('æŸå¤±å€¼')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. å­¦ä¹ ç‡å˜åŒ–
        ax4 = axes[1, 1]
        ax4.plot(epochs, self.loss_history['learning_rate'], 'orange', linewidth=2)
        ax4.set_title('å­¦ä¹ ç‡å˜åŒ–', fontsize=14, fontweight='bold')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('å­¦ä¹ ç‡')
        ax4.set_yscale('log')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_path = os.path.join(self.plots_dir, f'loss_curves_{timestamp}.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # åŒæ—¶ä¿å­˜æœ€æ–°ç‰ˆæœ¬
        latest_path = os.path.join(self.plots_dir, 'loss_curves_latest.png')
        plt.figure(figsize=(15, 12))
        
        # é‡æ–°ç»˜åˆ¶å¹¶ä¿å­˜æœ€æ–°ç‰ˆæœ¬
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # å¤åˆ¶ä¸Šé¢çš„ç»˜å›¾é€»è¾‘
        epochs = self.loss_history['epoch']
        
        # æ€»æŸå¤±
        ax1 = axes[0, 0]
        ax1.plot(epochs, self.loss_history['train_total'], 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
        ax1.plot(epochs, self.loss_history['val_total'], 'r-', label='éªŒè¯æŸå¤±', linewidth=2)
        ax1.set_title('æ€»æŸå¤±å˜åŒ–æ›²çº¿', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('æŸå¤±å€¼')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # è®­ç»ƒæŸå¤±åˆ†è§£
        ax2 = axes[0, 1]
        if self.loss_history['train_base']:
            ax2.plot(epochs, self.loss_history['train_base'], 'g-', label='åŸºç¡€ä»»åŠ¡', linewidth=2)
        if self.loss_history['train_recovery']:
            ax2.plot(epochs, self.loss_history['train_recovery'], 'm-', label='åˆ†å­æ¢å¤', linewidth=2)
        if self.loss_history['train_contrastive']:
            ax2.plot(epochs, self.loss_history['train_contrastive'], 'c-', label='å¯¹æ¯”å­¦ä¹ ', linewidth=2)
        ax2.set_title('è®­ç»ƒæŸå¤±åˆ†è§£', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('æŸå¤±å€¼')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # éªŒè¯æŸå¤±åˆ†è§£
        ax3 = axes[1, 0]
        if self.loss_history['val_base']:
            ax3.plot(epochs, self.loss_history['val_base'], 'g--', label='åŸºç¡€ä»»åŠ¡', linewidth=2)
        if self.loss_history['val_recovery']:
            ax3.plot(epochs, self.loss_history['val_recovery'], 'm--', label='åˆ†å­æ¢å¤', linewidth=2)
        if self.loss_history['val_contrastive']:
            ax3.plot(epochs, self.loss_history['val_contrastive'], 'c--', label='å¯¹æ¯”å­¦ä¹ ', linewidth=2)
        ax3.set_title('éªŒè¯æŸå¤±åˆ†è§£', fontsize=14, fontweight='bold')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('æŸå¤±å€¼')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # å­¦ä¹ ç‡
        ax4 = axes[1, 1]
        ax4.plot(epochs, self.loss_history['learning_rate'], 'orange', linewidth=2)
        ax4.set_title('å­¦ä¹ ç‡å˜åŒ–', fontsize=14, fontweight='bold')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('å­¦ä¹ ç‡')
        ax4.set_yscale('log')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(latest_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š æŸå¤±æ›²çº¿å·²ä¿å­˜: {plot_path}")
        print(f"ğŸ“Š æœ€æ–°æŸå¤±æ›²çº¿: {latest_path}")
    
    def save_loss_data(self):
        """ä¿å­˜æŸå¤±æ•°æ®ä¸ºCSVæ ¼å¼"""
        import pandas as pd
        
        # åˆ›å»ºDataFrame
        loss_df = pd.DataFrame(self.loss_history)
        
        # ä¿å­˜ä¸ºCSV
        csv_path = os.path.join(self.output_dir, 'loss_history.csv')
        loss_df.to_csv(csv_path, index=False)
        
        # ä¿å­˜ä¸ºJSONï¼ˆæ›´è¯¦ç»†çš„æ ¼å¼ï¼‰
        json_path = os.path.join(self.output_dir, 'loss_history.json')
        with open(json_path, 'w') as f:
            json.dump(self.loss_history, f, indent=2)
        
        print(f"ğŸ“ˆ æŸå¤±æ•°æ®å·²ä¿å­˜: {csv_path}")
    
    def update_loss_history(self, epoch, train_losses, val_losses, lr):
        """æ›´æ–°æŸå¤±å†å²è®°å½•"""
        self.loss_history['epoch'].append(epoch)
        self.loss_history['learning_rate'].append(lr)
        
        # è®­ç»ƒæŸå¤±
        self.loss_history['train_total'].append(train_losses.get('total_loss', 0))
        self.loss_history['train_base'].append(train_losses.get('base_loss', 0))
        self.loss_history['train_recovery'].append(train_losses.get('recovery_loss', 0))
        self.loss_history['train_contrastive'].append(train_losses.get('contrastive_loss', 0))
        
        # éªŒè¯æŸå¤±
        self.loss_history['val_total'].append(val_losses.get('total_loss', 0))
        self.loss_history['val_base'].append(val_losses.get('base_loss', 0))
        self.loss_history['val_recovery'].append(val_losses.get('recovery_loss', 0))
        self.loss_history['val_contrastive'].append(val_losses.get('contrastive_loss', 0))
    
    def move_batch_to_device(self, batch):
        """å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
        if isinstance(batch, dict):
            return {k: self.move_batch_to_device(v) for k, v in batch.items()}
        elif isinstance(batch, torch.Tensor):
            return batch.to(self.device)
        elif isinstance(batch, list):
            return [self.move_batch_to_device(item) for item in batch]
        else:
            return batch
    
    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_loss': self.best_loss,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'loss_history': self.loss_history,
            'config': self.config
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        latest_path = os.path.join(self.output_dir, 'latest_checkpoint.pt')
        torch.save(checkpoint, latest_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = os.path.join(self.output_dir, 'best_model.pt')
            torch.save(checkpoint, best_path)
            print(f"ğŸ’ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
        
        # å®šæœŸä¿å­˜
        if epoch % self.config['training']['save_interval'] == 0:
            epoch_path = os.path.join(self.output_dir, f'checkpoint_epoch_{epoch}.pt')
            torch.save(checkpoint, epoch_path)
    
    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if os.path.exists(checkpoint_path):
            print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            self.current_epoch = checkpoint['epoch']
            self.best_loss = checkpoint['best_loss']
            self.train_losses = checkpoint.get('train_losses', [])
            self.val_losses = checkpoint.get('val_losses', [])
            self.loss_history = checkpoint.get('loss_history', self.loss_history)
            
            print(f"âœ… ä»epoch {self.current_epoch}ç»§ç»­è®­ç»ƒ")
            return True
        return False
    
    def train(self, train_loader, val_loader):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("ğŸš€ å¼€å§‹SynthRetro-Pé¢„è®­ç»ƒ")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"ğŸ”§ è®­ç»ƒè®¾å¤‡: {self.device}")
        print(f"ğŸ“ˆ è®­ç»ƒè½®æ•°: {self.config['training']['epochs']}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ“Š å›¾ç‰‡ä¿å­˜: {self.plots_dir}")
        
        start_time = time.time()
        
        for epoch in range(self.current_epoch, self.config['training']['epochs']):
            epoch_start_time = time.time()
            
            print(f"\nğŸ“… Epoch {epoch+1}/{self.config['training']['epochs']}")
            print("-" * 60)
            
            # è®­ç»ƒ
            print("ğŸ”„ è®­ç»ƒé˜¶æ®µ...")
            train_losses = self.train_epoch(train_loader)
            
            # éªŒè¯
            print("ğŸ” éªŒè¯é˜¶æ®µ...")
            val_losses = self.validate_epoch(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            current_lr = self.scheduler.get_last_lr()[0]
            self.scheduler.step()
            
            # è®°å½•æŸå¤±
            self.train_losses.append(train_losses)
            self.val_losses.append(val_losses)
            
            # æ›´æ–°è¯¦ç»†æŸå¤±å†å²
            self.update_loss_history(epoch + 1, train_losses, val_losses, current_lr)
            
            # æ‰“å°ç»“æœ
            epoch_time = time.time() - epoch_start_time
            print(f"\nğŸ“Š Epoch {epoch+1} ç»“æœ:")
            print(f"   è®­ç»ƒæŸå¤±: {train_losses.get('total_loss', 0):.4f}")
            if 'base_loss' in train_losses:
                print(f"     - åŸºç¡€ä»»åŠ¡: {train_losses['base_loss']:.4f}")
            if 'recovery_loss' in train_losses:
                print(f"     - åˆ†å­æ¢å¤: {train_losses['recovery_loss']:.4f}")
            if 'contrastive_loss' in train_losses:
                print(f"     - å¯¹æ¯”å­¦ä¹ : {train_losses['contrastive_loss']:.4f}")
            print(f"   éªŒè¯æŸå¤±: {val_losses.get('total_loss', 0):.4f}")
            print(f"   å­¦ä¹ ç‡: {current_lr:.6f}")
            print(f"   è€—æ—¶: {epoch_time:.2f}ç§’")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            current_val_loss = val_losses.get('total_loss', float('inf'))
            is_best = current_val_loss < self.best_loss
            
            if is_best:
                self.best_loss = current_val_loss
                print(f"ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.4f}")
            
            self.save_checkpoint(epoch + 1, is_best)
            
            # ç»˜åˆ¶å¹¶ä¿å­˜æŸå¤±æ›²çº¿
            if (epoch + 1) % 5 == 0 or is_best:  # æ¯5ä¸ªepochæˆ–æœ€ä½³æ¨¡å‹æ—¶ä¿å­˜å›¾ç‰‡
                self.plot_loss_curves()
                self.save_loss_data()
            
            # æ—©åœæ£€æŸ¥
            if self.should_early_stop():
                print("â¹ï¸ æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
                break
        
        # è®­ç»ƒå®Œæˆåçš„æœ€ç»ˆä¿å­˜
        self.plot_loss_curves()
        self.save_loss_data()
        
        total_time = time.time() - start_time
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸ’ æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.4f}")
        print(f"ğŸ“Š æŸå¤±æ›²çº¿å›¾: {self.plots_dir}/loss_curves_latest.png")
    
    def should_early_stop(self):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ"""
        if len(self.val_losses) < self.config['training']['patience']:
            return False
        
        recent_losses = [loss.get('total_loss', float('inf')) 
                        for loss in self.val_losses[-self.config['training']['patience']:]]
        
        # å¦‚æœæœ€è¿‘å‡ ä¸ªepochæŸå¤±éƒ½æ²¡æœ‰æ”¹å–„ï¼Œè§¦å‘æ—©åœ
        return all(loss >= self.best_loss for loss in recent_losses)


def create_default_config():
    """åˆ›å»ºé»˜è®¤é…ç½®"""
    return {
        'model': {
            'hidden_size': 256,
            'embed_size': 32,
            'depth': 10,
            'num_atom_types': 12,
            'num_bond_types': 4,
            'projection_dim': 128,
            'temperature': 0.1
        },
        'training': {
            'epochs': 100,
            'learning_rate': 1e-4,
            'weight_decay': 1e-5,
            'lr_step_size': 20,
            'lr_gamma': 0.8,
            'batch_size': 8,  # è¾ƒå°çš„æ‰¹æ¬¡ä»¥é€‚åº”å†…å­˜é™åˆ¶
            'num_workers': 2,
            'log_interval': 100,
            'save_interval': 10,
            'patience': 10,
            'output_dir': '../results/synthretro_pretrain_models/'
        },
        'data': {
            'train_data_path': '../data/pretrain/pretrain_tensors_train.pkl',
            'test_data_path': '../data/pretrain/pretrain_tensors_test.pkl',
            'train_vocab_path': '../data/pretrain/vocab_train.txt',
            'test_vocab_path': '../data/pretrain/vocab_test.txt',
            'use_lazy_loading': True
        },
        'device': 'cuda'
    }


def main():
    parser = argparse.ArgumentParser(description='SynthRetro-Pé¢„è®­ç»ƒ')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume', type=str, help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--device', type=str, default='cuda', help='è®­ç»ƒè®¾å¤‡')
    parser.add_argument('--batch_size', type=int, default=8, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r') as f:
            config = json.load(f)
    else:
        config = create_default_config()
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    config['device'] = args.device
    if args.batch_size:
        config['training']['batch_size'] = args.batch_size
    if args.epochs:
        config['training']['epochs'] = args.epochs
    if args.lr:
        config['training']['learning_rate'] = args.lr
    
    print("ğŸ”§ è®­ç»ƒé…ç½®:")
    print(json.dumps(config, indent=2))
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    train_path = config['data']['train_data_path']
    test_path = config['data']['test_data_path']
    
    if not os.path.exists(train_path):
        print(f"âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {train_path}")
        return
    
    if not os.path.exists(test_path):
        print(f"âŒ æµ‹è¯•æ•°æ®ä¸å­˜åœ¨: {test_path}")
        return
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("ğŸ“‚ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    try:
        train_loader, val_loader = create_pretrain_dataloaders(
            train_data_path=config['data']['train_data_path'],
            test_data_path=config['data']['test_data_path'],
            train_vocab_path=config['data']['train_vocab_path'],
            test_vocab_path=config['data']['test_vocab_path'],
            batch_size=config['training']['batch_size'],
            num_workers=config['training']['num_workers'],
            use_lazy_loading=config['data']['use_lazy_loading']
        )
        print("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = PretrainTrainer(config)
    
    # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.resume:
        trainer.load_checkpoint(args.resume)
    
    # å¼€å§‹è®­ç»ƒ
    try:
        trainer.train(train_loader, val_loader)
    except KeyboardInterrupt:
        print("\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        trainer.save_checkpoint(trainer.current_epoch)
        trainer.plot_loss_curves()
        trainer.save_loss_data()
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    print("ğŸ¯ è®­ç»ƒè„šæœ¬æ‰§è¡Œå®Œæˆ")


if __name__ == "__main__":
    main()#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
G2Retro-Pé¢„è®­ç»ƒè®­ç»ƒè„šæœ¬

è¿™æ˜¯ä¸€ä¸ªæ–°å¢çš„æ–‡ä»¶ï¼Œåº”è¯¥æ”¾åœ¨ model/ ç›®å½•ä¸‹ï¼Œå‘½åä¸º train_pretrain.py
å®ç°å¤šä»»åŠ¡é¢„è®­ç»ƒçš„å®Œæ•´è®­ç»ƒæµç¨‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import os
import time
import json
import argparse
from datetime import datetime
import numpy as np
from collections import defaultdict
import gc

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from synthretro_pretrain import create_synthretro_pretrain_model
from pretrain_dataloader import create_pretrain_dataloaders

class PretrainTrainer:
    """G2Retro-Pé¢„è®­ç»ƒè®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')
        print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = create_g2retro_pretrain_model(config['model'])
        self.model.to(self.device)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=config['training']['learning_rate'],
            weight_decay=config['training']['weight_decay']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer,
            step_size=config['training']['lr_step_size'],
            gamma=config['training']['lr_gamma']
        )
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_loss = float('inf')
        self.train_losses = []
        self.val_losses = []
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = config['training']['output_dir']
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        config_path = os.path.join(self.output_dir, 'config.json')
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        epoch_losses = defaultdict(list)
        
        for batch_idx, batch in enumerate(train_loader):
            if batch is None:
                continue
                
            try:
                # æ•°æ®ç§»åˆ°GPU
                batch = self.move_batch_to_device(batch)
                
                # å‰å‘ä¼ æ’­
                self.optimizer.zero_grad()
                losses, predictions = self.model(batch)
                
                # åå‘ä¼ æ’­
                total_loss = losses['total_loss']
                total_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                
                # è®°å½•æŸå¤±
                for loss_name, loss_value in losses.items():
                    if isinstance(loss_value, torch.Tensor):
                        epoch_losses[loss_name].append(loss_value.item())
                
                # æ‰“å°è¿›åº¦
                if batch_idx % self.config['training']['log_interval'] == 0:
                    print(f"  æ‰¹æ¬¡ {batch_idx}/{len(train_loader)}: "
                          f"æ€»æŸå¤±={total_loss.item():.4f}")
                
                # å†…å­˜æ¸…ç†
                del losses, predictions, batch
                torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"âš ï¸ æ‰¹æ¬¡ {batch_idx} è®­ç»ƒå¤±è´¥: {e}")
                continue
        
        # è®¡ç®—epochå¹³å‡æŸå¤±
        avg_losses = {}
        for loss_name, loss_list in epoch_losses.items():
            if loss_list:
                avg_losses[loss_name] = np.mean(loss_list)
        
        return avg_losses
    
    def validate_epoch(self, val_loader):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        epoch_losses = defaultdict(list)
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(val_loader):
                if batch is None:
                    continue
                    
                try:
                    # æ•°æ®ç§»åˆ°GPU
                    batch = self.move_batch_to_device(batch)
                    
                    # å‰å‘ä¼ æ’­
                    losses, predictions = self.model(batch)
                    
                    # è®°å½•æŸå¤±
                    for loss_name, loss_value in losses.items():
                        if isinstance(loss_value, torch.Tensor):
                            epoch_losses[loss_name].append(loss_value.item())
                    
                    # å†…å­˜æ¸…ç†
                    del losses, predictions, batch
                    torch.cuda.empty_cache()
                    
                except Exception as e:
                    print(f"âš ï¸ éªŒè¯æ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                    continue
        
        # è®¡ç®—epochå¹³å‡æŸå¤±
        avg_losses = {}
        for loss_name, loss_list in epoch_losses.items():
            if loss_list:
                avg_losses[loss_name] = np.mean(loss_list)
        
        return avg_losses
    
    def move_batch_to_device(self, batch):
        """å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
        if isinstance(batch, dict):
            return {k: self.move_batch_to_device(v) for k, v in batch.items()}
        elif isinstance(batch, torch.Tensor):
            return batch.to(self.device)
        elif isinstance(batch, list):
            return [self.move_batch_to_device(item) for item in batch]
        else:
            return batch
    
    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_loss': self.best_loss,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'config': self.config
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        latest_path = os.path.join(self.output_dir, 'latest_checkpoint.pt')
        torch.save(checkpoint, latest_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = os.path.join(self.output_dir, 'best_model.pt')
            torch.save(checkpoint, best_path)
            print(f"ğŸ’ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
        
        # å®šæœŸä¿å­˜
        if epoch % self.config['training']['save_interval'] == 0:
            epoch_path = os.path.join(self.output_dir, f'checkpoint_epoch_{epoch}.pt')
            torch.save(checkpoint, epoch_path)
    
    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if os.path.exists(checkpoint_path):
            print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            self.current_epoch = checkpoint['epoch']
            self.best_loss = checkpoint['best_loss']
            self.train_losses = checkpoint.get('train_losses', [])
            self.val_losses = checkpoint.get('val_losses', [])
            
            print(f"âœ… ä»epoch {self.current_epoch}ç»§ç»­è®­ç»ƒ")
            return True
        return False
    
    def train(self, train_loader, val_loader):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("ğŸš€ å¼€å§‹G2Retro-Pé¢„è®­ç»ƒ")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"ğŸ”§ è®­ç»ƒè®¾å¤‡: {self.device}")
        print(f"ğŸ“ˆ è®­ç»ƒè½®æ•°: {self.config['training']['epochs']}")
        
        start_time = time.time()
        
        for epoch in range(self.current_epoch, self.config['training']['epochs']):
            epoch_start_time = time.time()
            
            print(f"\nğŸ“… Epoch {epoch+1}/{self.config['training']['epochs']}")
            print("-" * 60)
            
            # è®­ç»ƒ
            print("ğŸ”„ è®­ç»ƒé˜¶æ®µ...")
            train_losses = self.train_epoch(train_loader)
            
            # éªŒè¯
            print("ğŸ” éªŒè¯é˜¶æ®µ...")
            val_losses = self.validate_epoch(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # è®°å½•æŸå¤±
            self.train_losses.append(train_losses)
            self.val_losses.append(val_losses)
            
            # æ‰“å°ç»“æœ
            epoch_time = time.time() - epoch_start_time
            print(f"\nğŸ“Š Epoch {epoch+1} ç»“æœ:")
            print(f"   è®­ç»ƒæŸå¤±: {train_losses.get('total_loss', 0):.4f}")
            print(f"   éªŒè¯æŸå¤±: {val_losses.get('total_loss', 0):.4f}")
            print(f"   å­¦ä¹ ç‡: {self.scheduler.get_last_lr()[0]:.6f}")
            print(f"   è€—æ—¶: {epoch_time:.2f}ç§’")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            current_val_loss = val_losses.get('total_loss', float('inf'))
            is_best = current_val_loss < self.best_loss
            
            if is_best:
                self.best_loss = current_val_loss
                print(f"ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.4f}")
            
            self.save_checkpoint(epoch + 1, is_best)
            
            # æ—©åœæ£€æŸ¥
            if self.should_early_stop():
                print("â¹ï¸ æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
                break
        
        total_time = time.time() - start_time
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸ’ æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.4f}")
    
    def should_early_stop(self):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ"""
        if len(self.val_losses) < self.config['training']['patience']:
            return False
        
        recent_losses = [loss.get('total_loss', float('inf')) 
                        for loss in self.val_losses[-self.config['training']['patience']:]]
        
        # å¦‚æœæœ€è¿‘å‡ ä¸ªepochæŸå¤±éƒ½æ²¡æœ‰æ”¹å–„ï¼Œè§¦å‘æ—©åœ
        return all(loss >= self.best_loss for loss in recent_losses)


def create_default_config():
    """åˆ›å»ºé»˜è®¤é…ç½®"""
    return {
        'model': {
            'hidden_size': 256,
            'embed_size': 32,
            'depth': 10,
            'num_atom_types': 12,
            'num_bond_types': 4,
            'projection_dim': 128,
            'temperature': 0.1
        },
        'training': {
            'epochs': 100,
            'learning_rate': 1e-4,
            'weight_decay': 1e-5,
            'lr_step_size': 20,
            'lr_gamma': 0.8,
            'batch_size': 8,  # è¾ƒå°çš„æ‰¹æ¬¡ä»¥é€‚åº”å†…å­˜é™åˆ¶
            'num_workers': 2,
            'log_interval': 100,
            'save_interval': 10,
            'patience': 10,
            'output_dir': '../results/pretrain_models/'
        },
        'data': {
            'train_data_path': '../data/pretrain/pretrain_tensors_train.pkl',
            'test_data_path': '../data/pretrain/pretrain_tensors_test.pkl',
            'train_vocab_path': '../data/pretrain/vocab_train.txt',
            'test_vocab_path': '../data/pretrain/vocab_test.txt',
            'use_lazy_loading': True
        },
        'device': 'cuda'
    }


def main():
    parser = argparse.ArgumentParser(description='G2Retro-Pé¢„è®­ç»ƒ')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume', type=str, help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--device', type=str, default='cuda', help='è®­ç»ƒè®¾å¤‡')
    parser.add_argument('--batch_size', type=int, default=8, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r') as f:
            config = json.load(f)
    else:
        config = create_default_config()
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    config['device'] = args.device
    if args.batch_size:
        config['training']['batch_size'] = args.batch_size
    if args.epochs:
        config['training']['epochs'] = args.epochs
    if args.lr:
        config['training']['learning_rate'] = args.lr
    
    print("ğŸ”§ è®­ç»ƒé…ç½®:")
    print(json.dumps(config, indent=2))
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    train_path = config['data']['train_data_path']
    test_path = config['data']['test_data_path']
    
    if not os.path.exists(train_path):
        print(f"âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {train_path}")
        return
    
    if not os.path.exists(test_path):
        print(f"âŒ æµ‹è¯•æ•°æ®ä¸å­˜åœ¨: {test_path}")
        return
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("ğŸ“‚ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    try:
        train_loader, val_loader = create_pretrain_dataloaders(
            train_data_path=config['data']['train_data_path'],
            test_data_path=config['data']['test_data_path'],
            train_vocab_path=config['data']['train_vocab_path'],
            test_vocab_path=config['data']['test_vocab_path'],
            batch_size=config['training']['batch_size'],
            num_workers=config['training']['num_workers'],
            use_lazy_loading=config['data']['use_lazy_loading']
        )
        print("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = PretrainTrainer(config)
    
    # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.resume:
        trainer.load_checkpoint(args.resume)
    
    # å¼€å§‹è®­ç»ƒ
    try:
        trainer.train(train_loader, val_loader)
    except KeyboardInterrupt:
        print("\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        trainer.save_checkpoint(trainer.current_epoch)
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    print("ğŸ¯ è®­ç»ƒè„šæœ¬æ‰§è¡Œå®Œæˆ")


if __name__ == "__main__":
    main()
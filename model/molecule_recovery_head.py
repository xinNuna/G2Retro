#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†å­æ¢å¤ä»»åŠ¡å¤´ - åŸºäºMolCLRçš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼ˆå›¾ç»“æ„çº§åˆ«æ©ç ç‰ˆæœ¬ï¼‰

è¿™ä¸ªæ¨¡å—å®ç°äº†G2Retro-Pçš„åˆ†å­æ¢å¤ä»»åŠ¡ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ ä»å¢å¼ºåçš„åˆ†å­ä¸­æ¢å¤åŸå§‹åˆ†å­è¡¨ç¤ºã€‚
å®Œå…¨å‚è€ƒMolCLRçš„å®ç°ï¼Œä½¿ç”¨InfoNCEæŸå¤±è¿›è¡Œå¯¹æ¯”å­¦ä¹ ã€‚

æ–°å¢åŠŸèƒ½ï¼š
- æ”¯æŒåœ¨MolTreeå›¾ç»“æ„çº§åˆ«è¿›è¡Œæ©ç æ“ä½œ
- æ©ç æ„ŸçŸ¥ç¼–ç æœºåˆ¶
- å¢å¼ºçš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶

å‚è€ƒæ–‡çŒ®ï¼š
- MolCLR: Molecular Contrastive Learning of Representations via Graph Neural Networks
- PMSR: Learning Chemical Rules of Retrosynthesis with Pre-training

æ ¸å¿ƒæ€æƒ³ï¼š
1. åŸå§‹åˆ†å­å’Œå¢å¼ºåˆ†å­åº”è¯¥æœ‰ç›¸ä¼¼çš„è¡¨ç¤ºï¼ˆæ­£æ ·æœ¬å¯¹ï¼‰
2. ä¸åŒåˆ†å­çš„è¡¨ç¤ºåº”è¯¥ä¸åŒï¼ˆè´Ÿæ ·æœ¬å¯¹ï¼‰
3. é€šè¿‡InfoNCEæŸå¤±æœ€å¤§åŒ–æ­£æ ·æœ¬ç›¸ä¼¼åº¦ï¼Œæœ€å°åŒ–è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
4. æ©ç æ„ŸçŸ¥ï¼šç†è§£å“ªäº›éƒ¨åˆ†è¢«æ©ç ï¼Œå­¦ä¹ ä»éƒ¨åˆ†ä¿¡æ¯æ¢å¤å®Œæ•´è¡¨ç¤º
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Dict, Optional, List
import math
import copy

# å¯¼å…¥å›¾ç»“æ„çº§åˆ«æ©ç å¢å¼ºå‡½æ•°
def apply_molclr_graph_augmentation(mol_tree, masked_indices, augment_type):
    """
    æŒ‰ç…§MolCLRæ€æƒ³åœ¨MolTreeçš„å›¾ç»“æ„ä¸Šè¿›è¡Œæ©ç æ“ä½œ
    ä¸ä¿®æ”¹åº•å±‚åŒ–å­¦ç»“æ„ï¼Œåªåœ¨å›¾è¡¨ç¤ºçº§åˆ«è¿›è¡Œä¸´æ—¶å¢å¼º
    """
    import copy
    
    try:
        # æ·±æ‹·è´MolTreeä»¥é¿å…ä¿®æ”¹åŸå§‹å¯¹è±¡
        augmented_tree = copy.deepcopy(mol_tree)
        
        # è·å–åˆ†å­å›¾ (NetworkX DiGraph)
        mol_graph = augmented_tree.mol_graph
        
        if augment_type == 'atom_mask':
            # åŸå­æ©ç ï¼šåœ¨å›¾èŠ‚ç‚¹çº§åˆ«è¿›è¡Œæ©ç ï¼Œä¸æ”¹å˜åŒ–å­¦ç»“æ„
            for atom_idx in masked_indices:
                if atom_idx in mol_graph.nodes:
                    node_data = mol_graph.nodes[atom_idx]
                    # ä¿å­˜åŸå§‹ç‰¹å¾
                    node_data['original_label'] = node_data.get('label', '')
                    node_data['original_aroma'] = node_data.get('aroma', False)
                    # è®¾ç½®æ©ç æ ‡è®°
                    node_data['masked'] = True
                    node_data['label'] = '[MASK]'  # æ©ç æ ‡è®°
                    node_data['aroma'] = False  # é‡ç½®èŠ³é¦™æ€§
                    
        elif augment_type == 'bond_deletion':
            # é”®åˆ é™¤ï¼šåœ¨å›¾è¾¹çº§åˆ«è¿›è¡Œæ©ç ï¼Œä¸åˆ é™¤å®é™…åŒ–å­¦é”®
            edges_list = list(mol_graph.edges())
            for bond_idx in masked_indices:
                if bond_idx < len(edges_list):
                    edge = edges_list[bond_idx]
                    if mol_graph.has_edge(edge[0], edge[1]):
                        edge_data = mol_graph.edges[edge]
                        # ä¿å­˜åŸå§‹è¾¹ç‰¹å¾
                        edge_data['original_bond_type'] = edge_data.get('bond_type', 1)
                        edge_data['original_is_conju'] = edge_data.get('is_conju', False)
                        # è®¾ç½®æ©ç æ ‡è®°
                        edge_data['masked'] = True
                        edge_data['bond_type'] = 0  # è®¾ä¸ºæ— é”®ç±»å‹
                        edge_data['is_conju'] = False
                        
        elif augment_type == 'subgraph_removal':
            # å­å›¾ç§»é™¤ï¼šåœ¨å›¾èŠ‚ç‚¹çº§åˆ«è¿›è¡Œæ©ç 
            for atom_idx in masked_indices:
                if atom_idx in mol_graph.nodes:
                    node_data = mol_graph.nodes[atom_idx]
                    # ä¿å­˜åŸå§‹ç‰¹å¾
                    node_data['original_label'] = node_data.get('label', '')
                    node_data['original_aroma'] = node_data.get('aroma', False)
                    # è®¾ç½®æ©ç æ ‡è®°
                    node_data['masked'] = True
                    node_data['label'] = '[REMOVED]'  # ç§»é™¤æ ‡è®°
                    node_data['aroma'] = False
                    
                    # åŒæ—¶æ©ç ç›¸å…³çš„è¾¹
                    for neighbor in mol_graph.neighbors(atom_idx):
                        if mol_graph.has_edge(atom_idx, neighbor):
                            edge_data = mol_graph.edges[atom_idx, neighbor]
                            edge_data['masked'] = True
                            edge_data['original_bond_type'] = edge_data.get('bond_type', 1)
                            edge_data['bond_type'] = 0
        
        # æ›´æ–°å¢å¼ºä¿¡æ¯
        augmented_tree.augmented = True
        augmented_tree.augment_type = augment_type
        augmented_tree.masked_indices = masked_indices
        
        return augmented_tree
        
    except Exception as e:
        print(f"MolCLRå›¾å¢å¼ºé”™è¯¯: {e}")
        return mol_tree  # è¿”å›åŸå§‹æ ‘ä½œä¸ºå¤‡é€‰

class MoleculeRecoveryHead(nn.Module):
    """
    åˆ†å­æ¢å¤ä»»åŠ¡å¤´ - å®Œå…¨å‚è€ƒMolCLRå®ç°ï¼Œæ”¯æŒå›¾ç»“æ„çº§åˆ«æ©ç 
    
    é€šè¿‡å¯¹æ¯”å­¦ä¹ å­¦ä¹ åˆ†å­è¡¨ç¤ºï¼Œä½¿å¢å¼ºåçš„åˆ†å­èƒ½å¤Ÿæ¢å¤åˆ°åŸå§‹åˆ†å­çš„è¡¨ç¤ºç©ºé—´
    æ–°å¢ï¼šæ©ç æ„ŸçŸ¥æœºåˆ¶ï¼Œèƒ½å¤Ÿå¤„ç†åœ¨å›¾ç»“æ„çº§åˆ«è¿›è¡Œæ©ç çš„MolTree
    """
    
    def __init__(self, 
                 input_dim: int = 300,           # G2Retroåˆ†å­å›¾åµŒå…¥ç»´åº¦
                 projection_dim: int = 128,       # MolCLRæŠ•å½±ç»´åº¦
                 hidden_dim: int = 512,          # éšè—å±‚ç»´åº¦
                 temperature: float = 0.1,       # InfoNCEæ¸©åº¦å‚æ•°
                 dropout: float = 0.1):
        """
        åˆå§‹åŒ–åˆ†å­æ¢å¤ä»»åŠ¡å¤´
        
        Args:
            input_dim: è¾“å…¥åˆ†å­å›¾åµŒå…¥çš„ç»´åº¦
            projection_dim: æŠ•å½±åçš„è¡¨ç¤ºç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            temperature: InfoNCEæŸå¤±çš„æ¸©åº¦å‚æ•°
            dropout: Dropoutæ¯”ä¾‹
        """
        super(MoleculeRecoveryHead, self).__init__()
        
        self.input_dim = input_dim
        self.projection_dim = projection_dim
        self.temperature = temperature
        
        # MolCLRé£æ ¼çš„æŠ•å½±å¤´ - ä¸¤å±‚MLP
        # å®Œå…¨å‚è€ƒMolCLRçš„æ¶æ„è®¾è®¡
        self.projection_head = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, projection_dim),
            nn.BatchNorm1d(projection_dim)
        )
        
        # æ–°å¢ï¼šæ©ç æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶
        self.mask_attention = nn.MultiheadAttention(
            embed_dim=input_dim,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        
        # æ–°å¢ï¼šæ©ç ç±»å‹åµŒå…¥ï¼ˆåŒºåˆ†ä¸åŒçš„å¢å¼ºç±»å‹ï¼‰
        self.mask_type_embedding = nn.Embedding(4, input_dim)  # none, atom_mask, bond_deletion, subgraph_removal
        self.mask_type_map = {
            'none': 0,
            'atom_mask': 1, 
            'bond_deletion': 2,
            'subgraph_removal': 3
        }
        
        # æ–°å¢ï¼šæ©ç ä¿¡æ¯èåˆå±‚
        self.mask_fusion = nn.Sequential(
            nn.Linear(input_dim * 2, input_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(input_dim, input_dim)
        )
        
        # åˆå§‹åŒ–æƒé‡ï¼ˆä½¿ç”¨Heåˆå§‹åŒ–ï¼‰
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡ - å‚è€ƒMolCLRçš„åˆå§‹åŒ–ç­–ç•¥"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Embedding):
                nn.init.normal_(m.weight, 0, 0.1)
    
    def apply_mask_aware_encoding(self, 
                                 mol_embeddings: torch.Tensor,
                                 mask_info: List[Dict]) -> torch.Tensor:
        """
        åº”ç”¨æ©ç æ„ŸçŸ¥ç¼–ç 
        
        Args:
            mol_embeddings: åˆ†å­åµŒå…¥ [batch_size, input_dim]
            mask_info: æ©ç ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«typeå’Œindices
            
        Returns:
            æ©ç æ„ŸçŸ¥çš„åˆ†å­åµŒå…¥
        """
        batch_size = mol_embeddings.size(0)
        device = mol_embeddings.device
        
        # è·å–æ©ç ç±»å‹åµŒå…¥
        mask_types = []
        for info in mask_info:
            mask_type = info.get('type', 'none')
            mask_type_id = self.mask_type_map.get(mask_type, 0)
            mask_types.append(mask_type_id)
        
        mask_type_tensor = torch.tensor(mask_types, device=device)
        mask_type_embeds = self.mask_type_embedding(mask_type_tensor)  # [batch_size, input_dim]
        
        # èåˆåŸå§‹åµŒå…¥å’Œæ©ç ç±»å‹ä¿¡æ¯
        combined_embeds = torch.cat([mol_embeddings, mask_type_embeds], dim=1)  # [batch_size, input_dim*2]
        mask_aware_embeds = self.mask_fusion(combined_embeds)  # [batch_size, input_dim]
        
        # åº”ç”¨æ©ç æ„ŸçŸ¥æ³¨æ„åŠ›
        if len(mol_embeddings.shape) == 2:
            mol_embeddings_expanded = mol_embeddings.unsqueeze(1)  # [batch_size, 1, input_dim]
            mask_aware_expanded = mask_aware_embeds.unsqueeze(1)   # [batch_size, 1, input_dim]
        else:
            mol_embeddings_expanded = mol_embeddings
            mask_aware_expanded = mask_aware_embeds.unsqueeze(1).expand(-1, mol_embeddings.size(1), -1)
        
        # æ©ç æ„ŸçŸ¥æ³¨æ„åŠ›ï¼šæŸ¥è¯¢ä½¿ç”¨æ©ç æ„ŸçŸ¥åµŒå…¥ï¼Œé”®å€¼ä½¿ç”¨åŸå§‹åµŒå…¥
        attended_embeds, _ = self.mask_attention(
            mask_aware_expanded,      # query
            mol_embeddings_expanded,  # key
            mol_embeddings_expanded   # value
        )
        
        # å¦‚æœè¾“å…¥æ˜¯2Dï¼Œå‹ç¼©å›2D
        if len(mol_embeddings.shape) == 2:
            attended_embeds = attended_embeds.squeeze(1)
        
        return attended_embeds
    
    def forward(self, 
                original_embeddings: torch.Tensor,
                augmented_data: List[Dict],
                pretrain_infos: List[Dict],
                mol_encoder=None,
                vocab=None,
                avocab=None) -> Tuple[torch.Tensor, float]:
        """
        å‰å‘ä¼ æ’­ - è®¡ç®—åˆ†å­æ¢å¤çš„å¯¹æ¯”å­¦ä¹ æŸå¤±
        
        Args:
            original_embeddings: åŸå§‹åˆ†å­çš„å›¾åµŒå…¥ [batch_size, input_dim]
            augmented_data: å¢å¼ºæ•°æ®ä¿¡æ¯
            pretrain_infos: é¢„è®­ç»ƒä¿¡æ¯
            mol_encoder: åˆ†å­ç¼–ç å™¨ï¼ˆç”¨äºç¼–ç å¢å¼ºçš„MolTreeï¼‰
            vocab: è¯æ±‡è¡¨
            avocab: åŸå­è¯æ±‡è¡¨
            
        Returns:
            (loss, accuracy): æŸå¤±å€¼å’Œå‡†ç¡®ç‡
        """
        batch_size = original_embeddings.size(0)
        device = original_embeddings.device
        
        # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºæ©ç ä¿¡æ¯å’ŒçœŸæ­£çš„å¢å¼ºåµŒå…¥
        mask_info = []
        augmented_embeddings_list = []
        
        for i in range(batch_size):
            # è·å–å½“å‰æ ·æœ¬çš„å¢å¼ºä¿¡æ¯
            sample_aug_data = []
            if i < len(augmented_data):
                sample_aug_data = augmented_data[i] if isinstance(augmented_data[i], list) else [augmented_data[i]]
            
            if sample_aug_data and mol_encoder is not None and vocab is not None and avocab is not None:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¢å¼ºç‰ˆæœ¬çš„ä¿¡æ¯
                aug_info = sample_aug_data[0]
                mask_info.append({
                    'type': aug_info.get('augment_type', 'none'),
                    'indices': aug_info.get('masked_indices', [])
                })
                
                # åˆ›å»ºçœŸæ­£çš„å¢å¼ºMolTreeå¹¶è·å–å…¶åµŒå…¥
                try:
                    product_smiles = pretrain_infos[i]['product_smiles']
                    
                    # åˆ›å»ºåŸå§‹MolTree
                    from moltree import MolTree
                    original_tree = MolTree(product_smiles)
                    
                    # åº”ç”¨å›¾ç»“æ„çº§åˆ«æ©ç å¢å¼º
                    augmented_tree = apply_molclr_graph_augmentation(
                        original_tree,
                        aug_info.get('masked_indices', []),
                        aug_info.get('augment_type', 'none')
                    )
                    
                    # ä½¿ç”¨ç¼–ç å™¨è·å–å¢å¼ºMolTreeçš„åµŒå…¥
                    aug_batch, aug_tensors = MolTree.tensorize(
                        [augmented_tree], vocab, avocab, 
                        use_feature=True, product=True
                    )
                    
                    # ç¼–ç å¢å¼ºçš„åˆ†å­æ ‘
                    with torch.no_grad():
                        aug_embed, _, _ = mol_encoder.encode_with_gmpn([aug_tensors])
                    
                    augmented_embeddings_list.append(aug_embed)
                    print(f"æ ·æœ¬ {i}: æˆåŠŸè·å–å›¾ç»“æ„çº§åˆ«å¢å¼ºåµŒå…¥")
                    
                except Exception as e:
                    print(f"æ ·æœ¬ {i} å¢å¼ºåµŒå…¥è·å–é”™è¯¯: {e}")
                    # å¦‚æœå¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹åµŒå…¥
                    augmented_embeddings_list.append(original_embeddings[i:i+1])
                    mask_info.append({'type': 'none', 'indices': []})
            else:
                # æ²¡æœ‰å¢å¼ºæ•°æ®æˆ–ç¼ºå°‘ç¼–ç å™¨ï¼Œä½¿ç”¨åŸå§‹åµŒå…¥
                mask_info.append({'type': 'none', 'indices': []})
                augmented_embeddings_list.append(original_embeddings[i:i+1])
        
        # æ‹¼æ¥æ‰€æœ‰å¢å¼ºåµŒå…¥
        augmented_embeddings = torch.cat(augmented_embeddings_list, dim=0)
        
        # åº”ç”¨æ©ç æ„ŸçŸ¥ç¼–ç 
        mask_aware_augmented = self.apply_mask_aware_encoding(augmented_embeddings, mask_info)
        
        # é€šè¿‡æŠ•å½±å¤´è·å¾—å¯¹æ¯”å­¦ä¹ è¡¨ç¤º
        original_proj = self.projection_head(original_embeddings)     # [batch_size, projection_dim]
        augmented_proj = self.projection_head(mask_aware_augmented)   # [batch_size, projection_dim]
        
        # L2å½’ä¸€åŒ–ï¼ˆMolCLRçš„å…³é”®æ­¥éª¤ï¼‰
        original_proj = F.normalize(original_proj, p=2, dim=1)
        augmented_proj = F.normalize(augmented_proj, p=2, dim=1)
        
        # è®¡ç®—InfoNCEæŸå¤±
        contrastive_loss = self._compute_infonce_loss(original_proj, augmented_proj)
        
        # è®¡ç®—å‡†ç¡®ç‡
        similarity_matrix = torch.mm(original_proj, augmented_proj.t()) / self.temperature
        accuracy = self._compute_accuracy(similarity_matrix)
        
        return contrastive_loss, accuracy.item()
    
    def _compute_infonce_loss(self, 
                             z_i: torch.Tensor, 
                             z_j: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—InfoNCEå¯¹æ¯”å­¦ä¹ æŸå¤± - å®Œå…¨å‚è€ƒMolCLRå®ç°
        
        InfoNCEå…¬å¼ï¼š
        L = -log(exp(sim(z_i, z_j)/Ï„) / Î£_k exp(sim(z_i, z_k)/Ï„))
        
        å…¶ä¸­z_iæ˜¯åŸå§‹åˆ†å­è¡¨ç¤ºï¼Œz_jæ˜¯å¯¹åº”çš„å¢å¼ºåˆ†å­è¡¨ç¤ºï¼Œz_kæ˜¯æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰è´Ÿæ ·æœ¬
        
        Args:
            z_i: åŸå§‹åˆ†å­çš„å½’ä¸€åŒ–æŠ•å½±è¡¨ç¤º [batch_size, projection_dim]
            z_j: å¢å¼ºåˆ†å­çš„å½’ä¸€åŒ–æŠ•å½±è¡¨ç¤º [batch_size, projection_dim]
            
        Returns:
            InfoNCEæŸå¤±å€¼
        """
        batch_size = z_i.size(0)
        device = z_i.device
        
        # æ„å»ºæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„è¡¨ç¤ºçŸ©é˜µ
        # å°†z_iå’Œz_jæ‹¼æ¥ï¼Œå½¢æˆ2*batch_sizeçš„è¡¨ç¤ºçŸ©é˜µ
        representations = torch.cat([z_i, z_j], dim=0)  # [2*batch_size, projection_dim]
        
        # è®¡ç®—æ‰€æœ‰è¡¨ç¤ºä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.mm(representations, representations.t()) / self.temperature
        # å½¢çŠ¶: [2*batch_size, 2*batch_size]
        
        # åˆ›å»ºæ­£æ ·æœ¬mask
        # å¯¹äºç´¢å¼•iï¼Œå…¶æ­£æ ·æœ¬æ˜¯ç´¢å¼•(i + batch_size) % (2 * batch_size)
        positive_mask = torch.zeros(2 * batch_size, 2 * batch_size, dtype=torch.bool, device=device)
        for i in range(batch_size):
            positive_mask[i, i + batch_size] = True          # z_içš„æ­£æ ·æœ¬æ˜¯z_j[i]
            positive_mask[i + batch_size, i] = True          # z_j[i]çš„æ­£æ ·æœ¬æ˜¯z_i
        
        # åˆ›å»ºå¯¹è§’çº¿maskï¼ˆæ’é™¤è‡ªèº«ï¼‰
        diagonal_mask = torch.eye(2 * batch_size, dtype=torch.bool, device=device)
        
        # è®¡ç®—InfoNCEæŸå¤±
        # å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œè®¡ç®—å…¶ä¸æ­£æ ·æœ¬çš„ç›¸ä¼¼åº¦å’Œä¸æ‰€æœ‰è´Ÿæ ·æœ¬çš„ç›¸ä¼¼åº¦
        losses = []
        for i in range(2 * batch_size):
            # å½“å‰æ ·æœ¬ä¸æ‰€æœ‰æ ·æœ¬çš„ç›¸ä¼¼åº¦
            current_similarities = similarity_matrix[i]
            
            # æ’é™¤è‡ªèº«
            current_similarities = current_similarities[~diagonal_mask[i]]
            positive_mask_current = positive_mask[i][~diagonal_mask[i]]
            
            # æ­£æ ·æœ¬ç›¸ä¼¼åº¦
            positive_similarity = current_similarities[positive_mask_current]
            
            # æ‰€æœ‰æ ·æœ¬ï¼ˆåŒ…æ‹¬æ­£æ ·æœ¬ï¼‰çš„ç›¸ä¼¼åº¦ç”¨äºåˆ†æ¯
            denominator = torch.logsumexp(current_similarities, dim=0)
            
            # InfoNCEæŸå¤±: -log(exp(pos_sim) / sum(exp(all_sim)))
            loss = -positive_similarity + denominator
            losses.append(loss)
        
        # è¿”å›å¹³å‡æŸå¤±
        return torch.stack(losses).mean()
    
    def _compute_accuracy(self, similarity_matrix: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—å¯¹æ¯”å­¦ä¹ çš„å‡†ç¡®ç‡ - ç”¨äºç›‘æ§è®­ç»ƒè¿‡ç¨‹
        
        å‡†ç¡®ç‡å®šä¹‰ï¼šåœ¨ä¸€ä¸ªæ‰¹æ¬¡ä¸­ï¼Œæœ‰å¤šå°‘æ¯”ä¾‹çš„æ ·æœ¬çš„æœ€ç›¸ä¼¼æ ·æœ¬ç¡®å®æ˜¯å…¶æ­£æ ·æœ¬
        
        Args:
            similarity_matrix: ç›¸ä¼¼åº¦çŸ©é˜µ [batch_size, batch_size]
            
        Returns:
            å‡†ç¡®ç‡å€¼
        """
        batch_size = similarity_matrix.size(0)
        
        # æ‰¾åˆ°æ¯ä¸ªåŸå§‹åˆ†å­æœ€ç›¸ä¼¼çš„å¢å¼ºåˆ†å­
        _, top_indices = torch.topk(similarity_matrix, k=1, dim=1)
        
        # æ­£ç¡®çš„åŒ¹é…åº”è¯¥æ˜¯å¯¹è§’çº¿ï¼ˆæ¯ä¸ªåˆ†å­æœ€ç›¸ä¼¼çš„åº”è¯¥æ˜¯è‡ªå·±çš„å¢å¼ºç‰ˆæœ¬ï¼‰
        correct_matches = (top_indices.squeeze() == torch.arange(batch_size, device=similarity_matrix.device))
        
        return correct_matches.float().mean()

    def compute_molecular_similarity(self, 
                                   mol_embeddings_1: torch.Tensor,
                                   mol_embeddings_2: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—ä¸¤ç»„åˆ†å­ä¹‹é—´çš„ç›¸ä¼¼åº¦ - ç”¨äºä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°
        
        Args:
            mol_embeddings_1: ç¬¬ä¸€ç»„åˆ†å­åµŒå…¥ [N, input_dim]
            mol_embeddings_2: ç¬¬äºŒç»„åˆ†å­åµŒå…¥ [M, input_dim]
            
        Returns:
            ç›¸ä¼¼åº¦çŸ©é˜µ [N, M]
        """
        # æŠ•å½±åˆ°å¯¹æ¯”å­¦ä¹ ç©ºé—´
        proj_1 = F.normalize(self.projection_head(mol_embeddings_1), p=2, dim=1)
        proj_2 = F.normalize(self.projection_head(mol_embeddings_2), p=2, dim=1)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = torch.mm(proj_1, proj_2.t())
        
        return similarity


class InfoNCELoss(nn.Module):
    """
    ç‹¬ç«‹çš„InfoNCEæŸå¤±æ¨¡å— - å¯å¤ç”¨äºå…¶ä»–å¯¹æ¯”å­¦ä¹ ä»»åŠ¡
    """
    
    def __init__(self, temperature: float = 0.1):
        """
        åˆå§‹åŒ–InfoNCEæŸå¤±
        
        Args:
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶åˆ†å¸ƒçš„å°–é”ç¨‹åº¦
        """
        super(InfoNCELoss, self).__init__()
        self.temperature = temperature
    
    def forward(self, 
                anchor: torch.Tensor, 
                positive: torch.Tensor,
                negatives: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        è®¡ç®—InfoNCEæŸå¤±
        
        Args:
            anchor: é”šç‚¹è¡¨ç¤º [batch_size, dim]
            positive: æ­£æ ·æœ¬è¡¨ç¤º [batch_size, dim] 
            negatives: è´Ÿæ ·æœ¬è¡¨ç¤º [batch_size, num_negatives, dim]ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ‰¹æ¬¡å†…è´Ÿé‡‡æ ·
            
        Returns:
            InfoNCEæŸå¤±
        """
        batch_size = anchor.size(0)
        device = anchor.device
        
        # å½’ä¸€åŒ–
        anchor = F.normalize(anchor, p=2, dim=1)
        positive = F.normalize(positive, p=2, dim=1)
        
        if negatives is None:
            # æ‰¹æ¬¡å†…è´Ÿé‡‡æ ·ï¼šä½¿ç”¨æ‰¹æ¬¡ä¸­å…¶ä»–æ ·æœ¬ä½œä¸ºè´Ÿæ ·æœ¬
            representations = torch.cat([anchor, positive], dim=0)
            similarity_matrix = torch.mm(representations, representations.t()) / self.temperature
            
            # åˆ›å»ºæ ‡ç­¾ï¼šå¯¹äºanchor[i]ï¼Œpositive[i]æ˜¯æ­£æ ·æœ¬
            labels = torch.arange(batch_size, device=device)
            labels = torch.cat([labels + batch_size, labels], dim=0)
            
            # æ’é™¤è‡ªèº«
            mask = torch.eye(2 * batch_size, device=device).bool()
            similarity_matrix.masked_fill_(mask, float('-inf'))
            
            # è®¡ç®—äº¤å‰ç†µæŸå¤±
            loss = F.cross_entropy(similarity_matrix, labels)
            
        else:
            # æ˜¾å¼è´Ÿæ ·æœ¬
            negatives = F.normalize(negatives, p=2, dim=-1)
            
            # è®¡ç®—æ­£æ ·æœ¬ç›¸ä¼¼åº¦
            pos_sim = torch.sum(anchor * positive, dim=1) / self.temperature  # [batch_size]
            
            # è®¡ç®—è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
            neg_sim = torch.bmm(anchor.unsqueeze(1), negatives.transpose(1, 2)).squeeze(1) / self.temperature
            # [batch_size, num_negatives]
            
            # InfoNCEæŸå¤±
            logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)  # [batch_size, 1 + num_negatives]
            labels = torch.zeros(batch_size, dtype=torch.long, device=device)  # æ­£æ ·æœ¬åœ¨ç¬¬0ä½
            
            loss = F.cross_entropy(logits, labels)
        
        return loss


def create_molecular_recovery_head(config: Dict) -> MoleculeRecoveryHead:
    """
    å·¥å‚å‡½æ•°ï¼šæ ¹æ®é…ç½®åˆ›å»ºåˆ†å­æ¢å¤ä»»åŠ¡å¤´
    
    Args:
        config: é…ç½®å­—å…¸ï¼ŒåŒ…å«æ¨¡å‹å‚æ•°
        
    Returns:
        é…ç½®å¥½çš„åˆ†å­æ¢å¤ä»»åŠ¡å¤´
    """
    return MoleculeRecoveryHead(
        input_dim=config.get('input_dim', 300),
        projection_dim=config.get('projection_dim', 128),
        hidden_dim=config.get('hidden_dim', 512),
        temperature=config.get('temperature', 0.1),
        dropout=config.get('dropout', 0.1)
    )


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # æµ‹è¯•åˆ†å­æ¢å¤ä»»åŠ¡å¤´
    print("ğŸ§ª æµ‹è¯•åˆ†å­æ¢å¤ä»»åŠ¡å¤´...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºæ¨¡å‹
    recovery_head = MoleculeRecoveryHead(
        input_dim=300,
        projection_dim=128,
        hidden_dim=512,
        temperature=0.1
    ).to(device)
    
    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 32
    original_embeddings = torch.randn(batch_size, 300).to(device)
    augmented_data = [
        {'augment_type': 'none', 'masked_indices': []},
        {'augment_type': 'atom_mask', 'masked_indices': [0, 1, 2]},
        {'augment_type': 'bond_deletion', 'masked_indices': [0, 1]},
        {'augment_type': 'subgraph_removal', 'masked_indices': [0, 1, 2, 3]}
    ]
    pretrain_infos = [{'type': 'none', 'indices': []}] * batch_size
    
    # å‰å‘ä¼ æ’­
    results = recovery_head(original_embeddings, augmented_data, pretrain_infos)
    
    print(f"âœ… åˆ†å­æ¢å¤æŸå¤±: {results[0].item():.4f}")
    print(f"âœ… å¯¹æ¯”å­¦ä¹ å‡†ç¡®ç‡: {results[1]:.4f}")
    print(f"âœ… åŸå§‹åˆ†å­æŠ•å½±å½¢çŠ¶: {original_embeddings.shape}")
    print(f"âœ… å¢å¼ºåˆ†å­æŠ•å½±å½¢çŠ¶: {augmented_data[0]['augmented_embeddings'].shape}")
    
    # æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—
    similarity = recovery_head.compute_molecular_similarity(
        original_embeddings[:10], 
        augmented_data[0]['augmented_embeddings'][:10]
    )
    print(f"âœ… åˆ†å­ç›¸ä¼¼åº¦çŸ©é˜µå½¢çŠ¶: {similarity.shape}")
    
    print("ğŸ‰ åˆ†å­æ¢å¤ä»»åŠ¡å¤´æµ‹è¯•å®Œæˆï¼") 